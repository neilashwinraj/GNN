{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108eaaa7-9630-4440-a146-f78c7363c320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /localscratch-ssd/289625/matplotlib-yoep0oz9 because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import MoleculeNet\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial import cKDTree\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as geom_nn\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.nn.dense import DenseGCNConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60df55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the list from the pickle file ###\n",
    "### Train Data ###\n",
    "with open('train_data_np_scaled.pkl', 'rb') as f:\n",
    "    train_data_np_scaled = pickle.load(f)\n",
    "\n",
    "### Test Data ###\n",
    "with open('test_data_np_scaled.pkl', 'rb') as f:\n",
    "    test_data_np_scaled = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0830bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34407167, 0.34918513, 0.56854133, 0.63448512, 0.33022027,\n",
       "        0.40517533, 0.62870267, 0.53271002, 0.38819273, 0.28237867,\n",
       "        0.525932  , 0.6717093 , 0.32078433, 0.26959267, 0.62823667,\n",
       "        0.72224701, 0.400144  , 0.347111  , 0.46699   , 0.64623932,\n",
       "        0.27414187, 0.35712667, 0.66621133, 0.6190243 , 0.243908  ,\n",
       "        0.38105533, 0.49648533, 0.56017708, 0.37557587, 0.26455933,\n",
       "        0.69015347, 0.58668433, 0.36889   , 0.22820533, 0.480584  ,\n",
       "        0.59320475, 0.24428867, 0.25186333, 0.629588  , 0.59093708,\n",
       "        0.19227533, 0.29914933, 0.55064333, 0.56045067, 0.36573533,\n",
       "        0.3153772 , 0.410002  , 0.53469994, 0.22763533, 0.256208  ,\n",
       "        0.49946133, 0.62557304, 0.450466  , 0.47587867, 0.54980467,\n",
       "        0.49193433, 0.19378733, 0.31341667, 0.63136913, 0.55318804,\n",
       "        0.36573533, 0.3153772 , 0.74333533, 0.53469994, 0.        ,\n",
       "        0.        , 0.        , 0.63448512, 0.18889798],\n",
       "       [0.33643627, 0.35437233, 0.572982  , 0.59721892, 0.3303456 ,\n",
       "        0.41289533, 0.63407333, 0.49438106, 0.3778722 , 0.285454  ,\n",
       "        0.53213   , 0.69694464, 0.30935847, 0.26826933, 0.623852  ,\n",
       "        0.6446167 , 0.27097313, 0.36023533, 0.67021733, 0.57349328,\n",
       "        0.39143967, 0.3469202 , 0.463502  , 0.72735638, 0.24125733,\n",
       "        0.386686  , 0.49428267, 0.55602889, 0.37132493, 0.265036  ,\n",
       "        0.68107207, 0.60908094, 0.23863333, 0.249504  , 0.62650333,\n",
       "        0.63853476, 0.36371867, 0.23209867, 0.48401267, 0.59671606,\n",
       "        0.19274   , 0.32048733, 0.62781273, 0.52487582, 0.20847133,\n",
       "        0.416808  , 0.64137933, 0.29755478, 0.18865867, 0.30439067,\n",
       "        0.546608  , 0.57587923, 0.36450327, 0.3199346 , 0.73106087,\n",
       "        0.52893194, 0.22113067, 0.25312133, 0.49706067, 0.63814284,\n",
       "        0.451794  , 0.483208  , 0.55177933, 0.48575289, 0.        ,\n",
       "        0.        , 0.        , 0.59721892, 0.19656991],\n",
       "       [0.66453767, 0.3606574 , 0.5773    , 0.55711794, 0.66624047,\n",
       "        0.420356  , 0.63912667, 0.45542558, 0.69998853, 0.28749267,\n",
       "        0.539556  , 0.70110564, 0.6344466 , 0.26738667, 0.62025267,\n",
       "        0.64031383, 0.59994467, 0.36335733, 0.673292  , 0.59521161,\n",
       "        0.71636467, 0.34959867, 0.45935   , 0.70626919, 0.57193067,\n",
       "        0.391408  , 0.492364  , 0.55981394, 0.555604  , 0.42014267,\n",
       "        0.63514533, 0.29649668, 0.69961607, 0.26448467, 0.67321493,\n",
       "        0.59206236, 0.69612713, 0.3252262 , 0.71866447, 0.55018051,\n",
       "        0.52457   , 0.325606  , 0.6226818 , 0.53699802, 0.691992  ,\n",
       "        0.23470333, 0.489306  , 0.59485257, 0.56412667, 0.24760933,\n",
       "        0.62301133, 0.63892251, 0.51791467, 0.30892467, 0.541198  ,\n",
       "        0.57702427, 0.54856733, 0.250654  , 0.494166  , 0.63160543,\n",
       "        0.54802   , 0.49441667, 0.54434333, 0.49123672, 0.        ,\n",
       "        0.        , 0.        , 0.55711794, 0.20062845],\n",
       "       [0.66134667, 0.36750813, 0.580956  , 0.53089674, 0.67122793,\n",
       "        0.42653267, 0.64284867, 0.418749  , 0.68887853, 0.28899667,\n",
       "        0.54762667, 0.69594795, 0.6261914 , 0.26677667, 0.61627067,\n",
       "        0.64384311, 0.59447133, 0.36631867, 0.675476  , 0.60692367,\n",
       "        0.56919733, 0.42504067, 0.62655   , 0.3301362 , 0.56894733,\n",
       "        0.395272  , 0.49099267, 0.56515313, 0.7095534 , 0.354388  ,\n",
       "        0.45501933, 0.62779854, 0.6932524 , 0.33072187, 0.7074564 ,\n",
       "        0.56802239, 0.694882  , 0.26308667, 0.6670938 , 0.58023999,\n",
       "        0.522124  , 0.328734  , 0.61631253, 0.55470628, 0.68695   ,\n",
       "        0.236088  , 0.49623   , 0.59642243, 0.51322333, 0.31227667,\n",
       "        0.53525733, 0.59558497, 0.55666333, 0.24475733, 0.61913733,\n",
       "        0.62179586, 0.548506  , 0.49170333, 0.53997467, 0.54525157,\n",
       "        0.65954   , 0.51326733, 0.68105   , 0.01035743, 0.        ,\n",
       "        0.        , 0.        , 0.53089674, 0.20307741]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_np_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec2ebf-0fe7-4292-a284-2a85dd4cdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data (Ze Time series data) ###\n",
    "location = \"simple_connections_data/random_split/\"\n",
    "experiment = \"rho100_10percent_Re100/\"\n",
    "\n",
    "train_input = np.load(location+experiment+\"train_inputs.npy\")\n",
    "test_input = np.load(location+experiment+\"test_inputs.npy\")\n",
    "\n",
    "train_inputs_global = torch.tensor(np.load(location+experiment+\"train_input_scalar.npy\"))\n",
    "test_inputs_global = torch.tensor(np.load(location+experiment+\"test_input_scalar.npy\"))\n",
    "\n",
    "train_output = np.load(location+experiment+\"train_output.npy\")\n",
    "test_output = np.load(location+experiment+\"test_output.npy\")\n",
    "\n",
    "### edge index for basic connections ###\n",
    "edge_index = torch.tensor([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                           [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]])\n",
    "\n",
    "train_combined = list()\n",
    "test_combined = list()\n",
    "\n",
    "### Stacking up train data ###\n",
    "for i in range(len(train_input)):\n",
    "\n",
    "    ### setting inputs ###\n",
    "    x = torch.tensor(train_input[i]).float().clone().detach()\n",
    "    \n",
    "    ### adding drag force as y ###\n",
    "    y = torch.tensor(train_output[i]).float().clone().detach()\n",
    "    \n",
    "    # all_data_graph_struct.append(Data(x=x , edge_index=torch.tensor(mirror_edge_index(edge_index)) , y=y))\n",
    "    train_combined.append(Data(x=x.clone().detach() , edge_index=edge_index.clone().detach() , y=y[:,None].clone().detach()))\n",
    "\n",
    "### Stacking up test data ###\n",
    "for i in range(len(test_input)):\n",
    "\n",
    "    ### setting inputs ###\n",
    "    x = torch.tensor(test_input[i]).float().clone().detach()\n",
    "    \n",
    "    ### adding drag force as y ###\n",
    "    y = torch.tensor(test_output[i]).float().clone().detach()\n",
    "    \n",
    "    # all_data_graph_struct.append(Data(x=x , edge_index=torch.tensor(mirror_edge_index(edge_index)) , y=y))\n",
    "    test_combined.append(Data(x=x.clone().detach() , edge_index=edge_index.clone().detach() , y=y[:,None].clone().detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a5d34",
   "metadata": {},
   "source": [
    "# Graph Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66785b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN_with_attention(\n",
      "  (initial_conv): GATv2Conv(4, 85, heads=2)\n",
      "  (convs): ModuleList(\n",
      "    (0): GATv2Conv(170, 85, heads=2)\n",
      "    (1): GATv2Conv(170, 85, heads=2)\n",
      "  )\n",
      "  (fcnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=174, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Number of parameters:  134425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 174])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, global_add_pool as gap\n",
    "from torch.nn import Linear\n",
    "\n",
    "class GNN_with_attention(torch.nn.Module):\n",
    "    def __init__(self, num_gcn_layers=2, num_fcnn_layers=2, gcn_embedding_size=64, fcnn_embedding_size=64,\n",
    "                 num_nodes=16, num_features=4, dropout_prob=0.5, heads=2):\n",
    "        super(GNN_with_attention, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.num_gcn_layers = num_gcn_layers\n",
    "        self.num_fcnn_layers = num_fcnn_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # GATv2Conv layers\n",
    "        self.initial_conv = GATv2Conv(num_features, gcn_embedding_size, heads=heads)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_gcn_layers - 1):\n",
    "            self.convs.append(GATv2Conv(gcn_embedding_size * heads, gcn_embedding_size, heads=heads))\n",
    "\n",
    "        # FCNN layers\n",
    "        self.fcnn_layers = torch.nn.ModuleList()\n",
    "        if num_fcnn_layers > 0:\n",
    "            self.fcnn_layers.append(Linear(gcn_embedding_size * heads + 4, fcnn_embedding_size))\n",
    "            for _ in range(num_fcnn_layers - 2):\n",
    "                self.fcnn_layers.append(Linear(fcnn_embedding_size, fcnn_embedding_size))\n",
    "            self.fcnn_layers.append(Linear(fcnn_embedding_size, 1))\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(p=self.dropout_prob)\n",
    "\n",
    "    def forward(self, x, edge_index, x_scalar, batch_index):\n",
    "        # First Conv layer\n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = F.relu(hidden)\n",
    "        \n",
    "        # Other Conv layers with dropout at every other layer\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            hidden = conv(hidden, edge_index)\n",
    "            hidden = F.relu(hidden)\n",
    "            if i % 2 == 1:\n",
    "                hidden = self.dropout(hidden)\n",
    "\n",
    "        # Graph aggregation\n",
    "        hidden = gap(hidden, batch_index)   \n",
    "        hidden = torch.cat((hidden, x_scalar), axis=1)\n",
    "\n",
    "        # FCNN layers\n",
    "#         for i, layer in enumerate(self.fcnn_layers):\n",
    "#             hidden = layer(hidden)\n",
    "#             if i < len(self.fcnn_layers) - 1:  # Apply ReLU to all but the last layer\n",
    "#                 hidden = F.relu(hidden)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "model = GNN_with_attention(num_gcn_layers=3, num_fcnn_layers=3, gcn_embedding_size=85, fcnn_embedding_size=64,\n",
    "                 num_nodes=16, num_features=4, dropout_prob=0.2, heads=2)\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "### example implementation ###\n",
    "\n",
    "### edge index for basic connections ###\n",
    "edge_index = torch.tensor([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                           [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]])\n",
    "\n",
    "# x, edge_index, x_scalar, batch_index\n",
    "test_output = model(torch.randn(16,4),edge_index,\n",
    "                   torch.randn(1,4),torch.zeros(16).int().to(torch.int64))\n",
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a915562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 174])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "temp = torch.stack([model(torch.randn(16,4)*i,edge_index,\n",
    "                   torch.randn(1,4),torch.zeros(16).int().to(torch.int64)) for i in range(4)]).transpose(1,0)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421da7a",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad78b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  592421\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TimeSeriesTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward=2048, max_len=5000):\n",
    "        super(TimeSeriesTransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.output_projection = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        x = self.input_projection(x)  # (batch_size, seq_len, d_model)\n",
    "        x = x.transpose(0, 1)  # (seq_len, batch_size, d_model)\n",
    "        x = self.positional_encoding(x)  # (seq_len, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)  # (seq_len, batch_size, d_model)\n",
    "        x = x.transpose(0, 1)  # (batch_size, seq_len, d_model)\n",
    "        x = x.transpose(1, 2)  # (batch_size, d_model, seq_len)\n",
    "        x = self.pooling(x)  # (batch_size, d_model, 1)\n",
    "        x = x.squeeze(2)  # (batch_size, d_model)\n",
    "        x = self.output_projection(x)  # (batch_size, 1)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "# input_dim = 10  # Number of features in the time series data\n",
    "# d_model = 64    # Dimensionality of the embeddings\n",
    "# nhead = 8       # Number of attention heads\n",
    "# num_layers = 6  # Number of transformer encoder layers\n",
    "\n",
    "model = TimeSeriesTransformerEncoder(input_dim=174, d_model=66, nhead=3, num_layers=2)\n",
    "input_tensor = torch.rand(32, 5, 174)  # (batch_size, seq_len, input_dim)\n",
    "output = model(input_tensor)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
    "print(output.shape)  # (batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63354e",
   "metadata": {},
   "source": [
    "# Combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa7daca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  199102\n"
     ]
    }
   ],
   "source": [
    "class GNN_Transformer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_gcn_layers=2, num_fcnn_layers=2, gcn_embedding_size=128, fcnn_embedding_size=64,\n",
    "                 num_nodes=16, num_features=4, dropout_prob=0.2, heads=2, ### GAT parameters\n",
    "                 input_dim=132, d_model=124, nhead=4, num_layers=2, dim_feedforward=64, max_len=5): ### Transformer parameters\n",
    "        \n",
    "        super(GNN_Transformer, self).__init__()\n",
    "        self.GNN = GNN_with_attention(num_gcn_layers,num_fcnn_layers,gcn_embedding_size,fcnn_embedding_size,\n",
    "                                      num_nodes,num_features,dropout_prob,heads)\n",
    "        self.transformer = TimeSeriesTransformerEncoder(input_dim,d_model,nhead,\n",
    "                                                        num_layers,dim_feedforward,max_len)\n",
    "        \n",
    "    def forward(self,x, edge_index, x_scalar, batch_index):\n",
    "        \n",
    "        gnn_embedding = self.GNN(x, edge_index, x_scalar, batch_index)\n",
    "        print(\"gnn_embedding\",gnn_embedding.shape)\n",
    "        exit()\n",
    "        prediction = self.transformer(gnn_embedding)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "gnn_transformer = GNN_Transformer(num_gcn_layers=3, num_fcnn_layers=3, gcn_embedding_size=85, fcnn_embedding_size=64,\n",
    "                                  num_nodes=16, num_features=4, dropout_prob=0.2, heads=2,\n",
    "                                  input_dim=174, d_model=66, nhead=3, num_layers=2)\n",
    "\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in gnn_transformer.parameters()))\n",
    "\n",
    "# output = gnn_transformer(torch.randn(16,4),edge_index,\n",
    "#                    torch.randn(1,4),torch.zeros(16).int().to(torch.int64))\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec82ab-61e6-4078-ab41-43ff42a2b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap data in a data loader\n",
    "# NUM_GRAPHS_PER_BATCH = 64\n",
    "# N = 36000\n",
    "\n",
    "# train_loader = DataLoader(list(zip(train_combined[0:N],train_inputs_global[0:N])), \n",
    "#                     batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "# test_loader = DataLoader(list(zip(test_combined,test_inputs_global)), \n",
    "#                     batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6104e78-0823-4cd1-83d2-6615f8478ae2",
   "metadata": {},
   "source": [
    "Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc3d02-8dd6-4974-8134-3bb9f5d4fcf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Root mean squared error\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.00025)  \n",
    "\n",
    "# Use GPU for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "### lr scheduler ###\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "epoch_loss_train = list()\n",
    "epoch_loss_val = list()\n",
    "lr_list = list()\n",
    "\n",
    "# model.load_state_dict(torch.load(\"model_general\"))\n",
    "# model = model.to(device)\n",
    "\n",
    "save_loc = \"\"\n",
    "\n",
    "for epoch in range(0,100):\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "\n",
    "    current_loss = 0.0\n",
    "    loss_train = list()\n",
    "    loss_val = list()\n",
    "    \n",
    "    for batch,inputs_global in train_loader:\n",
    "\n",
    "        batch.to(device)\n",
    "        inputs_global = inputs_global.float().cuda() \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model( batch.x.float() , batch.edge_index, inputs_global, batch.batch)\n",
    "        \n",
    "        loss = loss_fn(pred, batch.y)\n",
    "        loss.backward()  \n",
    "        \n",
    "        # Update using the gradients\n",
    "        optimizer.step()   \n",
    "\n",
    "        current_loss += loss.item()\n",
    "        loss_train.append(loss.item())\n",
    "        \n",
    "    for batch,inputs_global in test_loader:\n",
    "\n",
    "        batch.to(device)\n",
    "        inputs_global = inputs_global.float().cuda() \n",
    "        \n",
    "        pred = model( batch.x.float() , batch.edge_index,inputs_global, batch.batch)\n",
    "        \n",
    "        loss = loss_fn(pred,batch.y)\n",
    "        loss_val.append(loss.item())\n",
    "        \n",
    "    print(f'Epoch {epoch+1} finished with training loss = '+str(np.array(loss_train).mean()))\n",
    "    print(f'testing loss = '+str(np.array(loss_val).mean()) + '\\n' )\n",
    "\n",
    "    epoch_loss_train.append(np.array(loss_train).mean())\n",
    "    epoch_loss_val.append(np.array(loss_val).mean())\n",
    "\n",
    "    ### applying lr scheduling ###\n",
    "    lr_list.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step(epoch_loss_train[-1])\n",
    "\n",
    "    if epoch%1==0:\n",
    "        \n",
    "        torch.save(model.state_dict(), save_loc+'model_'+str(epoch))\n",
    "\n",
    "    np.save(save_loc+\"epoch_loss_train\",epoch_loss_train)\n",
    "    np.save(save_loc+\"epoch_loss_val\",epoch_loss_val)\n",
    "\n",
    "print(\"Training has completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116d915-f039-4ad6-8885-56e63cacaf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(epoch_loss_train)\n",
    "plt.semilogy(epoch_loss_val)\n",
    "plt.semilogy(lr_list)\n",
    "plt.savefig(location+experiment+\"train_val_loss_vs_epochs\")\n",
    "print(epoch_loss_train[-1],epoch_loss_val[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

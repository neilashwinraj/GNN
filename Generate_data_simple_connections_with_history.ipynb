{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d15cf2-f47d-434b-84cb-94bb3c79c276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /localscratch-ssd/295146/matplotlib-uwxs15lv because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ed6a0-c522-4929-a6d4-837c2c395502",
   "metadata": {},
   "source": [
    "Generate from Ze time series dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b597fa-8c9b-4289-aa52-78fdd1691d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_periodic_coordinates(coord, size):\n",
    "    \"\"\"\n",
    "    Generate all coordinates within a cubic domain considering periodic boundary conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        coord (pandas dataframe): A pandas dataframe containing the columns (x, y, z) of a point.\n",
    "        size (int): The size of the cubic domain along each axis.\n",
    "    Returns:\n",
    "        list: A list of tuples containing all coordinates within the cubic domain.\n",
    "    \"\"\"\n",
    "    ### Keep copy of original dataframe and copy for each periodic bc shift ###\n",
    "    coord_copy = [coord.copy() for _ in range(27)]\n",
    "    stacked_df = pd.concat(coord_copy, axis=0)\n",
    "    stacked_df = stacked_df.reset_index(drop=True, inplace=False)\n",
    "    \n",
    "    # Get coordinates ###\n",
    "    if isinstance(coord, pd.DataFrame):\n",
    "        coord = coord[[\"x\",\"y\",\"z\"]].values\n",
    "\n",
    "    # Generate all combinations of displacements (-1, 0, 1) along each axis\n",
    "    displacements = list(itertools.product([-1, 0, 1], repeat=3))\n",
    "\n",
    "    # Generate all coordinates by applying periodic boundary conditions\n",
    "    tp_coordinates = list()\n",
    "    \n",
    "    for dx, dy, dz in displacements:\n",
    "          \n",
    "        temp = list()\n",
    "        \n",
    "        for i in range(len(coord)):\n",
    "            \n",
    "            x, y, z = coord[i,0],coord[i,1],coord[i,2]\n",
    "            \n",
    "            new_x = x + dx*size\n",
    "            new_y = y + dy*size\n",
    "            new_z = z + dz*size\n",
    "\n",
    "            temp.append((new_x,new_y,new_z))\n",
    "            \n",
    "        tp_coordinates.append( np.array(temp) )\n",
    "    \n",
    "    stacked_df[[\"x\",\"y\",\"z\"]] = np.vstack(tp_coordinates) \n",
    "    \n",
    "    return np.vstack(tp_coordinates),stacked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fc9653-576c-4e90-bb65-a82d652f6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_time_series_data(time_series_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Groups the data based on case_ID and time \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    ### load raw data from ze time series data ###\n",
    "    pd_list  = list()\n",
    "    \n",
    "    for (col1_val, col2_val), group in time_series_data.groupby(['case_ID', 'time']):\n",
    "    \n",
    "        pd_list.append(group)\n",
    "    \n",
    "    return pd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c4f4e0-3397-4a2a-ba51-141f5eb0b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearest_neighbor_data(time_series_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper function (in some sense, can be condensed more)to do the data generation \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    \n",
    "    pd_list = group_time_series_data(time_series_data)\n",
    "    \n",
    "    nearest_neighbor_data = list()\n",
    "    nearest_neighbor_data_extra = list()\n",
    "    scalar_data = list()\n",
    "    \n",
    "    ### Loop over different groups ###\n",
    "    \n",
    "    for i in range(len(pd_list)):\n",
    "        \n",
    "        print(\"Currently on case_time subgroup : \",str(i+1))\n",
    "        tp_particles,stacked_df = get_periodic_coordinates(pd_list[i],5)\n",
    "        tree = cKDTree(tp_particles)\n",
    "        \n",
    "        ### Loop over all particles in a group and getting the nearest neighbors ###\n",
    "        idx = np.stack([ tree.query(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values,16)[1] for j in range(len(pd_list[i])) ])\n",
    "        nearest_neighbor_data.append(tp_particles[idx])\n",
    "        \n",
    "        ### merging nodal data to the coordinates ###\n",
    "        nearest_neighbor_data_extra.append(merge_columns_to_pandas_list(tp_particles[idx],\"local_Re\",stacked_df))\n",
    "        \n",
    "        ### Getting the scalar data ###\n",
    "        scalar_data.append( pd_list[i][[\"Density_ratio\",\"glb_phi\",\"glb_Re\",\"local_Re\",\"Drag\"]] )\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    ### Populate graph and scalar lists ###\n",
    "    nearest_neighbor_data = np.stack(nearest_neighbor_data)\n",
    "    nearest_neighbor_data_extra = np.stack(nearest_neighbor_data_extra)\n",
    "    \n",
    "    nearest_neighbor_data = nearest_neighbor_data.reshape(nearest_neighbor_data.shape[0]*nearest_neighbor_data.shape[1]\n",
    "                                           ,nearest_neighbor_data.shape[2]*nearest_neighbor_data.shape[3])\n",
    "    \n",
    "    nearest_neighbor_data_extra = nearest_neighbor_data_extra.reshape(nearest_neighbor_data_extra.shape[0]*nearest_neighbor_data_extra.shape[1]\n",
    "                                           ,nearest_neighbor_data_extra.shape[2]*nearest_neighbor_data_extra.shape[3])\n",
    "    \n",
    "    scalar_data = np.stack(scalar_data)\n",
    "    scalar_data = scalar_data.reshape(scalar_data.shape[0]*scalar_data.shape[1],scalar_data.shape[2])    \n",
    "    \n",
    "    ### change code if you want to return nearest_neighbor_data or extra ### \n",
    "    return np.concatenate( (nearest_neighbor_data_extra,scalar_data) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7561cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns_to_pandas_list(nearest_neighbor_data,variable_list,master_dataframe):\n",
    "\n",
    "    \"\"\" given a list of pandas dataframe with the x,y,z locations and re and phi ,this function will\n",
    "        merge each pandas dataframe from the list with the master dataframe with all the columns  \n",
    "    \"\"\"\n",
    "\n",
    "    joined =[pd.DataFrame(nearest_neighbor_data[i],columns=[\"x\",\"y\",\"z\"]) for i in range(len(nearest_neighbor_data))]\n",
    "\n",
    "    for i in range(len(joined)):\n",
    "        \n",
    "        temp = copy.deepcopy(joined[i])\n",
    "        add = pd.merge(temp,master_dataframe,how=\"inner\",on=['x','y','z'],sort=False)[variable_list]\n",
    "        joined[i] = pd.concat([temp,add], axis=1)\n",
    "        \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e23f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_nearest_neighbor_data(nearest_neighbor_data,pd_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes nearest neighbor data and the pd_list and it will return a pandas dataframe with each row\n",
    "    having the particle ID (integer), the time step (integer) and the case (integer) of which the particle is a part of\n",
    "    ,and the remaining columns will be the nearest neighbor row itself.\n",
    "    \"\"\"\n",
    "    case_column = np.stack( [ pd_list[i][\"case_ID\"].values for i in range(len(pd_list)) ] ).flatten()\n",
    "    particle_id_column = np.stack( [ np.arange(pd_list[i].shape[0])+1 for i in range(len(pd_list)) ] ).flatten()\n",
    "    time_column = np.stack( [ pd_list[i][\"time\"].values for i in range(len(pd_list)) ] ).flatten()\n",
    "    \n",
    "    ### Combining columns with nearest_neighbor_data ###\n",
    "    nearest_neighbor_data_modified = np.concatenate( (case_column[:,None],particle_id_column[:,None],time_column[:,None]\n",
    "                ,nearest_neighbor_data),axis=1 )\n",
    "    \n",
    "    return nearest_neighbor_data_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87578c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def generate_temporally_history_datasets_for_single_group(single_df,history_length,sampling_rate):\n",
    "        \n",
    "        \"\"\"\n",
    "        performs the data generation for a single group\n",
    "        \"\"\"\n",
    "        start_index = history_length*sampling_rate\n",
    "        \n",
    "        for i in range(start_index,len(single_df)):\n",
    "            \n",
    "            extracted_sequences = [ [single_df.iloc[k - j * sampling_rate] for j in range(history_length + 1)]\n",
    "                                    for k in range(start_index, len(single_df)) ]\n",
    "                              \n",
    "        extracted_sequences = [pd.concat(series_list, axis=1).T for series_list in extracted_sequences]\n",
    "        \n",
    "        return extracted_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d4ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporally_history_datasets_for_single_group(single_df, history_length, sampling_rate):\n",
    "    \"\"\"\n",
    "    Performs the data generation for a single group\n",
    "    \"\"\"\n",
    "    start_index = history_length * sampling_rate\n",
    "    num_rows = len(single_df)\n",
    "    indices = []\n",
    "\n",
    "    # Create a list of indices to extract\n",
    "    for i in range(start_index, num_rows):\n",
    "        indices.append([i - j * sampling_rate for j in range(history_length + 1)])\n",
    "    \n",
    "    \n",
    "    # Flatten the list of indices\n",
    "    flat_indices = [index for sublist in indices for index in sublist]\n",
    "\n",
    "    # Use iloc to extract all needed rows at once\n",
    "    extracted_sequences = single_df.iloc[flat_indices]\n",
    "\n",
    "    # Reshape the dataframe to have the desired format\n",
    "    reshaped_data = []\n",
    "    for i in range(0, len(flat_indices), history_length + 1):\n",
    "        reshaped_data.append(extracted_sequences.iloc[i:i + history_length + 1].reset_index(drop=True))\n",
    "    \n",
    "    return reshaped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "305234b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporally_history_datasets(grouped_dfs,history_length=3,sampling_rate=2):\n",
    "    \n",
    "    \"\"\" \n",
    "    Given a list of pandas dataframes where each element is the temporal trajectory of one particle\n",
    "    , this functions operates on each of the elements and gives historical time data points, for instance if\n",
    "    the trajectory has 100 time steps, and the history is length is 3 with the sampling rate being 2. The first\n",
    "    data point will be of timestep 1-3-5 and the label would be the drag from timestep 5, second datapoint would be \n",
    "    2-4-6 and the label would be the drag at 6 and so on.\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_sequences = list()\n",
    "    \n",
    "    for i in range(len(grouped_dfs)):\n",
    "        \n",
    "        print(\"Currently on particle number : \",str(i+1))\n",
    "        \n",
    "        extracted_sequences.append( generate_temporally_history_datasets_for_single_group( grouped_dfs[i] , history_length \n",
    "                                                                                , sampling_rate) )\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    reversed_extracted_sequences = [[df.iloc[::-1].reset_index(drop=True) for df in sublist] for sublist in extracted_sequences]\n",
    "    \n",
    "    return reversed_extracted_sequences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8fda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_over_all_levels(data):\n",
    "    \n",
    "    # Flatten the list of lists into a single list of DataFrames\n",
    "    flattened_list = [df for sublist in data for df in sublist]\n",
    "\n",
    "    # Concatenate all DataFrames in the flattened list into a single DataFrame\n",
    "    combined_dataframe = pd.concat(flattened_list, ignore_index=True)\n",
    "    \n",
    "    return combined_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c129c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(nearest_neighbor_data_modified):\n",
    "    \n",
    "    ### Define new column names ###\n",
    "    new_column_names = {0: 'case', 1: 'particle_ID', 2: 'time',3:\"x_poi\",4:\"y_poi\",5:\"z_poi\",6:\"local Re\"}\n",
    "\n",
    "    for i in range(15):\n",
    "\n",
    "        new_column_names[len(new_column_names)] = \"x_\" + str(i + 1)\n",
    "        new_column_names[len(new_column_names)] = \"y_\" + str(i + 1)\n",
    "        new_column_names[len(new_column_names)] = \"z_\" + str(i + 1)\n",
    "        new_column_names[len(new_column_names)] = \"local_Re_\" + str(i + 1)\n",
    "\n",
    "    new_column_names[len(new_column_names)] = \"density_ratio\"\n",
    "    new_column_names[len(new_column_names)] = \"glb_phi\"\n",
    "    new_column_names[len(new_column_names)] = \"glb_Re\"\n",
    "    new_column_names[len(new_column_names)] = \"local_Re\"\n",
    "    new_column_names[len(new_column_names)] = \"Drag\"\n",
    "    \n",
    "    return nearest_neighbor_data_modified.rename(columns=new_column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a12476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_xyz_columns_and_others(df):\n",
    "#     # Initialize the MinMaxScaler\n",
    "#     scaler = MinMaxScaler()\n",
    "\n",
    "#     # Iterate over each column in the dataframe\n",
    "#     for col in df.columns:\n",
    "#         if col.startswith('x_') or col.startswith('y_') or col.startswith('z_'):\n",
    "#             # Apply the custom scaling operation to each column that starts with 'x_', 'y_', or 'z_'\n",
    "#             df[col] = (df[col] - (-5)) / (10 - (-5))\n",
    "#         else:\n",
    "#             # Collect the other columns to be scaled using MinMaxScaler\n",
    "#             df[col] = scaler.fit_transform(df[[col]])\n",
    "            \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "972fb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_xyz_and_other_columns(train_df, test_df):\n",
    "    \n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "#     scaler = RobustScaler()\n",
    "    train_df_copy,test_df_copy = train_df.copy(), test_df.copy()\n",
    "    \n",
    "    # Iterate over each column in the dataframe\n",
    "    for col in train_df_copy.columns:\n",
    "        if col.startswith('x_') or col.startswith('y_') or col.startswith('z_'):\n",
    "            # Apply the custom scaling operation to each column that starts with 'x_', 'y_', or 'z_'\n",
    "            train_df_copy[col] = (train_df_copy[col] - (-5)) / (10 - (-5))\n",
    "            test_df_copy[col] = (test_df_copy[col] - (-5)) / (10 - (-5))\n",
    "        else:\n",
    "            # Fit the scaler on the train data and transform both train and test data\n",
    "            train_df_copy[col] = scaler.fit_transform(train_df_copy[[col]])\n",
    "            test_df_copy[col] = scaler.transform(test_df_copy[[col]])\n",
    "            \n",
    "    return train_df_copy, test_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87d283b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_xyz_and_other_columns(train_df, test_df):\n",
    "    # Initialize the MinMaxScaler\n",
    "    #     scaler = RobustScaler()\n",
    "    scaler = MinMaxScaler()\n",
    "    train_df_copy, test_df_copy = train_df.copy(), test_df.copy()\n",
    "    \n",
    "    # Find the columns starting with specific prefixes\n",
    "    x_columns = [col for col in train_df.columns if col.startswith('x_')]\n",
    "    y_columns = [col for col in train_df.columns if col.startswith('y_')]\n",
    "    z_columns = [col for col in train_df.columns if col.startswith('z_')]\n",
    "    local_re_columns = [col for col in train_df.columns if col.startswith('local_Re_')]\n",
    "    vpx_columns = [col for col in train_df.columns if col.startswith('vpx_')]\n",
    "    vpy_columns = [col for col in train_df.columns if col.startswith('vpy_')]\n",
    "    vpz_columns = [col for col in train_df.columns if col.startswith('vpz_')]\n",
    "\n",
    "    # Custom scaling for x_, y_, and z_ columns\n",
    "    for col in x_columns + y_columns + z_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - (-5)) / (10 - (-5))\n",
    "        test_df_copy[col] = (test_df_copy[col] - (-5)) / (10 - (-5))\n",
    "\n",
    "    # Compute global max for local_Re_, vpx_, vpy_, and vpz_ columns from train data\n",
    "    global_max_local_re = train_df[local_re_columns].max().max()\n",
    "    global_max_vpx = train_df[vpx_columns].max().max()\n",
    "    global_max_vpy = train_df[vpy_columns].max().max()\n",
    "    global_max_vpz = train_df[vpz_columns].max().max()\n",
    "\n",
    "    # Compute global min for local_Re_, vpx_, vpy_, and vpz_ columns from train data\n",
    "    global_min_local_re = train_df[local_re_columns].min().min()\n",
    "    global_min_vpx = train_df[vpx_columns].min().min()\n",
    "    global_min_vpy = train_df[vpy_columns].min().min()\n",
    "    global_min_vpz = train_df[vpz_columns].min().min()\n",
    "\n",
    "    # Scaling for local_Re_ columns\n",
    "    for col in local_re_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_local_re) / (global_max_local_re - global_min_local_re)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_local_re) / (global_max_local_re - global_min_local_re)\n",
    "\n",
    "    # Scaling for vpx_ columns\n",
    "    for col in vpx_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_vpx) / (global_max_vpx - global_min_vpx)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_vpx) / (global_max_vpx - global_min_vpx)\n",
    "\n",
    "    # Scaling for vpy_ columns\n",
    "    for col in vpy_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_vpy) / (global_max_vpy - global_min_vpy)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_vpy) / (global_max_vpy - global_min_vpy)\n",
    "\n",
    "    # Scaling for vpz_ columns\n",
    "    for col in vpz_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_vpz) / (global_max_vpz - global_min_vpz)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_vpz) / (global_max_vpz - global_min_vpz)\n",
    "\n",
    "    # Scaling for other columns using MinMaxScaler\n",
    "    for col in train_df.columns:\n",
    "        if not (col.startswith('x_') or col.startswith('y_') or col.startswith('z_') or \n",
    "                col.startswith('local_Re_') or col.startswith('vpx_') or \n",
    "                col.startswith('vpy_') or col.startswith('vpz_')):\n",
    "            train_df_copy[col] = scaler.fit_transform(train_df_copy[[col]])\n",
    "            test_df_copy[col] = scaler.transform(test_df_copy[[col]])\n",
    "            \n",
    "    return train_df_copy, test_df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80a88cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "def plot_3d_trajectory(trajectory,color=\"red\",title=\"plot\",limits=[-5,10],middle_cube=\"big\"):\n",
    "    \"\"\"\n",
    "    Plots a 3D trajectory given a NumPy array with x, y, z coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - trajectory: A NumPy array of shape (n, 3), where n is the number of timesteps,\n",
    "                  and each row represents the [x, y, z] coordinates at a timestep.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a NumPy array\n",
    "    trajectory = np.array(trajectory)\n",
    "    \n",
    "    # Check if the input has the correct shape\n",
    "#     if trajectory.shape[1] != 3:\n",
    "#         raise ValueError(\"Input array must have shape (n, 3) where n is the number of timesteps.\")\n",
    "    \n",
    "    # Extract x, y, z coordinates\n",
    "    x = trajectory[:, 0]\n",
    "    y = trajectory[:, 1]\n",
    "    z = trajectory[:, 2]\n",
    "    \n",
    "    # Create a 3D plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the trajectory\n",
    "    ax.scatter(x[0], y[0], z[0], label='3D trajectory',c=\"red\",s=100)\n",
    "    ax.scatter(x[1:], y[1:], z[1:], label='3D trajectory',c=color,s=25,alpha=0.25)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # set limits\n",
    "    ax.set_xlim([limits[0],limits[1]])\n",
    "    ax.set_ylim([limits[0],limits[1]])\n",
    "    ax.set_zlim([limits[0],limits[1]])\n",
    "    \n",
    "    # Show legend\n",
    "    ax.legend()\n",
    "    \n",
    "    if middle_cube==\"big\":\n",
    "        # Create vertices for the cube\n",
    "        vertices = [[0, 0, 0], [5, 0, 0], [5, 5, 0], [0, 5, 0],\n",
    "                [0, 0, 5], [5, 0, 5], [5, 5, 5], [0, 5, 5]]\n",
    "    \n",
    "    if middle_cube==\"small\":\n",
    "        \n",
    "        # Create vertices for the cube\n",
    "        vertices = [[0.333, 0.333, 0.333], [0.666, 0.333, 0.333], [0.666, 0.666, 0.333], [0.333, 0.666, 0.333],\n",
    "                [0.333, 0.333, 0.666], [0.666, 0.333, 0.666], [0.666, 0.666, 0.666], [0.333, 0.666, 0.666]]\n",
    "    \n",
    "    # Define the 6 faces of the cube\n",
    "    faces = [[vertices[j] for j in [0, 1, 2, 3]],\n",
    "             [vertices[j] for j in [4, 5, 6, 7]], \n",
    "             [vertices[j] for j in [0, 1, 5, 4]], \n",
    "             [vertices[j] for j in [2, 3, 7, 6]], \n",
    "             [vertices[j] for j in [1, 2, 6, 5]], \n",
    "             [vertices[j] for j in [4, 0, 3, 7]]]\n",
    "    \n",
    "    # Create a Poly3DCollection for the cube\n",
    "    cube = Poly3DCollection(faces, alpha=0.1, facecolors='cyan')\n",
    "    \n",
    "    \n",
    "    ax.add_collection3d(cube)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0d93504-ae34-4f09-b4f2-cf14bf501100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on case_time subgroup :  414\n"
     ]
    }
   ],
   "source": [
    "# ### Read data ###\n",
    "experiment = \"rho2_30percent_Re300\"\n",
    "\n",
    "time_series_data = pd.read_csv(\"../ze_time_series_data_raw/\"+experiment+\".dat\",index_col=False)\n",
    "\n",
    "time_series_data = pd.read_csv(\"../ze_time_series_data_raw/data_with_xyz_velocities/\"+experiment+\".dat\",index_col=False)\n",
    "\n",
    "pd_list = group_time_series_data(time_series_data)\n",
    "nearest_neighbor_data = generate_nearest_neighbor_data(time_series_data)\n",
    "\n",
    "### add particle id, case and time column to the dataset ###\n",
    "nearest_neighbor_data_modified = modify_nearest_neighbor_data(nearest_neighbor_data,pd_list)\n",
    "nearest_neighbor_data_modified = pd.DataFrame(nearest_neighbor_data_modified)\n",
    "save=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c15a18b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on particle number :  144\n"
     ]
    }
   ],
   "source": [
    "### splitting the data such that each grouped df is the trajectory of a single particle across all cases ###\n",
    "### Renaming the columns\n",
    "rename_columns(nearest_neighbor_data_modified)\n",
    "\n",
    "### each element of grouped_dfs is a particle and its trajectory ###\n",
    "nearest_neighbor_data_grouped = nearest_neighbor_data_modified.groupby([\"case\",\"particle_ID\"])\n",
    "grouped_dfs = [group for _, group in nearest_neighbor_data_grouped]\n",
    "\n",
    "### IMPORTANT : generating the sequential data ###\n",
    "history_length = 5\n",
    "sampling_rate = 1\n",
    "extracted_sequences = generate_temporally_history_datasets(grouped_dfs,history_length=history_length,\n",
    "                                                           sampling_rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff523185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Drop outlier drag forces (when train data has no input drag forces)###\n",
    "# drag_forces = list()\n",
    "\n",
    "# for i in range(len(extracted_sequences)):\n",
    "#     for j in range(len(extracted_sequences[i])):\n",
    "        \n",
    "#         drag_forces.append(extracted_sequences[i][j].iloc[-1][\"Drag\"])\n",
    "\n",
    "# drag_forces = np.array(drag_forces)\n",
    "\n",
    "# idx = list()\n",
    "\n",
    "# ### Finding indices not satisfying the condition ### \n",
    "# for i in range(len(extracted_sequences)):\n",
    "#     for j in range(len(extracted_sequences[i])):\n",
    "        \n",
    "#         if np.abs(extracted_sequences[i][j][\"Drag\"].values[-1]) < np.abs(drag_forces.mean())*3:\n",
    "            \n",
    "#             idx.append([i,j])\n",
    "\n",
    "\n",
    "            \n",
    "# ### Filtering out the indices ### \n",
    "\n",
    "# print(\"number of datapoints before filterting : \",np.sum([len(extracted_sequences[i]) \n",
    "#                                                   for i in range(len(extracted_sequences))]))\n",
    "\n",
    "# extracted_sequences = [\n",
    "#     [df for j, df in enumerate(sublist) if [i, j]  in idx]\n",
    "#     for i, sublist in enumerate(extracted_sequences)\n",
    "# ]\n",
    "\n",
    "# print(\"number of datapoints before filterting : \",np.sum([len(extracted_sequences[i]) \n",
    "#                                                   for i in range(len(extracted_sequences))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "136efd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of datapoints before filterting :  29088\n",
      "number of datapoints before filterting :  28528\n"
     ]
    }
   ],
   "source": [
    "### Drop outlier drag forces (when train data HAS input drag forces,here the mean will be caluclated on the target   ###\n",
    "### drag forces , but the datapoints will be rejected if any of the drags in the input sequence will not satisfy the ###\n",
    "### the condition ###\n",
    "\n",
    "drag_forces = list()\n",
    "\n",
    "for i in range(len(extracted_sequences)):\n",
    "    for j in range(len(extracted_sequences[i])):\n",
    "        \n",
    "        drag_forces.append(extracted_sequences[i][j].iloc[-1][\"Drag\"])\n",
    "\n",
    "drag_forces = np.array(drag_forces)\n",
    "\n",
    "idx = list()\n",
    "\n",
    "### Finding indices not satisfying the condition ### \n",
    "for i in range(len(extracted_sequences)):\n",
    "    for j in range(len(extracted_sequences[i])):\n",
    "        \n",
    "            \n",
    "        if np.all( np.abs(extracted_sequences[i][j][\"Drag\"].values) < np.abs(drag_forces.mean())*3 ):\n",
    "                                                                         \n",
    "            idx.append([i,j])\n",
    "\n",
    "            \n",
    "### Filtering out the indices ### \n",
    "\n",
    "print(\"number of datapoints before filterting : \",np.sum([len(extracted_sequences[i]) \n",
    "                                                  for i in range(len(extracted_sequences))]))\n",
    "\n",
    "extracted_sequences = [\n",
    "    [df for j, df in enumerate(sublist) if [i, j]  in idx]\n",
    "    for i, sublist in enumerate(extracted_sequences)\n",
    "]\n",
    "\n",
    "print(\"number of datapoints before filterting : \",np.sum([len(extracted_sequences[i]) \n",
    "                                                  for i in range(len(extracted_sequences))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88195b4e",
   "metadata": {},
   "source": [
    "# Train Val Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f6ae2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For splitting data as train and test (Random sample particle Datasplit) ###\n",
    "### define train/test indices ###\n",
    "# test_particles_id = np.random.randint(0,len(extracted_sequences)-1,7)\n",
    "# train_particles_id = np.setdiff1d(np.arange(len(extracted_sequences)),test_particles_id)\n",
    "\n",
    "### extracting the train and test datasets ###\n",
    "# train_data_pd = concat_over_all_levels([extracted_sequences[i] for i in train_particles_id])\n",
    "# test_data_pd = concat_over_all_levels([extracted_sequences[i] for i in test_particles_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2667cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For splitting data as train and test (half time Datasplit) ###\n",
    "import pandas as pd\n",
    "\n",
    "# Example data setup (assuming df_list_of_lists is your list of list of DataFrames)\n",
    "# df_list_of_lists = [[df1, df2], [df3, df4], ...]\n",
    "\n",
    "# Define your time limits for each case\n",
    "half_time_1 = int(grouped_dfs[0].time.values[-1]*0.80)  # Replace with your actual limit for case_1\n",
    "half_time_2 = int(grouped_dfs[-1].time.values[-1]*0.80)  # Replace with your actual limit for case_2\n",
    "\n",
    "# Initialize the lists to collect pairs (i, j)\n",
    "train_pairs = []\n",
    "test_pairs = []\n",
    "\n",
    "# Iterate through the outer and inner lists\n",
    "for i, outer_list in enumerate(extracted_sequences):\n",
    "    \n",
    "    for j, df in enumerate(outer_list):\n",
    "        # Check the case and apply the appropriate time filter\n",
    "        if 'case' in df.columns and 'time' in df.columns:\n",
    "            \n",
    "            ### Train Pairs ###\n",
    "            if all(df[\"case\"].values==1) & all(df[\"time\"].values<=half_time_1):\n",
    "                train_pairs.append([i, j])\n",
    "                                   \n",
    "            if all(df[\"case\"].values==2) & all(df[\"time\"].values<=half_time_2):\n",
    "                train_pairs.append([i, j])\n",
    "                                                          \n",
    "            ### Test Pairs ###\n",
    "            if all(df[\"case\"].values==1) & any(df[\"time\"].values>half_time_1):\n",
    "                test_pairs.append([i, j])\n",
    "                                   \n",
    "            if all(df[\"case\"].values==2) & any(df[\"time\"].values>half_time_2):\n",
    "                test_pairs.append([i, j])\n",
    "                \n",
    "train_dataset = pd.concat([extracted_sequences[i][j] for (i, j) in train_pairs])\n",
    "test_dataset = pd.concat([extracted_sequences[i][j] for (i, j) in test_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1ae5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for POI based coordinates ### \n",
    "for col in train_dataset.columns:\n",
    "    \n",
    "    if col.startswith(\"x_\") and not col.endswith(\"_poi\"):\n",
    "        train_dataset[col] -= train_dataset[\"x_poi\"]\n",
    "        \n",
    "    if col.startswith(\"y_\") and not col.endswith(\"_poi\"):\n",
    "        train_dataset[col] -= train_dataset[\"y_poi\"]\n",
    "        \n",
    "    if col.startswith(\"z_\") and not col.endswith(\"_poi\"):\n",
    "        train_dataset[col] -= train_dataset[\"z_poi\"]\n",
    "        \n",
    "for col in test_dataset.columns:\n",
    "    \n",
    "    if col.startswith(\"x_\") and not col.endswith(\"_poi\"):\n",
    "        test_dataset[col] -= test_dataset[\"x_poi\"]\n",
    "        \n",
    "    if col.startswith(\"y_\") and not col.endswith(\"_poi\"): \n",
    "        test_dataset[col] -= test_dataset[\"y_poi\"]\n",
    "        \n",
    "    if col.startswith(\"z_\") and not col.endswith(\"_poi\"): \n",
    "        test_dataset[col] -= test_dataset[\"z_poi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80afcef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_particle_id = test_dataset[[\"case\",\"particle_ID\",\"time\"]]\n",
    "# case_particle_id.to_csv(\"/home/neilashwinraj/gnns/volatile/test_dataset_identifiers\")\n",
    "\n",
    "# case_particle_id_np = np.array(case_particle_id.values)\n",
    "# n_test = case_particle_id_np.shape[0]//(extracted_sequences[0][0].shape[0])\n",
    "# case_particle_id_np  = np.array_split(case_particle_id_np,n_test,axis=0)\n",
    "\n",
    "# np.save(\"/home/neilashwinraj/gnns/volatile/test_dataset_identifiers_np\",case_particle_id_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dd3fccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-103.509113, 110.135844)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"Drag\"].values.min(),train_dataset[\"Drag\"].values.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e6b70",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2701e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scaling the data ###\n",
    "train_dataset_scaled, test_dataset_scaled = scale_xyz_and_other_columns(train_dataset,test_dataset)\n",
    "\n",
    "### if poi coordinates (now doing this as default)###\n",
    "### Set poi to zeros ###\n",
    "train_dataset_scaled[[\"x_poi\",\"y_poi\",\"z_poi\"]] = 0\n",
    "test_dataset_scaled[[\"x_poi\",\"y_poi\",\"z_poi\"]] = 0\n",
    "\n",
    "### Reshaping by chunking the data ### \n",
    "n_train = train_dataset.shape[0]//(extracted_sequences[0][0].shape[0])\n",
    "train_dataset_scaled = np.array_split(train_dataset_scaled.values[:,3:],n_train,axis=0)\n",
    "\n",
    "n_test = test_dataset.shape[0]//(extracted_sequences[0][0].shape[0])\n",
    "test_dataset_scaled  = np.array_split(test_dataset_scaled.values[:,3:],n_test,axis=0)\n",
    "\n",
    "### if inputs with drag forces, make the output (last drag in each segment as zero) extract it and save it as output ###\n",
    "### train and test drag forces ###\n",
    "\n",
    "train_drag_forces_scaled = list()\n",
    "for i in range(len(train_dataset_scaled)):\n",
    "    train_drag_forces_scaled.append(train_dataset_scaled[i][-1][-1])\n",
    "    train_dataset_scaled[i][-1][-1] = 0 \n",
    "    \n",
    "train_drag_forces_scaled = np.stack(train_drag_forces_scaled)\n",
    "\n",
    "test_drag_forces_scaled = list()\n",
    "for i in range(len(test_dataset_scaled)):\n",
    "    test_drag_forces_scaled.append(test_dataset_scaled[i][-1][-1])\n",
    "    test_dataset_scaled[i][-1][-1] = 0 \n",
    "    \n",
    "test_drag_forces_scaled = np.stack(test_drag_forces_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drags auxilliary ###\n",
    "train_drag_forces_unscaled = np.stack( [extracted_sequences[i][j][\"Drag\"].values[-1] for (i, j) in train_pairs] )[:,None]\n",
    "test_drag_forces_unscaled = np.stack( [extracted_sequences[i][j][\"Drag\"].values[-1] for (i, j) in test_pairs] )[:,None]\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_drag_forces_unscaled)\n",
    "\n",
    "train_drag_forces_scaled = scaler.transform(train_drag_forces_unscaled)\n",
    "test_drag_forces_scaled = scaler.transform(test_drag_forces_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b415320",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the raw data trajectory ###\n",
    "single_particle_trajectory = grouped_dfs[0].values[:,3:]\n",
    "k=0\n",
    "\n",
    "# for i in range(len(single_particle_trajectory)):\n",
    "for i in range(0,len(single_particle_trajectory),sampling_rate):\n",
    "    \n",
    "    if k==6:\n",
    "        break\n",
    "    \n",
    "    title = \" time step number : \"+str(i+10)\n",
    "    plot_3d_trajectory(single_particle_trajectory[i+10][0:64].reshape(16,4),color=\"green\",title = title,limits=[-5,10])\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd2346",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plotting the input data trajectory ###\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(6):\n",
    "        \n",
    "        plot_3d_trajectory(train_dataset_scaled[i+10][j][0:64].reshape(16,4),color=\"green\",title = title ,limits=[0,1],middle_cube=\"small\")\n",
    "        clear_output(wait=True)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892bb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the list to a pickle file (unscaled)###\n",
    "\n",
    "# ### Train data ###\n",
    "# with open(\"simple_connections_data/temporal_split/\"+experiment+'/train_data_np_unscaled.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_data_np_scaled, f)\n",
    "    \n",
    "# ### Test data ### \n",
    "# with open(\"simple_connections_data/temporal_split/\"+experiment+'/test_data_np_unscaled.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_data_np_scaled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the list to a pickle file (scaled)###\n",
    "\n",
    "### Train data ###\n",
    "with open(\"simple_connections_data/temporal_split/\"+experiment+'/data_inputs_with_drag_minmax_scaler/train_data_np_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dataset_scaled, f)\n",
    "    \n",
    "### Test data ### \n",
    "with open(\"simple_connections_data/temporal_split/\"+experiment+'/data_inputs_with_drag_minmax_scaler/test_data_np_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dataset_scaled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a527a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save drag forces (scaled and unscaled)###\n",
    "\n",
    "### Train data ###\n",
    "# np.save(\"simple_connections_data/temporal_split/\"+experiment+'/data_inputs_with_drag_minmax_scaler/train_output_unscaled',train_drag_forces_unscaled)\n",
    "np.save(\"simple_connections_data/temporal_split/\"+experiment+'/data_inputs_with_drag_minmax_scaler/train_output_scaled',train_drag_forces_scaled)\n",
    "\n",
    "### Test data ###\n",
    "# np.save(\"simple_connections_data/temporal_split/\"+experiment+'/data_inputs_with_drag_minmax_scaler/test_output_unscaled',test_drag_forces_unscaled)\n",
    "np.save(\"simple_connections_data/temporal_split/\"+experiment+'/data_inputs_with_drag_minmax_scaler/test_output_scaled',test_drag_forces_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee63684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             case_1_particles_train = df[(df['case'] == 1) & (df['time'] <= time_1)]\n",
    "#             case_2_particles_train = df[(df['case'] == 2) & (df['time'] <= time_2)]\n",
    "#             case_1_particles_test = df[(df['case'] == 1) & (df['time'] > time_1)]\n",
    "#             case_2_particles_test = df[(df['case'] == 2) & (df['time'] > time_2)]\n",
    "            \n",
    "            # If the DataFrame has any rows after filtering for training, add the pair (i, j) to train_pairs\n",
    "#             if not case_1_particles_train.empty or not case_2_particles_train.empty:\n",
    "#                 train_pairs.append((i, j))\n",
    "\n",
    "            # If the DataFrame has any rows after filtering for testing, add the pair (i, j) to test_pairs\n",
    "#             if not case_1_particles_test.empty or not case_2_particles_test.empty:\n",
    "#                 test_pairs.append((i, j))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

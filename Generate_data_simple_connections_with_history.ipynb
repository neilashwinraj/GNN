{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d15cf2-f47d-434b-84cb-94bb3c79c276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /localscratch-ssd/288604/matplotlib-ju_vrbj2 because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ed6a0-c522-4929-a6d4-837c2c395502",
   "metadata": {},
   "source": [
    "Generate from Ze time series dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b597fa-8c9b-4289-aa52-78fdd1691d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_periodic_coordinates(coord, size):\n",
    "    \"\"\"\n",
    "    Generate all coordinates within a cubic domain considering periodic boundary conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        coord (pandas dataframe): A pandas dataframe containing the columns (x, y, z) of a point.\n",
    "        size (int): The size of the cubic domain along each axis.\n",
    "    Returns:\n",
    "        list: A list of tuples containing all coordinates within the cubic domain.\n",
    "    \"\"\"\n",
    "    ### Keep copy of original dataframe and copy for each periodic bc shift ###\n",
    "    coord_copy = [coord.copy() for _ in range(27)]\n",
    "    stacked_df = pd.concat(coord_copy, axis=0)\n",
    "    stacked_df = stacked_df.reset_index(drop=True, inplace=False)\n",
    "    \n",
    "    # Get coordinates ###\n",
    "    if isinstance(coord, pd.DataFrame):\n",
    "        coord = coord[[\"x\",\"y\",\"z\"]].values\n",
    "\n",
    "    # Generate all combinations of displacements (-1, 0, 1) along each axis\n",
    "    displacements = list(itertools.product([-1, 0, 1], repeat=3))\n",
    "\n",
    "    # Generate all coordinates by applying periodic boundary conditions\n",
    "    tp_coordinates = list()\n",
    "    \n",
    "    for dx, dy, dz in displacements:\n",
    "          \n",
    "        temp = list()\n",
    "        \n",
    "        for i in range(len(coord)):\n",
    "            \n",
    "            x, y, z = coord[i,0],coord[i,1],coord[i,2]\n",
    "            \n",
    "            new_x = x + dx*size\n",
    "            new_y = y + dy*size\n",
    "            new_z = z + dz*size\n",
    "\n",
    "            temp.append((new_x,new_y,new_z))\n",
    "            \n",
    "        tp_coordinates.append( np.array(temp) )\n",
    "    \n",
    "    stacked_df[[\"x\",\"y\",\"z\"]] = np.vstack(tp_coordinates) \n",
    "    \n",
    "    return np.vstack(tp_coordinates),stacked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fc9653-576c-4e90-bb65-a82d652f6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_time_series_data(time_series_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Groups the data based on case_ID and time \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    ### load raw data from ze time series data ###\n",
    "    pd_list  = list()\n",
    "    \n",
    "    for (col1_val, col2_val), group in time_series_data.groupby(['case_ID', 'time']):\n",
    "    \n",
    "        pd_list.append(group)\n",
    "    \n",
    "    return pd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c4f4e0-3397-4a2a-ba51-141f5eb0b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearest_neighbor_data(time_series_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper function (in some sense, can be condensed more)to do the data generation \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    \n",
    "    pd_list = group_time_series_data(time_series_data)\n",
    "    \n",
    "    nearest_neighbor_data = list()\n",
    "    nearest_neighbor_data_extra = list()\n",
    "    scalar_data = list()\n",
    "    \n",
    "    ### Loop over different groups ###\n",
    "    \n",
    "    for i in range(len(pd_list)):\n",
    "        \n",
    "        print(\"Currently on case_time subgroup : \",str(i+1))\n",
    "        tp_particles,stacked_df = get_periodic_coordinates(pd_list[i],5)\n",
    "        tree = cKDTree(tp_particles)\n",
    "        \n",
    "        ### Loop over all particles in a group and getting the nearest neighbors ###\n",
    "        idx = np.stack([ tree.query(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values,16)[1] for j in range(len(pd_list[i])) ])\n",
    "        nearest_neighbor_data.append(tp_particles[idx])\n",
    "        \n",
    "        ### merging nodal data to the coordinates ###\n",
    "        nearest_neighbor_data_extra.append(merge_columns_to_pandas_list(tp_particles[idx],\"local_Re\",stacked_df))\n",
    "        \n",
    "        ### Getting the scalar data ###\n",
    "        scalar_data.append( pd_list[i][[\"Density_ratio\",\"glb_phi\",\"glb_Re\",\"local_Re\",\"Drag\"]] )\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    ### Populate graph and scalar lists ###\n",
    "    nearest_neighbor_data = np.stack(nearest_neighbor_data)\n",
    "    nearest_neighbor_data_extra = np.stack(nearest_neighbor_data_extra)\n",
    "    \n",
    "    nearest_neighbor_data = nearest_neighbor_data.reshape(nearest_neighbor_data.shape[0]*nearest_neighbor_data.shape[1]\n",
    "                                           ,nearest_neighbor_data.shape[2]*nearest_neighbor_data.shape[3])\n",
    "    \n",
    "    nearest_neighbor_data_extra = nearest_neighbor_data_extra.reshape(nearest_neighbor_data_extra.shape[0]*nearest_neighbor_data_extra.shape[1]\n",
    "                                           ,nearest_neighbor_data_extra.shape[2]*nearest_neighbor_data_extra.shape[3])\n",
    "    \n",
    "    scalar_data = np.stack(scalar_data)\n",
    "    scalar_data = scalar_data.reshape(scalar_data.shape[0]*scalar_data.shape[1],scalar_data.shape[2])    \n",
    "    \n",
    "    ### change code if you want to return nearest_neighbor_data or extra ### \n",
    "    return np.concatenate( (nearest_neighbor_data_extra,scalar_data) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7561cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns_to_pandas_list(nearest_neighbor_data,variable_list,master_dataframe):\n",
    "\n",
    "    \"\"\" given a list of pandas dataframe with the x,y,z locations and re and phi ,this function will\n",
    "        merge each pandas dataframe from the list with the master dataframe with all the columns  \n",
    "    \"\"\"\n",
    "\n",
    "    joined =[pd.DataFrame(nearest_neighbor_data[i],columns=[\"x\",\"y\",\"z\"]) for i in range(len(nearest_neighbor_data))]\n",
    "\n",
    "    for i in range(len(joined)):\n",
    "        \n",
    "        temp = copy.deepcopy(joined[i])\n",
    "        add = pd.merge(temp,master_dataframe,how=\"inner\",on=['x','y','z'],sort=False)[variable_list]\n",
    "        joined[i] = pd.concat([temp,add], axis=1)\n",
    "        \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e23f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_nearest_neighbor_data(nearest_neighbor_data,pd_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes nearest neighbor data and the pd_list and it will return a pandas dataframe with each row\n",
    "    having the particle ID (integer), the time step (integer) and the case (integer) of which the particle is a part of\n",
    "    ,and the remaining columns will be the nearest neighbor row itself.\n",
    "    \"\"\"\n",
    "    case_column = np.stack( [ pd_list[i][\"case_ID\"].values for i in range(len(pd_list)) ] ).flatten()\n",
    "    particle_id_column = np.stack( [ np.arange(pd_list[i].shape[0])+1 for i in range(len(pd_list)) ] ).flatten()\n",
    "    time_column = np.stack( [ pd_list[i][\"time\"].values for i in range(len(pd_list)) ] ).flatten()\n",
    "    \n",
    "    ### Combining columns with nearest_neighbor_data ###\n",
    "    nearest_neighbor_data_modified = np.concatenate( (case_column[:,None],particle_id_column[:,None],time_column[:,None]\n",
    "                ,nearest_neighbor_data),axis=1 )\n",
    "    \n",
    "    return nearest_neighbor_data_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "87578c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def generate_temporally_history_datasets_for_single_group(single_df,history_length,sampling_rate):\n",
    "        \n",
    "        \"\"\"\n",
    "        performs the data generation for a single group\n",
    "        \"\"\"\n",
    "        start_index = history_length*sampling_rate\n",
    "        \n",
    "        for i in range(start_index,len(single_df)):\n",
    "            \n",
    "            extracted_sequences = [ [single_df.iloc[k - j * sampling_rate] for j in range(history_length + 1)]\n",
    "                                    for k in range(start_index, len(single_df)) ]\n",
    "                              \n",
    "        extracted_sequences = [pd.concat(series_list, axis=1).T for series_list in extracted_sequences]\n",
    "        \n",
    "        return extracted_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "305234b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporally_history_datasets(grouped_dfs,history_length=3,sampling_rate=2):\n",
    "    \n",
    "    \"\"\" \n",
    "    Given a list of pandas dataframes where each element is the temporal trajectory of one particle\n",
    "    , this functions operates on each of the elements and gives historical time data points, for instance if\n",
    "    the trajectory has 100 time steps, and the history is length is 3 with the sampling rate being 2. The first\n",
    "    data point will be of timestep 1-3-5 and the label would be the drag from timestep 5, second datapoint would be \n",
    "    2-4-6 and the label would be the drag at 6 and so on.\n",
    "    \"\"\"\n",
    "    \n",
    "    def generate_temporally_history_datasets_for_single_group(single_df,history_length,sampling_rate):\n",
    "        \n",
    "        \"\"\"\n",
    "        performs the data generation for a single group\n",
    "        \"\"\"\n",
    "        start_index = history_length*sampling_rate\n",
    "        \n",
    "        for i in range(start_index,len(single_df)):\n",
    "            \n",
    "            extracted_sequences = [ [single_df.iloc[k - j * sampling_rate] for j in range(history_length + 1)]\n",
    "                                    for k in range(start_index, len(single_df)) ]\n",
    "                              \n",
    "        extracted_sequences = [pd.concat(series_list, axis=1).T for series_list in extracted_sequences]\n",
    "        \n",
    "        return extracted_sequences\n",
    "    \n",
    "    extracted_sequences = list()\n",
    "    \n",
    "    for i in range(len(grouped_dfs)):\n",
    "        \n",
    "        print(\"Currently on particle number : \",str(i+1))\n",
    "        \n",
    "        extracted_sequences.append( generate_temporally_history_datasets_for_single_group( grouped_dfs[i] , history_length \n",
    "                                                                                , sampling_rate) )\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    return extracted_sequences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2e8fda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_over_all_levels(data):\n",
    "    \n",
    "    # Flatten the list of lists into a single list of DataFrames\n",
    "    flattened_list = [df for sublist in data for df in sublist]\n",
    "\n",
    "    # Concatenate all DataFrames in the flattened list into a single DataFrame\n",
    "    combined_dataframe = pd.concat(flattened_list, ignore_index=True)\n",
    "    \n",
    "    return combined_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d93504-ae34-4f09-b4f2-cf14bf501100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on case_time subgroup :  323\n"
     ]
    }
   ],
   "source": [
    "# ### Read data ###\n",
    "experiment = \"rho2_40percent_Re100\"\n",
    "time_series_data = pd.read_csv(\"../ze_time_series_data_raw/\"+experiment+\".dat\")\n",
    "\n",
    "pd_list = group_time_series_data(time_series_data)\n",
    "nearest_neighbor_data = generate_nearest_neighbor_data(time_series_data)\n",
    "\n",
    "### add particle id, case and time column to the dataset ###\n",
    "nearest_neighbor_data_modified = modify_nearest_neighbor_data(nearest_neighbor_data,pd_list)\n",
    "save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c15a18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### splitting the data such that each grouped df is the trajectory of a single particle across all cases ###\n",
    "nearest_neighbor_data_modified = pd.DataFrame(nearest_neighbor_data_modified)\n",
    "\n",
    "new_column_names = {0: 'case', 1: 'particle_ID', 2: 'time',3:\"x\",4:\"y\",5:\"z\",6:\"local Re\"}\n",
    "nearest_neighbor_data_modified.rename(columns=new_column_names, inplace=True)\n",
    "nearest_neighbor_data_modified = nearest_neighbor_data_modified.groupby([\"case\",\"particle_ID\"])\n",
    "\n",
    "### each element of grouped_dfs is a particle and its trajectory ###\n",
    "grouped_dfs = [group for _, group in nearest_neighbor_data_modified]\n",
    "\n",
    "### IMPORTANT : generating the sequnetial data ###\n",
    "extracted_sequences = generate_temporally_history_datasets(grouped_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3f6ae2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For splitting data as train and test ###\n",
    "\n",
    "### define train/test indices ###\n",
    "test_particles_id = np.random.randint(0,len(extracted_sequences)-1,35)\n",
    "train_particles_id =np.setdiff1d(np.arange(len(extracted_sequences)),test_particles_id)\n",
    "\n",
    "### extracting the train and test datasets ###\n",
    "train_data = concat_over_all_levels([extracted_sequences[i] for i in train_particles_id])\n",
    "test_data = concat_over_all_levels([extracted_sequences[i] for i in test_particles_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e12f9673-4a5f-4009-a1e0-64618c69bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scaling the data ###\n",
    "### Inputs ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data.values)\n",
    "\n",
    "train_input_scaled = scaler.transform(train_data.values)\n",
    "test_input_scaled = scaler.transform(test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d6913b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99704, 72)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8b97fb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24926"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of parts needed\n",
    "num_parts = int(np.ceil(len(train_input_scaled) / 4))\n",
    "# Split the array\n",
    "parts = np.array_split(train_input_scaled, num_parts)\n",
    "\n",
    "len(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7bc66c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 72)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save==True:\n",
    "    \n",
    "    ### Creat Directory ###\n",
    "    directory = \"simple_connections_data/random_split/\"+experiment\n",
    "    \n",
    "    ### if directory does not exist ###\n",
    "    if not os.path.exists(directory):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        if os.path.exists(directory):\n",
    "            print(f\"Directory '{directory}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to create directory '{directory}'.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/train_inputs\",train_input_scaled.reshape(train_input_scaled.shape[0],16,4))\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/test_inputs\",test_input_scaled.reshape(test_input_scaled.shape[0],16,4))\n",
    "\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/train_input_scalar\",train_output_scaled[:,0:4])\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/test_input_scalar\",test_output_scaled[:,0:4])\n",
    "\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/train_output\",train_output_scaled[:,4:])\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/test_output\",test_output_scaled[:,4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45725c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the data as test and train (Case-wise) ###\n",
    "X_train, X_test = nearest_neighbor_data[0:change_point,0:64],nearest_neighbor_data[change_point:,0:64]\n",
    "y_train, y_test = nearest_neighbor_data[0:change_point,64:],nearest_neighbor_data[change_point:,64:]\n",
    "\n",
    "### Scaling the data ###\n",
    "### Inputs ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "train_input_scaled = scaler.transform(X_train)\n",
    "test_input_scaled = scaler.transform(X_test)\n",
    "\n",
    "### Scalar Inputs and outputs (both are under outputs and thus needs to be separated during saving) ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train)\n",
    "\n",
    "train_output_scaled = scaler.transform(y_train)\n",
    "test_output_scaled = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save==True:\n",
    "    \n",
    "        ### Creat Directory ###\n",
    "    directory = \"simple_connections_data/case_split/\"+experiment\n",
    "    \n",
    "    ### if directory does not exist ###\n",
    "    if not os.path.exists(directory):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        if os.path.exists(directory):\n",
    "            print(f\"Directory '{directory}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to create directory '{directory}'.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/train_inputs\",train_input_scaled.reshape(train_input_scaled.shape[0],16,4))\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/test_inputs\",test_input_scaled.reshape(test_input_scaled.shape[0],16,4))\n",
    "\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/train_input_scalar\",train_output_scaled[:,0:4])\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/test_input_scalar\",test_output_scaled[:,0:4])\n",
    "\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/train_output\",train_output_scaled[:,4:])\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/test_output\",test_output_scaled[:,4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59726d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### mid-time splitting ###\n",
    "# ### Splitting the data as test and train (Splitting each case into two halves) ###\n",
    "\n",
    "# ### Features Train and Test ###\n",
    "X_train_1,X_train_2 = nearest_neighbor_data[0:change_point//2,0:64],nearest_neighbor_data[change_point:change_point_2,0:64]\n",
    "X_train = np.concatenate((X_train_1,X_train_2),axis=0)\n",
    "\n",
    "X_test_1,X_test_2 = nearest_neighbor_data[change_point//2:change_point,0:64],nearest_neighbor_data[change_point_2:,0:64]\n",
    "X_test = np.concatenate((X_test_1,X_test_2),axis=0)\n",
    "\n",
    "### Labels Train and Test ###\n",
    "y_train_1,y_train_2 = nearest_neighbor_data[0:change_point//2,64:],nearest_neighbor_data[change_point:change_point_2,64:]\n",
    "y_train = np.concatenate((y_train_1,y_train_2),axis=0)\n",
    "\n",
    "y_test_1,y_test_2 = nearest_neighbor_data[change_point//2:change_point,64:],nearest_neighbor_data[change_point_2:,64:]\n",
    "y_test = np.concatenate((y_test_1,y_test_2),axis=0)\n",
    "\n",
    "### Scaling the data ###\n",
    "### Inputs ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "train_input_scaled = scaler.transform(X_train)\n",
    "test_input_scaled = scaler.transform(X_test)\n",
    "\n",
    "### Scalar Inputs and outputs (both are under outputs and thus needs to be separated during saving) ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train)\n",
    "\n",
    "train_output_scaled = scaler.transform(y_train)\n",
    "test_output_scaled = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save==True:\n",
    "    \n",
    "    ### Creat Directory ###\n",
    "    directory = \"simple_connections_data/time_split/\"+experiment\n",
    "    \n",
    "    ### if directory does not exist ###\n",
    "    if not os.path.exists(directory):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        if os.path.exists(directory):\n",
    "            print(f\"Directory '{directory}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to create directory '{directory}'.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/train_inputs\",train_input_scaled.reshape(train_input_scaled.shape[0],16,4))\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/test_inputs\",test_input_scaled.reshape(test_input_scaled.shape[0],16,4))\n",
    "\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/train_input_scalar\",train_output_scaled[:,0:4])\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/test_input_scalar\",test_output_scaled[:,0:4])\n",
    "\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/train_output\",train_output_scaled[:,4:])\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/test_output\",test_output_scaled[:,4:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

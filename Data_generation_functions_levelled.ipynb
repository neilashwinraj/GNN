{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b87856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /localscratch-ssd/295289/matplotlib-5qf32zov because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import MoleculeNet\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from torch import nn\n",
    "import torch_geometric.nn as geom_nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01270d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_periodic_coordinates(coord, size):\n",
    "    \"\"\"\n",
    "    Generate all coordinates within a cubic domain considering periodic boundary conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        coord (pandas dataframe): A pandas dataframe containing the columns (x, y, z) of a point.\n",
    "        size (int): The size of the cubic domain along each axis.\n",
    "    Returns:\n",
    "        list: A list of tuples containing all coordinates within the cubic domain.\n",
    "    \"\"\"\n",
    "    ### make a copy of coord ###\n",
    "    coord_copy = coord.copy()\n",
    "    coord_copy = [coord_copy] * 27\n",
    "    coord_copy = pd.concat(coord_copy, ignore_index=True)\n",
    "    \n",
    "    # Get coordinates ###\n",
    "    if isinstance(coord, pd.DataFrame):\n",
    "        coord = coord[[\"x\",\"y\",\"z\"]].values\n",
    "\n",
    "    # Generate all combinations of displacements (-1, 0, 1) along each axis\n",
    "    displacements = list(itertools.product([-1, 0, 1], repeat=3))\n",
    "\n",
    "    # Generate all coordinates by applying periodic boundary conditions\n",
    "    tp_coordinates = list()\n",
    "    \n",
    "    for dx, dy, dz in displacements:\n",
    "        \n",
    "        temp = list()\n",
    "        \n",
    "        for i in range(len(coord)):\n",
    "            \n",
    "            x, y, z = coord[i,0],coord[i,1],coord[i,2]  \n",
    "\n",
    "            new_x = x + dx*size\n",
    "            new_y = y + dy*size\n",
    "            new_z = z + dz*size\n",
    "\n",
    "            temp.append((new_x,new_y,new_z))\n",
    "            \n",
    "        tp_coordinates.append( np.array(temp) )\n",
    "    \n",
    "    coord_copy[[\"x\",\"y\",\"z\"]] = np.vstack(tp_coordinates)\n",
    "    \n",
    "    return np.vstack(tp_coordinates),coord_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a82ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_time_series_data(time_series_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Groups the data based on case_ID and time \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    ### load raw data from ze time series data ###\n",
    "    pd_list  = list()\n",
    "    \n",
    "    for (col1_val, col2_val), group in time_series_data.groupby(['case_ID', 'time']):\n",
    "    \n",
    "        pd_list.append(group)\n",
    "    \n",
    "    return pd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a06b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearest_neighbor_data(time_series_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper function (in some sense, can be condensed more)to do the data generation \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    \n",
    "    pd_list = group_time_series_data(time_series_data)\n",
    "    \n",
    "    nearest_neighbor_data = list()\n",
    "    scalar_data = list()\n",
    "    \n",
    "    ### Loop over different groups ###\n",
    "    \n",
    "    for i in range(len(pd_list)):\n",
    "\n",
    "        tp_particles = get_periodic_coordinates(pd_list[i],5)\n",
    "        tree = cKDTree(tp_particles)\n",
    "\n",
    "        ### Loop over all particles in a group and getting the nearest neighbors ###\n",
    "        idx = np.stack([ tree.query(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values,16)[1] for j in range(len(pd_list[i])) ])\n",
    "        nearest_neighbor_data.append(tp_particles[idx])\n",
    "\n",
    "        ### Getting the scalar data ###\n",
    "        scalar_data.append( pd_list[i][[\"Density_ratio\",\"glb_phi\",\"glb_Re\",\"local_Re\",\"Drag\"]] )\n",
    "\n",
    "    nearest_neighbor_data = np.stack(nearest_neighbor_data)\n",
    "    nearest_neighbor_data = nearest_neighbor_data.reshape(nearest_neighbor_data.shape[0]*nearest_neighbor_data.shape[1]\n",
    "                                           ,nearest_neighbor_data.shape[2]*nearest_neighbor_data.shape[3])\n",
    "    \n",
    "    scalar_data = np.stack(scalar_data)\n",
    "    scalar_data = scalar_data.reshape(scalar_data.shape[0]*scalar_data.shape[1],scalar_data.shape[2])    \n",
    "    \n",
    "    return np.concatenate( (nearest_neighbor_data,scalar_data) ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2272a73",
   "metadata": {},
   "source": [
    "# Neigh Level Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdaf7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_difference(array1,array2):\n",
    "    \n",
    "    ### First one must be the bigger array ###\n",
    "    set1 = set(map(tuple, array1))\n",
    "    set2 = set(map(tuple, array2))\n",
    "    \n",
    "    # Find the set difference\n",
    "    set_difference = set1 - set2\n",
    "\n",
    "    # Convert the set difference back to a NumPy array\n",
    "    return np.array(list(set_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ad9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_2d(array):\n",
    "    \"\"\"\n",
    "    Reshape an n-dimensional NumPy array to a 2D array, flattening all dimensions except the last one.\n",
    "\n",
    "    Parameters:\n",
    "        array (numpy.ndarray): Input n-dimensional array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Reshaped 2D array.\n",
    "    \"\"\"\n",
    "    # Flatten all dimensions except the last one\n",
    "    new_shape_first_dim = np.prod(array.shape[:-1])\n",
    "    \n",
    "    # Keep the last dimension intact\n",
    "    new_shape_second_dim = array.shape[-1]\n",
    "\n",
    "    # Reshape the array\n",
    "    reshaped_array = array.reshape(new_shape_first_dim, new_shape_second_dim)\n",
    "\n",
    "    return reshaped_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dd96575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearest_neighbor_data_neigh_levels(poi,tp_particles,num_levels=3,neigh_per_level=[5,3,2]):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper function to do the data generation \n",
    "    \n",
    "    Parameters:\n",
    "       poi (pandas dataframe) : coordinates of the particle of interest \n",
    "       tp_particles : the set of all particles (inlcuding particles obatined from peridic shifting)\n",
    "       num_levels : num of levels of neighborhood/shells wanted (at least 2)\n",
    "       neigh_per_level : number of neighbors to search per neighborhood/shell\n",
    "       \n",
    "    Returns:\n",
    "        : numpy array with the nearest neighbors and their corresponding index\n",
    "        : edge indexs, which is a an a tensor of all how all the neighbors are connected\n",
    "    \"\"\"\n",
    "\n",
    "    ### Defining KDtree ###\n",
    "    tree = KDTree(tp_particles)\n",
    "    \n",
    "    ### Loop over different groups ###\n",
    "    nearest_neighbor_data = list()\n",
    "    \n",
    "    for i in range(num_levels):\n",
    "        \n",
    "        ### Define query point/points ###\n",
    "        if i==0:\n",
    "            \n",
    "            idx = tree.query(poi,neigh_per_level[0],return_distance=False,sort_results=True)\n",
    "            nearest_neighbor_data.append(tp_particles[idx][0][1:])\n",
    "\n",
    "            particles_to_remove = np.concatenate( (poi,np.stack(nearest_neighbor_data)[0]) )\n",
    "            remaining_particles = array_difference(tp_particles,particles_to_remove)\n",
    "        \n",
    "        if i>0:\n",
    "                                                   \n",
    "            tree = KDTree(remaining_particles)\n",
    "            query_points = reshape_to_2d( nearest_neighbor_data[-1] ) \n",
    "\n",
    "            ### get nearest neighbors ###\n",
    "            idx = [ tree.query(query_points[j][None,:],neigh_per_level[i],return_distance=False,sort_results=True) for j in range(len(query_points)) ]\n",
    "            \n",
    "            ### flattening idx ###\n",
    "            \n",
    "            nearest_neighbor_data.append( np.stack([remaining_particles[idx[i]] for i in range(len(idx))]).squeeze(1) )\n",
    "            particles_to_remove = np.concatenate( ( particles_to_remove , reshape_to_2d(nearest_neighbor_data[-1]) ) ) \n",
    "            remaining_particles = array_difference(remaining_particles,particles_to_remove)\n",
    "\n",
    "    ### Correcting the shape of the first element ###\n",
    "    nearest_neighbor_data[0] = nearest_neighbor_data[0][None,:,:]\n",
    "        \n",
    "    return nearest_neighbor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c332f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list_of_indices_for_particles(poi,nearest_neighbor_data):\n",
    "\n",
    "    all_involved_particles = np.concatenate( (poi,\n",
    "                                          reshape_to_2d(nearest_neighbor_data[0]),\n",
    "                                          reshape_to_2d(nearest_neighbor_data[1]),\n",
    "                                          reshape_to_2d(nearest_neighbor_data[2]) ) )\n",
    "\n",
    "    indexes = np.unique(all_involved_particles,axis=0,return_index=True)[1]\n",
    "    all_involved_particles = np.stack([all_involved_particles[index] for index in sorted(indexes)])\n",
    "    \n",
    "    all_involved_particles = np.concatenate( ( np.arange(len(all_involved_particles))[:,None], all_involved_particles ),axis=1 )\n",
    "    all_involved_particles_pd = pd.DataFrame(all_involved_particles,columns=[\"Particle No\",\"x\",\"y\",\"z\"])\n",
    "\n",
    "    return all_involved_particles_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d61aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_connections_single_particle(index_list,particle_of_origin,connected_particles):\n",
    "\n",
    "    ### idx of the particle of origin ### \n",
    "    idx=np.where((index_list[\"x\"].values==particle_of_origin[0])&(index_list[\"y\"].values==particle_of_origin[1])&(index_list[\"z\"].values==particle_of_origin[2]))[0][0]\n",
    "    origin_id = index_list.iloc[idx][\"Particle No\"]\n",
    "    \n",
    "    ### idx of the connected particles ### \n",
    "    idx = (np.stack( [ (index_list[\"x\"].values==connected_particles[i][0])\n",
    "         &(index_list[\"y\"].values==connected_particles[i][1])\n",
    "         &(index_list[\"z\"].values==connected_particles[i][2]) for i in range(len(connected_particles))] ))\n",
    "    \n",
    "    idx = [np.where(idx[i])[0][0] for i in range(idx.shape[0])]\n",
    "    \n",
    "    connected_id = [ index_list.iloc[idx[i]][\"Particle No\"]  for i in range(len(idx))]\n",
    "\n",
    "    return origin_id,connected_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fccef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_connections_multiple_particle(index_list,nearest_neighbor_data,num_levels,neigh_per_level):\n",
    "\n",
    "    ### Define edge connectors list ###\n",
    "    edge_index = list()\n",
    "\n",
    "    ### iterate over consecutive neighborhood levels ###\n",
    "    for i in range(num_levels-1):\n",
    "\n",
    "        ### Define temporary previous and next levels (reshape the previous to only dimensions)###\n",
    "        previous_level = reshape_to_2d( nearest_neighbor_data[i] )\n",
    "        next_level = nearest_neighbor_data[i+1] \n",
    "\n",
    "        for j in range(len(previous_level)):\n",
    "            \n",
    "            edge_index.append( generate_connections_single_particle(index_list = index_list,\n",
    "                             particle_of_origin = previous_level[j],\n",
    "                             connected_particles = next_level[j]) )\n",
    "            \n",
    "    edge_index = edge_index_post_proc(edge_index)\n",
    "    \n",
    "    ### add connections from POI to the first level neighbors ###\n",
    "    edge_index = [([0,i]) for i in range(neigh_per_level[0]-1)] + edge_index\n",
    "    \n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ff21bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns_to_pandas_list(pandas_dataframe,variable_list,master_dataframe):\n",
    "\n",
    "    \"\"\" given a list of pandas dataframe with the x,y,z locations and re and phi ,this function will\n",
    "        merge each pandas dataframe from the list with the master dataframe with all the columns  \n",
    "    \"\"\"\n",
    "\n",
    "    joined = copy.deepcopy(pandas_dataframe)\n",
    "    add = pd.merge(pandas_dataframe,master_dataframe,how=\"inner\",on=[\"x\",\"y\",\"z\"],sort=False)[variable_list]\n",
    "        \n",
    "    return  pd.concat([joined,add], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35dfbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_index_post_proc(edge_index):\n",
    "    \n",
    "    return [( [edge_index[i][0] , edge_index[i][1][j] ] ) for i in range(len(edge_index)) for j in range(len(edge_index[i][1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5dde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_wrapper(pd_list,num_levels=3,neigh_per_level=[5,3,2]):\n",
    "\n",
    "    nodes_list = list()\n",
    "    connections_list = list()\n",
    "    delete_list = list()\n",
    "    \n",
    "    ### iterate over case_time subgroups ###\n",
    "#     for i in range(50):\n",
    "    for i in range(len(pd_list)):\n",
    "        \n",
    "        tp_particles,tp_particles_dataframe = get_periodic_coordinates(pd_list[i],5)\n",
    "\n",
    "        ### iterate over the particles in each case_time subgroup ###\n",
    "        for j in range(pd_list[i].shape[0]):\n",
    "            \n",
    "            print(\"Currently on case_time subgroup : \",str(i+1), \"and on particle number : \",str(j+1))\n",
    "            poi = np.array(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values[None,:])\n",
    "\n",
    "            ### Generate nearest neighbors data ###\n",
    "            nearest_neighbor_data = generate_nearest_neighbor_data_neigh_levels(poi,tp_particles,num_levels=num_levels\n",
    "                                                                                ,neigh_per_level=neigh_per_level)\n",
    "            \n",
    "            delete_list.append(nearest_neighbor_data)\n",
    "            \n",
    "            ### Find unique data particles and assign an index to each of them ###\n",
    "            all_involved_particles_pd = generate_list_of_indices_for_particles(poi,nearest_neighbor_data)\n",
    "            \n",
    "            ### Generate connection indexes ###\n",
    "            edge_index = generate_connections_multiple_particle(all_involved_particles_pd,nearest_neighbor_data\n",
    "                                                                ,num_levels=num_levels,neigh_per_level=neigh_per_level)\n",
    "\n",
    "            ### Append local features to all_involved_particles_pd ###\n",
    "            all_involved_particles_pd_extra = merge_columns_to_pandas_list(all_involved_particles_pd\n",
    "                                                                          ,variable_list=[\"local_Re\",\"vpx\",\"vpy\",\"vpz\",\"Drag\"],\n",
    "                                                                           master_dataframe=tp_particles_dataframe)\n",
    "          \n",
    "            ### append data to lists ###\n",
    "            nodes_list.append(all_involved_particles_pd_extra)\n",
    "            connections_list.append(edge_index)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    return nodes_list,connections_list,delete_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a0f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_data(df_list_train, df_list_test):\n",
    "    # Scaling parameters for x, y, z\n",
    "    scale_min, scale_max = -5, 10\n",
    "\n",
    "    # Function to scale x, y, z columns\n",
    "    def scale_xyz(df):\n",
    "        df = df.copy()  # Ensure we are working on a copy\n",
    "        for col in ['x', 'y', 'z']:\n",
    "            df[col] = (df[col] - scale_min) / (scale_max - scale_min)\n",
    "        return df\n",
    "\n",
    "    # Apply the custom scaling for x, y, z columns to both train and test DataFrames\n",
    "    scaled_dfs_train = [scale_xyz(df) for df in df_list_train]\n",
    "    scaled_dfs_test = [scale_xyz(df) for df in df_list_test]\n",
    "\n",
    "    # List of columns to be scaled with MinMaxScaler\n",
    "    other_columns = ['local_Re', 'vpx', 'vpy', 'vpz']\n",
    "\n",
    "    # Combine all 'other' columns from all training DataFrames into one DataFrame for MinMax scaling\n",
    "    combined_other_train = pd.concat([df[other_columns] for df in scaled_dfs_train])\n",
    "\n",
    "    # Initialize and fit the MinMaxScaler on the combined 'other' columns from training data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(combined_other_train)\n",
    "\n",
    "    # Apply the MinMaxScaler to the 'other' columns in both train and test DataFrames\n",
    "    for df in scaled_dfs_train:\n",
    "        df[other_columns] = scaler.transform(df[other_columns])\n",
    "    for df in scaled_dfs_test:\n",
    "        df[other_columns] = scaler.transform(df[other_columns])\n",
    "\n",
    "    return scaled_dfs_train, scaled_dfs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ba53fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_columns(tensor):\n",
    "    # Check if the tensor has exactly 2 rows\n",
    "    if tensor.shape[0] != 2:\n",
    "        raise ValueError(\"The input tensor must have exactly 2 rows.\")\n",
    "    \n",
    "    # Transpose the tensor to get columns as rows, then convert to a set of tuples to remove duplicates\n",
    "    transposed_tensor = tensor.t()\n",
    "    unique_columns = set(map(tuple, transposed_tensor.tolist()))\n",
    "    \n",
    "    # Convert back to a tensor\n",
    "    unique_tensor = torch.tensor(list(unique_columns)).t()\n",
    "    \n",
    "    return unique_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833546a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on case_time subgroup :  297 and on particle number :  18\n"
     ]
    }
   ],
   "source": [
    "### Read data ###\n",
    "experiment = \"rho2_10percent_Re100\"\n",
    "time_series_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/data_with_xyz_velocities/\"+experiment+\".dat\",index_col=False)\n",
    "pd_list = group_time_series_data(time_series_data)\n",
    "nodes_list,connections_list,nearest_neighbor_data = final_wrapper(pd_list)\n",
    "\n",
    "### Pre-processing ###\n",
    "\n",
    "### extract the necessary features ###\n",
    "input_node_list = [nodes_list[i][[\"x\",\"y\",\"z\",\"local_Re\",\"vpx\",\"vpy\",\"vpz\"]] for i in range(len(nodes_list))]\n",
    "\n",
    "### convert connections_list to int datatype ###\n",
    "connections_list = [torch.tensor(sublist, dtype=torch.int64).T for sublist in connections_list]\n",
    "\n",
    "for i in range(len(connections_list)):\n",
    "    connections_list[i] = remove_duplicate_columns(connections_list[i])\n",
    "\n",
    "### extracting the drag forces from node list ###\n",
    "drag_forces = [nodes_list[i][\"Drag\"].values[0] for i in range(len(nodes_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40679e",
   "metadata": {},
   "source": [
    "# Extrapolation Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ff47489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select rows for the train dataset based on the given conditions\n",
    "\n",
    "start_time = 0\n",
    "interval_length = 120\n",
    "interval_length_val = 30\n",
    "\n",
    "### Defining the train indices ###\n",
    "train_case_1_idx = np.where(((time_series_data['case_ID'] == 1) & (time_series_data['time'] <= start_time+interval_length)\n",
    "                                                                & (time_series_data['time'] >= start_time)))[0]\n",
    "train_case_2_idx = np.where(((time_series_data['case_ID'] == 2) & (time_series_data['time'] <= start_time+interval_length)\n",
    "                                                                & (time_series_data['time'] >= start_time)))[0]\n",
    "train_idx = np.concatenate((train_case_1_idx,train_case_2_idx))\n",
    "\n",
    "### Defining the test indices ###\n",
    "test_case_1_idx = np.where(((time_series_data['case_ID'] == 1) & (time_series_data['time'] <= start_time+interval_length+interval_length_val)\n",
    "                                                                & (time_series_data['time'] >= start_time+interval_length)))[0]\n",
    "test_case_2_idx = np.where(((time_series_data['case_ID'] == 2) & (time_series_data['time'] <= start_time+interval_length+interval_length_val)\n",
    "                                                                & (time_series_data['time'] >= start_time+interval_length)))[0]\n",
    "test_idx = np.concatenate((test_case_1_idx,test_case_2_idx))\n",
    "\n",
    "### split data into train and test ###\n",
    "### input_node_list ###\n",
    "input_node_list_train = [input_node_list[i] for i in train_idx]\n",
    "input_node_list_test = [input_node_list[i] for i in test_idx]\n",
    "\n",
    "### connection_list ###\n",
    "connections_list_train = [connections_list[i] for i in train_idx]\n",
    "connections_list_test = [connections_list[i] for i in test_idx]\n",
    "\n",
    "### drag forces ###\n",
    "drag_forces_train = [drag_forces[i] for i in train_idx]\n",
    "drag_forces_test = [drag_forces[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474774c",
   "metadata": {},
   "source": [
    "# Data Scaling (Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab2ff4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In case : using poi  based local coordinates ###\n",
    "def subtract_first_row(df):\n",
    "    df.loc[:, ['x', 'y', 'z']] = df.loc[:, ['x', 'y', 'z']].sub(df.loc[0, ['x', 'y', 'z']])\n",
    "    return df\n",
    "\n",
    "input_node_list_train = [subtract_first_row(df) for df in input_node_list_train]\n",
    "input_node_list_test = [subtract_first_row(df) for df in input_node_list_test]\n",
    "\n",
    "def scale_xyz(df):\n",
    "    df.loc[:, 'x'] = (df['x'] - (-5)) / (10 - (-5))\n",
    "    df.loc[:, 'y'] = (df['y'] - (-5)) / (10 - (-5))\n",
    "    df.loc[:, 'z'] = (df['z'] - (-5)) / (10 - (-5))\n",
    "    return df\n",
    "\n",
    "# Scale x, y, and z for both train and test lists\n",
    "input_node_list_train_scaled = [scale_xyz(df) for df in input_node_list_train]\n",
    "input_node_list_test_scaled = [scale_xyz(df) for df in input_node_list_test]\n",
    "\n",
    "# Combine all train DataFrames to fit the MinMaxScaler\n",
    "combined_train = pd.concat(input_node_list_train_scaled, ignore_index=True)\n",
    "\n",
    "# Identify other columns\n",
    "other_columns = combined_train.columns.difference(['x', 'y', 'z'])\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the combined train DataFrame's other columns\n",
    "scaler.fit(combined_train[other_columns])\n",
    "\n",
    "# Function to scale other columns using the fitted scaler\n",
    "def scale_other_columns(df, scaler, other_columns):\n",
    "    df.loc[:,other_columns] = scaler.transform(df[other_columns])\n",
    "    return df\n",
    "\n",
    "# Scale other columns for both train and test lists\n",
    "input_node_list_train_scaled = [scale_other_columns(df, scaler, other_columns) for df in input_node_list_train_scaled]\n",
    "input_node_list_test_scaled = [scale_other_columns(df, scaler, other_columns) for df in input_node_list_test_scaled]\n",
    "\n",
    "### Implement this (making POI) x,y,z 0 for local coordinates ###\n",
    "# Setting the first row's x, y, z values to zeros for each DataFrame in the list\n",
    "for df in input_node_list_train_scaled:\n",
    "    df.loc[0, [\"x\", \"y\", \"z\"]] = np.zeros(3)\n",
    "\n",
    "for df in input_node_list_test_scaled:\n",
    "    df.loc[0, [\"x\", \"y\", \"z\"]] = np.zeros(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8c018",
   "metadata": {},
   "source": [
    "# Data Scaling (Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e5b766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.array(drag_forces_train)[:,None])\n",
    "\n",
    "drag_forces_train_scaled = scaler.transform(np.array(drag_forces_train)[:,None])\n",
    "\n",
    "drag_forces_test_scaled = scaler.transform(np.array(drag_forces_test)[:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3de0d9",
   "metadata": {},
   "source": [
    "# Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d343f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the unscaled data ###\n",
    "with open(\"extrapolation/\"+experiment+\"/120_150_levelled_poi_coordinates/train_input_unscaled.pkl\", 'wb') as file:\n",
    "    pickle.dump(input_node_list_train, file)\n",
    "    \n",
    "with open(\"extrapolation/\"+experiment+\"/120_150_levelled_poi_coordinates/test_input_unscaled.pkl\", 'wb') as file:\n",
    "    pickle.dump(input_node_list_test, file)\n",
    "    \n",
    "with open(\"extrapolation/\"+experiment+\"/120_150_levelled_poi_coordinates/train_output_unscaled.pkl\", 'wb') as file:\n",
    "    pickle.dump(drag_forces_train, file)\n",
    "    \n",
    "with open(\"extrapolation/\"+experiment+\"/120_150_levelled_poi_coordinates/test_output_unscaled.pkl\", 'wb') as file:\n",
    "    pickle.dump(drag_forces_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42800a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the scaled data ###\n",
    "with open(\"extrapolation/\"+experiment+\"/120_150_levelled_poi_coordinates/train_input_scaled.pkl\", 'wb') as file:\n",
    "    pickle.dump(input_node_list_train_scaled, file)\n",
    "    \n",
    "with open(\"extrapolation/\"+experiment+\"/120_150_levelled_poi_coordinates/test_input_scaled.pkl\", 'wb') as file:\n",
    "    pickle.dump(input_node_list_test_scaled, file)\n",
    "    \n",
    "with open(\"extrapolation/\"+experiment+\"/120_150_levelled_poi_coordinates/train_output_scaled.pkl\", 'wb') as file:\n",
    "    pickle.dump(drag_forces_train_scaled, file)\n",
    "    \n",
    "with open(\"extrapolation/\"+experiment+\"/120_150_levelled_poi_coordinates/test_output_scaled.pkl\", 'wb') as file:\n",
    "    pickle.dump(drag_forces_test_scaled, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "936b0908",
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving node connections (same from train and test) ###\n",
    "with open(\"extrapolation/\"+experiment+\"/120_150_levelled_poi_coordinates/train_edge_connections.pkl\", 'wb') as file:\n",
    "    pickle.dump(connections_list_train, file)\n",
    "    \n",
    "with open(\"extrapolation/\"+experiment+\"/120_150_levelled_poi_coordinates/test_edge_connections.pkl\", 'wb') as file:\n",
    "    pickle.dump(connections_list_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727dc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b87856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /localscratch-ssd/287500/matplotlib-0vm0v92w because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "# from torch_geometric.datasets import MoleculeNet\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "# import torch\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from torch import nn\n",
    "# import torch_geometric.nn as geom_nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01270d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_periodic_coordinates(coord, size):\n",
    "    \"\"\"\n",
    "    Generate all coordinates within a cubic domain considering periodic boundary conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        coord (pandas dataframe): A pandas dataframe containing the columns (x, y, z) of a point.\n",
    "        size (int): The size of the cubic domain along each axis.\n",
    "    Returns:\n",
    "        list: A list of tuples containing all coordinates within the cubic domain.\n",
    "    \"\"\"\n",
    "    ### make a copy of coord ###\n",
    "    coord_copy = coord.copy()\n",
    "    coord_copy = [coord_copy] * 27\n",
    "    coord_copy = pd.concat(coord_copy, ignore_index=True)\n",
    "    \n",
    "    # Get coordinates ###\n",
    "    if isinstance(coord, pd.DataFrame):\n",
    "        coord = coord[[\"x\",\"y\",\"z\"]].values\n",
    "\n",
    "    # Generate all combinations of displacements (-1, 0, 1) along each axis\n",
    "    displacements = list(itertools.product([-1, 0, 1], repeat=3))\n",
    "\n",
    "    # Generate all coordinates by applying periodic boundary conditions\n",
    "    tp_coordinates = list()\n",
    "    \n",
    "    for dx, dy, dz in displacements:\n",
    "        \n",
    "        temp = list()\n",
    "        \n",
    "        for i in range(len(coord)):\n",
    "            \n",
    "            x, y, z = coord[i,0],coord[i,1],coord[i,2]  \n",
    "\n",
    "            new_x = x + dx*size\n",
    "            new_y = y + dy*size\n",
    "            new_z = z + dz*size\n",
    "\n",
    "            temp.append((new_x,new_y,new_z))\n",
    "            \n",
    "        tp_coordinates.append( np.array(temp) )\n",
    "    \n",
    "    coord_copy[[\"x\",\"y\",\"z\"]] = np.vstack(tp_coordinates)\n",
    "    \n",
    "    return np.vstack(tp_coordinates),coord_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a82ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_time_series_data(time_series_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Groups the data based on case_ID and time \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    ### load raw data from ze time series data ###\n",
    "    pd_list  = list()\n",
    "    \n",
    "    for (col1_val, col2_val), group in time_series_data.groupby(['case_ID', 'time']):\n",
    "    \n",
    "        pd_list.append(group)\n",
    "    \n",
    "    return pd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a06b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearest_neighbor_data(time_series_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper function (in some sense, can be condensed more)to do the data generation \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    \n",
    "    pd_list = group_time_series_data(time_series_data)\n",
    "    \n",
    "    nearest_neighbor_data = list()\n",
    "    scalar_data = list()\n",
    "    \n",
    "    ### Loop over different groups ###\n",
    "    \n",
    "    for i in range(len(pd_list)):\n",
    "\n",
    "        tp_particles = get_periodic_coordinates(pd_list[i],5)\n",
    "        tree = cKDTree(tp_particles)\n",
    "\n",
    "        ### Loop over all particles in a group and getting the nearest neighbors ###\n",
    "        idx = np.stack([ tree.query(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values,16)[1] for j in range(len(pd_list[i])) ])\n",
    "        nearest_neighbor_data.append(tp_particles[idx])\n",
    "\n",
    "        ### Getting the scalar data ###\n",
    "        scalar_data.append( pd_list[i][[\"Density_ratio\",\"glb_phi\",\"glb_Re\",\"local_Re\",\"Drag\"]] )\n",
    "\n",
    "    nearest_neighbor_data = np.stack(nearest_neighbor_data)\n",
    "    nearest_neighbor_data = nearest_neighbor_data.reshape(nearest_neighbor_data.shape[0]*nearest_neighbor_data.shape[1]\n",
    "                                           ,nearest_neighbor_data.shape[2]*nearest_neighbor_data.shape[3])\n",
    "    \n",
    "    scalar_data = np.stack(scalar_data)\n",
    "    scalar_data = scalar_data.reshape(scalar_data.shape[0]*scalar_data.shape[1],scalar_data.shape[2])    \n",
    "    \n",
    "    return np.concatenate( (nearest_neighbor_data,scalar_data) ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2272a73",
   "metadata": {},
   "source": [
    "# Neigh Level Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdaf7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_difference(array1,array2):\n",
    "    \n",
    "    ### First one must be the bigger array ###\n",
    "    set1 = set(map(tuple, array1))\n",
    "    set2 = set(map(tuple, array2))\n",
    "    \n",
    "    # Find the set difference\n",
    "    set_difference = set1 - set2\n",
    "\n",
    "    # Convert the set difference back to a NumPy array\n",
    "    return np.array(list(set_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ad9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_2d(array):\n",
    "    \"\"\"\n",
    "    Reshape an n-dimensional NumPy array to a 2D array, flattening all dimensions except the last one.\n",
    "\n",
    "    Parameters:\n",
    "        array (numpy.ndarray): Input n-dimensional array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Reshaped 2D array.\n",
    "    \"\"\"\n",
    "    # Flatten all dimensions except the last one\n",
    "    new_shape_first_dim = np.prod(array.shape[:-1])\n",
    "    \n",
    "    # Keep the last dimension intact\n",
    "    new_shape_second_dim = array.shape[-1]\n",
    "\n",
    "    # Reshape the array\n",
    "    reshaped_array = array.reshape(new_shape_first_dim, new_shape_second_dim)\n",
    "\n",
    "    return reshaped_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dd96575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearest_neighbor_data_neigh_levels(poi,tp_particles,num_levels=3,neigh_per_level=[5,3,2]):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper function to do the data generation \n",
    "    \n",
    "    Parameters:\n",
    "       poi (pandas dataframe) : coordinates of the particle of interest \n",
    "       tp_particles : the set of all particles (inlcuding particles obatined from peridic shifting)\n",
    "       num_levels : num of levels of neighborhood/shells wanted (at least 2)\n",
    "       neigh_per_level : number of neighbors to search per neighborhood/shell\n",
    "       \n",
    "    Returns:\n",
    "        : numpy array with the nearest neighbors and their corresponding index\n",
    "        : edge indexs, which is a an a tensor of all how all the neighbors are connected\n",
    "    \"\"\"\n",
    "\n",
    "    ### Defining KDtree ###\n",
    "    tree = KDTree(tp_particles)\n",
    "    \n",
    "    ### Loop over different groups ###\n",
    "    nearest_neighbor_data = list()\n",
    "    \n",
    "    for i in range(num_levels):\n",
    "        \n",
    "        ### Define query point/points ###\n",
    "        if i==0:\n",
    "            \n",
    "            idx = tree.query(poi,neigh_per_level[0],return_distance=False,sort_results=True)\n",
    "            nearest_neighbor_data.append(tp_particles[idx][0][1:])\n",
    "#             print(\"nearest neighbors addition i=0\",tp_particles[idx][0][1:].shape)\n",
    "            particles_to_remove = np.concatenate( (poi,np.stack(nearest_neighbor_data)[0]) )\n",
    "            remaining_particles = array_difference(tp_particles,particles_to_remove)\n",
    "        \n",
    "        if i>0:\n",
    "                                                   \n",
    "            tree = KDTree(remaining_particles)\n",
    "            query_points = reshape_to_2d( nearest_neighbor_data[-1] ) \n",
    "\n",
    "            ### get nearest neighbors ###\n",
    "            idx = [ tree.query(query_points[j][None,:],neigh_per_level[i],return_distance=False,sort_results=True) for j in range(len(query_points)) ]\n",
    "            \n",
    "            ### flattening idx ###\n",
    "            \n",
    "            nearest_neighbor_data.append( np.stack([remaining_particles[idx[i]] for i in range(len(idx))]).squeeze(1) )\n",
    "#             print(\"nearest neighbors addition i>0\",np.stack([remaining_particles[idx[i]] for i in range(len(idx))]).squeeze(1).shape)\n",
    "            particles_to_remove = np.concatenate( ( particles_to_remove , reshape_to_2d(nearest_neighbor_data[-1]) ) ) \n",
    "            remaining_particles = array_difference(remaining_particles,particles_to_remove)\n",
    "\n",
    "    ### Correcting the shape of the first element ###\n",
    "    nearest_neighbor_data[0] = nearest_neighbor_data[0][None,:,:]\n",
    "        \n",
    "    return nearest_neighbor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4c332f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list_of_indices_for_particles(poi,nearest_neighbor_data):\n",
    "\n",
    "    all_involved_particles = np.concatenate( (poi,\n",
    "                                          reshape_to_2d(nearest_neighbor_data[0]),\n",
    "                                          reshape_to_2d(nearest_neighbor_data[1]),\n",
    "                                          reshape_to_2d(nearest_neighbor_data[2]) ) )\n",
    "\n",
    "    indexes = np.unique(all_involved_particles,axis=0,return_index=True)[1]\n",
    "    all_involved_particles = np.stack([all_involved_particles[index] for index in sorted(indexes)])\n",
    "    \n",
    "    all_involved_particles = np.concatenate( ( np.arange(len(all_involved_particles))[:,None], all_involved_particles ),axis=1 )\n",
    "    all_involved_particles_pd = pd.DataFrame(all_involved_particles,columns=[\"Particle No\",\"x\",\"y\",\"z\"])\n",
    "\n",
    "    return all_involved_particles_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d61aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_connections_single_particle(index_list,particle_of_origin,connected_particles):\n",
    "\n",
    "    ### idx of the particle of origin ### \n",
    "    idx=np.where((index_list[\"x\"].values==particle_of_origin[0])&(index_list[\"y\"].values==particle_of_origin[1])&(index_list[\"z\"].values==particle_of_origin[2]))[0][0]\n",
    "    origin_id = index_list.iloc[idx][\"Particle No\"]\n",
    "    \n",
    "    ### idx of the connected particles ### \n",
    "    idx = (np.stack( [ (index_list[\"x\"].values==connected_particles[i][0])\n",
    "         &(index_list[\"y\"].values==connected_particles[i][1])\n",
    "         &(index_list[\"z\"].values==connected_particles[i][2]) for i in range(len(connected_particles))] ))\n",
    "    \n",
    "    idx = [np.where(idx[i])[0][0] for i in range(idx.shape[0])]\n",
    "    \n",
    "    connected_id = [ index_list.iloc[idx[i]][\"Particle No\"]  for i in range(len(idx))]\n",
    "\n",
    "    return origin_id,connected_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fccef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_connections_multiple_particle(index_list,nearest_neighbor_data,num_levels,neigh_per_level):\n",
    "\n",
    "    ### Define edge connectors list ###\n",
    "    edge_index = list()\n",
    "\n",
    "    ### iterate over consecutive neighborhood levels ###\n",
    "    for i in range(num_levels-1):\n",
    "\n",
    "        ### Define temporary previous and next levels (reshape the previous to only dimensions)###\n",
    "        previous_level = reshape_to_2d( nearest_neighbor_data[i] )\n",
    "        next_level = nearest_neighbor_data[i+1] \n",
    "        \n",
    "        for j in range(len(previous_level)):\n",
    "            \n",
    "            edge_index.append( generate_connections_single_particle(index_list = index_list,\n",
    "                             particle_of_origin = previous_level[j],\n",
    "                             connected_particles = next_level[j]) )\n",
    "    \n",
    "    edge_index = edge_index_post_proc(edge_index)\n",
    "    \n",
    "    ### add connections from POI to the first level neighbors ###\n",
    "    edge_index = [([0,i]) for i in range(neigh_per_level[0]-1)] + edge_index\n",
    "    \n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ff21bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns_to_pandas_list(pandas_dataframe,variable_list,master_dataframe):\n",
    "\n",
    "    \"\"\" given a list of pandas dataframe with the x,y,z locations and re and phi ,this function will\n",
    "        merge each pandas dataframe from the list with the master dataframe with all the columns  \n",
    "    \"\"\"\n",
    "\n",
    "    joined = copy.deepcopy(pandas_dataframe)\n",
    "    add = pd.merge(pandas_dataframe,master_dataframe,how=\"inner\",on=[\"x\",\"y\",\"z\"],sort=False)[variable_list]\n",
    "        \n",
    "    return  pd.concat([joined,add], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35dfbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_index_post_proc(edge_index):\n",
    "    \n",
    "    return [( [edge_index[i][0] , edge_index[i][1][j] ] ) for i in range(len(edge_index)) for j in range(len(edge_index[i][1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f5dde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_wrapper(pd_list,num_levels=3,neigh_per_level=[5,3,2]):\n",
    "\n",
    "    nodes_list = list()\n",
    "    connections_list = list()\n",
    "    \n",
    "    ### iterate over case_time subgroups ###\n",
    "#     for i in range(50):\n",
    "    for i in range(len(pd_list)):\n",
    "        \n",
    "        tp_particles,tp_particles_dataframe = get_periodic_coordinates(pd_list[i],5)\n",
    "\n",
    "        ### iterate over the particles in each case_time subgroup ###\n",
    "        for j in range(pd_list[i].shape[0]):\n",
    "            \n",
    "            print(\"Currently on case_time subgroup : \",str(i+1), \"and on particle number : \",str(j+1))\n",
    "            poi = np.array(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values[None,:])\n",
    "\n",
    "            ### Generate nearest neighbors data ###\n",
    "            nearest_neighbor_data = generate_nearest_neighbor_data_neigh_levels(poi,tp_particles,num_levels=num_levels\n",
    "                                                                                ,neigh_per_level=neigh_per_level)\n",
    "            \n",
    "            ### Find unique data particles and assign an index to each of them ###\n",
    "            all_involved_particles_pd = generate_list_of_indices_for_particles(poi,nearest_neighbor_data)\n",
    "            \n",
    "            ### Generate connection indexes ###\n",
    "            edge_index = generate_connections_multiple_particle(all_involved_particles_pd,nearest_neighbor_data\n",
    "                                                                ,num_levels=num_levels,neigh_per_level=neigh_per_level)\n",
    "\n",
    "            ### Append local features to all_involved_particles_pd ###\n",
    "            all_involved_particles_pd_extra = merge_columns_to_pandas_list(all_involved_particles_pd\n",
    "                                                                          ,variable_list=[\"local_Re\"],\n",
    "                                                                           master_dataframe=tp_particles_dataframe)\n",
    "          \n",
    "            ### append data to lists ###\n",
    "            nodes_list.append(all_involved_particles_pd_extra)\n",
    "            connections_list.append(edge_index)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    return nodes_list,connections_list,nearest_neighbor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7833546a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on case_time subgroup :  984 and on particle number :  24\n"
     ]
    }
   ],
   "source": [
    "### Read data ###\n",
    "time_series_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho100_10percent_Re100.dat\")\n",
    "pd_list = group_time_series_data(time_series_data)\n",
    "nodes_list,connections_list,nearest_neighbor_data = final_wrapper(pd_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b103da",
   "metadata": {},
   "source": [
    "End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3de0d9",
   "metadata": {},
   "source": [
    "# Quick Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffdf22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nodes_list.pkl', 'rb') as file:\n",
    "    node_list = pickle.load(file)\n",
    "    \n",
    "with open('connections_list.pkl', 'rb') as file:\n",
    "    connections_list = pickle.load(file)\n",
    "\n",
    "### Pandas to numpy array ###\n",
    "# node_list = [np.array(node_list[i].values[:,1:]) for i in range(len(node_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d288db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_array_randomly(node_list, n):\n",
    "    # Create the array from 0 to len(node_list) - 1\n",
    "    array = np.arange(len(node_list))\n",
    "    \n",
    "    # Shuffle the array randomly\n",
    "    array = np.random.shuffle(array)\n",
    "    \n",
    "    # Compute the split index\n",
    "    split_index = int(len(array) * n / 100)\n",
    "    \n",
    "    # Split the array\n",
    "    part1 = array[:split_index]\n",
    "    part2 = array[split_index:]\n",
    "    \n",
    "    return part1, part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fe2b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(data,train_indices,test_indices):\n",
    "\n",
    "    ### Extracting Train Dataset ###\n",
    "    train_dat = [ data[train_indices[i]] for i in range(len(train_indices)) ]\n",
    "\n",
    "    ### Extracting Test Dataset ###\n",
    "    test_dat = [ data[test_indices[i]] for i in range(len(test_indices)) ]\n",
    "\n",
    "    ### Scaling inputs ###\n",
    "    ### Getting min and max values ###\n",
    "    max_vals = np.max(np.stack([train_dat[i].x[:,j].max() for i in \n",
    "                                range(len(train_dat)) for j in range(train_dat[i].x.shape[1]) ]).reshape(len(train_dat),train_dat[0].x.shape[1]),axis=0)\n",
    "    \n",
    "    min_vals = np.min(np.stack([train_dat[i].x[:,j].min() for i in \n",
    "                                range(len(train_dat)) for j in range(train_dat[i].x.shape[1]) ]).reshape(len(train_dat),train_dat[0].x.shape[1]),axis=0)\n",
    "    \n",
    "    ### Performing the scaling ###\n",
    "    train_dat_input = [ (train_dat[i].x - min_vals)/(max_vals - min_vals) for i in range(len(train_dat))] \n",
    "    test_dat_input = [ (test_dat[i].x - min_vals)/(max_vals - min_vals) for i in range(len(test_dat))]\n",
    "\n",
    "    ### Scaling outputs ###\n",
    "    ### Getting min and max values ###\n",
    "    max_vals = np.max( np.stack( [train_dat[i].y.max() for i in range(len(train_dat))] ),axis=0)\n",
    "    min_vals = np.min( np.stack( [train_dat[i].y.min() for i in range(len(train_dat))] ),axis=0)\n",
    "\n",
    "    ### Performing the scaling ###\n",
    "    train_dat_output = [ (train_dat[i].y - min_vals)/(max_vals - min_vals) for i in range(len(train_dat))] \n",
    "    test_dat_output = [ (test_dat[i].y - min_vals)/(max_vals - min_vals) for i in range(len(test_dat))]\n",
    "\n",
    "    ### Combine x and y edge indices from the train and test dataset to form a pygnn Data variable ###\n",
    "    train_combined = list()\n",
    "    test_combined = list()\n",
    "    \n",
    "    for i in range(len(train_dat_input)):\n",
    "\n",
    "        x = train_dat_input[i].clone()\n",
    "        edge_index = (train_dat[i].edge_index).clone()\n",
    "        y = (train_dat_output[i]).clone()\n",
    "        train_combined.append(Data(x=x , edge_index=edge_index , y=y))\n",
    "\n",
    "    for i in range(len(test_dat_input)):\n",
    "\n",
    "        x = test_dat_input[i].clone()\n",
    "        edge_index = (test_dat[i].edge_index).clone()\n",
    "        y = (test_dat_output[i]).clone()\n",
    "        test_combined.append(Data(x=x , edge_index=edge_index , y=y))\n",
    "\n",
    "    return train_combined,test_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d343f164",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Data reshaping ###\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# train_indices,test_indices = split_array_randomly(node_list,80)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m### Data scaling ###\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_combined,test_combined \u001b[38;5;241m=\u001b[39m \u001b[43mmin_max_scaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m, in \u001b[0;36mmin_max_scaler\u001b[0;34m(data, train_indices, test_indices)\u001b[0m\n\u001b[1;32m      7\u001b[0m test_dat \u001b[38;5;241m=\u001b[39m [ data[test_indices[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_indices)) ]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m### Scaling inputs ###\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m### Getting min and max values ###\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m max_vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mstack([train_dat[i]\u001b[38;5;241m.\u001b[39mx[:,j]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m     12\u001b[0m                             \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dat)) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_dat[i]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) ])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(train_dat),train_dat[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m min_vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(np\u001b[38;5;241m.\u001b[39mstack([train_dat[i]\u001b[38;5;241m.\u001b[39mx[:,j]\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m     15\u001b[0m                             \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dat)) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_dat[i]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) ])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(train_dat),train_dat[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m### Performing the scaling ###\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m test_dat \u001b[38;5;241m=\u001b[39m [ data[test_indices[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_indices)) ]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m### Scaling inputs ###\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m### Getting min and max values ###\u001b[39;00m\n\u001b[1;32m     11\u001b[0m max_vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mstack([train_dat[i]\u001b[38;5;241m.\u001b[39mx[:,j]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \n\u001b[0;32m---> 12\u001b[0m                             \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dat)) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mtrain_dat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m) ])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(train_dat),train_dat[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m min_vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(np\u001b[38;5;241m.\u001b[39mstack([train_dat[i]\u001b[38;5;241m.\u001b[39mx[:,j]\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m     15\u001b[0m                             \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dat)) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_dat[i]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) ])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(train_dat),train_dat[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m### Performing the scaling ###\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "### Data reshaping ###\n",
    "# train_indices,test_indices = split_array_randomly(node_list,80)\n",
    "\n",
    "### Data scaling ###\n",
    "train_combined,test_combined = min_max_scaler(node_list,np.arange(100),np.arange(200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

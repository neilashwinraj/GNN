{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d15cf2-f47d-434b-84cb-94bb3c79c276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /localscratch-ssd/304269/matplotlib-73dfh1hb because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ed6a0-c522-4929-a6d4-837c2c395502",
   "metadata": {},
   "source": [
    "# Generate from Ze time series dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b597fa-8c9b-4289-aa52-78fdd1691d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_periodic_coordinates(coord, size):\n",
    "    \"\"\"\n",
    "    Generate all coordinates within a cubic domain considering periodic boundary conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        coord (pandas dataframe): A pandas dataframe containing the columns (x, y, z) of a point.\n",
    "        size (int): The size of the cubic domain along each axis.\n",
    "    Returns:\n",
    "        list: A list of tuples containing all coordinates within the cubic domain.\n",
    "    \"\"\"\n",
    "    ### Keep copy of original dataframe and copy for each periodic bc shift ###\n",
    "    coord_copy = [coord.copy() for _ in range(27)]\n",
    "    stacked_df = pd.concat(coord_copy, axis=0)\n",
    "    stacked_df = stacked_df.reset_index(drop=True, inplace=False)\n",
    "    \n",
    "    # Get coordinates ###\n",
    "    if isinstance(coord, pd.DataFrame):\n",
    "        coord = coord[[\"x\",\"y\",\"z\"]].values\n",
    "\n",
    "    # Generate all combinations of displacements (-1, 0, 1) along each axis\n",
    "    displacements = list(itertools.product([-1, 0, 1], repeat=3))\n",
    "\n",
    "    # Generate all coordinates by applying periodic boundary conditions\n",
    "    tp_coordinates = list()\n",
    "    \n",
    "    for dx, dy, dz in displacements:\n",
    "          \n",
    "        temp = list()\n",
    "        \n",
    "        for i in range(len(coord)):\n",
    "            \n",
    "            x, y, z = coord[i,0],coord[i,1],coord[i,2]\n",
    "            \n",
    "            new_x = x + dx*size\n",
    "            new_y = y + dy*size\n",
    "            new_z = z + dz*size\n",
    "\n",
    "            temp.append((new_x,new_y,new_z))\n",
    "            \n",
    "        tp_coordinates.append( np.array(temp) )\n",
    "    \n",
    "    stacked_df[[\"x\",\"y\",\"z\"]] = np.vstack(tp_coordinates) \n",
    "    \n",
    "    return np.vstack(tp_coordinates),stacked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fc9653-576c-4e90-bb65-a82d652f6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_time_series_data(time_series_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Groups the data based on case_ID and time \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    ### load raw data from ze time series data ###\n",
    "    pd_list  = list()\n",
    "    \n",
    "    for (col1_val, col2_val), group in time_series_data.groupby(['case_ID', 'time']):\n",
    "    \n",
    "        pd_list.append(group)\n",
    "    \n",
    "    return pd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c4f4e0-3397-4a2a-ba51-141f5eb0b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearest_neighbor_data(time_series_data,num_nearest_neighbors):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper function (in some sense, can be condensed more)to do the data generation \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    \n",
    "    pd_list = group_time_series_data(time_series_data)\n",
    "    \n",
    "    nearest_neighbor_data = list()\n",
    "    nearest_neighbor_data_extra = list()\n",
    "    scalar_data = list()\n",
    "    \n",
    "    ### Loop over different groups ###\n",
    "    \n",
    "    for i in range(len(pd_list)):\n",
    "        \n",
    "        print(\"Currently on case_time subgroup : \",str(i+1))\n",
    "        tp_particles,stacked_df = get_periodic_coordinates(pd_list[i],5)\n",
    "        tree = cKDTree(tp_particles)\n",
    "        \n",
    "        ### Loop over all particles in a group and getting the nearest neighbors ###\n",
    "        idx = np.stack([ tree.query(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values,int(num_nearest_neighbors+1))[1] for j in range(len(pd_list[i])) ])\n",
    "        nearest_neighbor_data.append(tp_particles[idx])\n",
    "        \n",
    "        ### merging nodal data to the coordinates ###\n",
    "        nearest_neighbor_data_extra.append(merge_columns_to_pandas_list(tp_particles[idx],\"local_Re\",stacked_df))\n",
    "        \n",
    "        ### Getting the scalar data ###\n",
    "        scalar_data.append( pd_list[i][[\"Density_ratio\",\"glb_phi\",\"glb_Re\",\"local_Re\",\"Drag\"]] )\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    ### Populate graph and scalar lists ###\n",
    "    nearest_neighbor_data = np.stack(nearest_neighbor_data)\n",
    "    nearest_neighbor_data_extra = np.stack(nearest_neighbor_data_extra)\n",
    "    \n",
    "    nearest_neighbor_data = nearest_neighbor_data.reshape(nearest_neighbor_data.shape[0]*nearest_neighbor_data.shape[1]\n",
    "                                           ,nearest_neighbor_data.shape[2]*nearest_neighbor_data.shape[3])\n",
    "    \n",
    "    nearest_neighbor_data_extra = nearest_neighbor_data_extra.reshape(nearest_neighbor_data_extra.shape[0]*nearest_neighbor_data_extra.shape[1]\n",
    "                                           ,nearest_neighbor_data_extra.shape[2]*nearest_neighbor_data_extra.shape[3])\n",
    "    \n",
    "    scalar_data = np.stack(scalar_data)\n",
    "    scalar_data = scalar_data.reshape(scalar_data.shape[0]*scalar_data.shape[1],scalar_data.shape[2])    \n",
    "    \n",
    "    ### change code if you want to return nearest_neighbor_data or extra ### \n",
    "    return np.concatenate( (nearest_neighbor_data_extra,scalar_data) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7561cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns_to_pandas_list(nearest_neighbor_data,variable_list,master_dataframe):\n",
    "\n",
    "    \"\"\" given a list of pandas dataframe with the x,y,z locations and re and phi ,this function will\n",
    "        merge each pandas dataframe from the list with the master dataframe with all the columns  \n",
    "    \"\"\"\n",
    "\n",
    "    joined =[pd.DataFrame(nearest_neighbor_data[i],columns=[\"x\",\"y\",\"z\"]) for i in range(len(nearest_neighbor_data))]\n",
    "\n",
    "    for i in range(len(joined)):\n",
    "        \n",
    "        temp = copy.deepcopy(joined[i])\n",
    "        add = pd.merge(temp,master_dataframe,how=\"inner\",on=['x','y','z'],sort=False)[variable_list]\n",
    "        joined[i] = pd.concat([temp,add], axis=1)\n",
    "        \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e23f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_nearest_neighbor_data(nearest_neighbor_data,pd_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes nearest neighbor data and the pd_list and it will return a pandas dataframe with each row\n",
    "    having the particle ID (integer), the time step (integer) and the case (integer) of which the particle is a part of\n",
    "    ,and the remaining columns will be the nearest neighbor row itself.\n",
    "    \"\"\"\n",
    "    case_column = np.stack( [ pd_list[i][\"case_ID\"].values for i in range(len(pd_list)) ] ).flatten()\n",
    "    particle_id_column = np.stack( [ np.arange(pd_list[i].shape[0])+1 for i in range(len(pd_list)) ] ).flatten()\n",
    "    time_column = np.stack( [ pd_list[i][\"time\"].values for i in range(len(pd_list)) ] ).flatten()\n",
    "    \n",
    "    ### Combining columns with nearest_neighbor_data ###\n",
    "    nearest_neighbor_data_modified = np.concatenate( (case_column[:,None],particle_id_column[:,None],time_column[:,None]\n",
    "                ,nearest_neighbor_data),axis=1 )\n",
    "    \n",
    "    return nearest_neighbor_data_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87578c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    def generate_temporally_history_datasets_for_single_group(single_df,history_length,sampling_rate):\n",
    "        \n",
    "#         \"\"\"\n",
    "#         performs the data generation for a single group\n",
    "#         \"\"\"\n",
    "#         start_index = history_length*sampling_rate\n",
    "        \n",
    "#         for i in range(start_index,len(single_df)):\n",
    "            \n",
    "#             extracted_sequences = [ [single_df.iloc[k - j * sampling_rate] for j in range(history_length + 1)]\n",
    "#                                     for k in range(start_index, len(single_df)) ]\n",
    "                              \n",
    "#         extracted_sequences = [pd.concat(series_list, axis=1).T for series_list in extracted_sequences]\n",
    "        \n",
    "#         return extracted_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d4ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporally_history_datasets_for_single_group(single_df, history_length, sampling_rate):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs the data generation for a single group\n",
    "    \"\"\"\n",
    "    \n",
    "    start_index = history_length * sampling_rate\n",
    "    num_rows = len(single_df)\n",
    "    indices = []\n",
    "\n",
    "    # Create a list of indices to extract\n",
    "    for i in range(start_index, num_rows):\n",
    "        indices.append([i - j * sampling_rate for j in range(history_length + 1)])\n",
    "    \n",
    "    \n",
    "    # Flatten the list of indices\n",
    "    flat_indices = [index for sublist in indices for index in sublist]\n",
    "\n",
    "    # Use iloc to extract all needed rows at once\n",
    "    extracted_sequences = single_df.iloc[flat_indices]\n",
    "\n",
    "    # Reshape the dataframe to have the desired format\n",
    "    reshaped_data = []\n",
    "    for i in range(0, len(flat_indices), history_length + 1):\n",
    "        reshaped_data.append(extracted_sequences.iloc[i:i + history_length + 1].reset_index(drop=True))\n",
    "    \n",
    "    return reshaped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "305234b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporally_history_datasets(grouped_dfs,history_length=3,sampling_rate=2):\n",
    "    \n",
    "    \"\"\" \n",
    "    Given a list of pandas dataframes where each element is the temporal trajectory of one particle\n",
    "    , this functions operates on each of the elements and gives historical time data points, for instance if\n",
    "    the trajectory has 100 time steps, and the history is length is 3 with the sampling rate being 2. The first\n",
    "    data point will be of timestep 1-3-5 and the label would be the drag from timestep 5, second datapoint would be \n",
    "    2-4-6 and the label would be the drag at 6 and so on.\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_sequences = list()\n",
    "    \n",
    "    for i in range(len(grouped_dfs)):\n",
    "        \n",
    "        print(\"Currently on particle number : \",str(i+1))\n",
    "        \n",
    "        extracted_sequences.append( generate_temporally_history_datasets_for_single_group( grouped_dfs[i] , history_length \n",
    "                                                                                , sampling_rate) )\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    reversed_extracted_sequences = [[df.iloc[::-1].reset_index(drop=True) for df in sublist] for sublist in extracted_sequences]\n",
    "    \n",
    "    return reversed_extracted_sequences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8fda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_over_all_levels(data):\n",
    "    \n",
    "    # Flatten the list of lists into a single list of DataFrames\n",
    "    flattened_list = [df for sublist in data for df in sublist]\n",
    "\n",
    "    # Concatenate all DataFrames in the flattened list into a single DataFrame\n",
    "    combined_dataframe = pd.concat(flattened_list, ignore_index=True)\n",
    "    \n",
    "    return combined_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c129c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(nearest_neighbor_data_modified,num_nearest_neighbors):\n",
    "    \n",
    "    ### Define new column names ###\n",
    "    new_column_names = {0: 'case', 1: 'particle_ID', 2: 'time',3:\"x_poi\",4:\"y_poi\",5:\"z_poi\",6:\"local_Re\"}\n",
    "\n",
    "    for i in range(num_nearest_neighbors):\n",
    "\n",
    "        new_column_names[len(new_column_names)] = \"x_\" + str(i + 1)\n",
    "        new_column_names[len(new_column_names)] = \"y_\" + str(i + 1)\n",
    "        new_column_names[len(new_column_names)] = \"z_\" + str(i + 1)\n",
    "        new_column_names[len(new_column_names)] = \"local_Re_\" + str(i + 1)\n",
    "\n",
    "    new_column_names[len(new_column_names)] = \"density_ratio\"\n",
    "    new_column_names[len(new_column_names)] = \"glb_phi\"\n",
    "    new_column_names[len(new_column_names)] = \"glb_Re\"\n",
    "    new_column_names[len(new_column_names)] = \"local_Re_aux\"\n",
    "    new_column_names[len(new_column_names)] = \"Drag\"\n",
    "    \n",
    "    return nearest_neighbor_data_modified.rename(columns=new_column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a12476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_xyz_columns_and_others(df):\n",
    "#     # Initialize the MinMaxScaler\n",
    "#     scaler = MinMaxScaler()\n",
    "\n",
    "#     # Iterate over each column in the dataframe\n",
    "#     for col in df.columns:\n",
    "#         if col.startswith('x_') or col.startswith('y_') or col.startswith('z_'):\n",
    "#             # Apply the custom scaling operation to each column that starts with 'x_', 'y_', or 'z_'\n",
    "#             df[col] = (df[col] - (-5)) / (10 - (-5))\n",
    "#         else:\n",
    "#             # Collect the other columns to be scaled using MinMaxScaler\n",
    "#             df[col] = scaler.fit_transform(df[[col]])\n",
    "            \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "972fb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_xyz_and_other_columns(train_df, test_df):\n",
    "    \n",
    "#     # Initialize the MinMaxScaler\n",
    "#     scaler = MinMaxScaler()\n",
    "# #     scaler = RobustScaler()\n",
    "#     train_df_copy,test_df_copy = train_df.copy(), test_df.copy()\n",
    "    \n",
    "#     # Iterate over each column in the dataframe\n",
    "#     for col in train_df_copy.columns:\n",
    "#         if col.startswith('x_') or col.startswith('y_') or col.startswith('z_'):\n",
    "#             # Apply the custom scaling operation to each column that starts with 'x_', 'y_', or 'z_'\n",
    "#             train_df_copy[col] = (train_df_copy[col] - (-5)) / (10 - (-5))\n",
    "#             test_df_copy[col] = (test_df_copy[col] - (-5)) / (10 - (-5))\n",
    "#         else:\n",
    "#             # Fit the scaler on the train data and transform both train and test data\n",
    "#             train_df_copy[col] = scaler.fit_transform(train_df_copy[[col]])\n",
    "#             test_df_copy[col] = scaler.transform(test_df_copy[[col]])\n",
    "            \n",
    "#     return train_df_copy, test_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87d283b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_xyz_and_other_columns(train_df, test_df):\n",
    "    ### Initialize the scaler ###\n",
    "    scaler = RobustScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    train_df_copy, test_df_copy = train_df.copy(), test_df.copy()\n",
    "    \n",
    "    # Find the columns starting with specific prefixes\n",
    "    x_columns = [col for col in train_df.columns if col.startswith('x_')]\n",
    "    y_columns = [col for col in train_df.columns if col.startswith('y_')]\n",
    "    z_columns = [col for col in train_df.columns if col.startswith('z_')]\n",
    "    \n",
    "    local_re_columns = [col for col in train_df.columns if col.startswith('local_Re')]\n",
    "\n",
    "    vpx_columns = [col for col in train_df.columns if col.startswith('vpx_')]\n",
    "    vpy_columns = [col for col in train_df.columns if col.startswith('vpy_')]\n",
    "    vpz_columns = [col for col in train_df.columns if col.startswith('vpz_')]\n",
    "\n",
    "    # Custom scaling for x_, y_, and z_ columns\n",
    "    for col in x_columns + y_columns + z_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - (-5)) / (10 - (-5))\n",
    "        test_df_copy[col] = (test_df_copy[col] - (-5)) / (10 - (-5))\n",
    "\n",
    "    # Compute global max for local_Re_, vpx_, vpy_, and vpz_ and disp_x,disp_y,disp_z columns from train data\n",
    "    global_max_local_re = train_df[local_re_columns].max().max()\n",
    "\n",
    "    global_max_vpx = train_df[vpx_columns].max().max()\n",
    "    global_max_vpy = train_df[vpy_columns].max().max()\n",
    "    global_max_vpz = train_df[vpz_columns].max().max()\n",
    "    \n",
    "    disp_x_max = train_df[\"disp_x\"].max()\n",
    "    disp_y_max = train_df[\"disp_y\"].max()\n",
    "    disp_z_max = train_df[\"disp_z\"].max()\n",
    "\n",
    "    # Compute global min for local_Re_, vpx_, vpy_, and vpz_  and disp_x,disp_y,disp_z columns from train data\n",
    "    global_min_local_re = train_df[local_re_columns].min().min()\n",
    "\n",
    "    global_min_vpx = train_df[vpx_columns].min().min()\n",
    "    global_min_vpy = train_df[vpy_columns].min().min()\n",
    "    global_min_vpz = train_df[vpz_columns].min().min()\n",
    "    \n",
    "    disp_x_min = train_df[\"disp_x\"].min()\n",
    "    disp_y_min = train_df[\"disp_y\"].min()\n",
    "    disp_z_min = train_df[\"disp_z\"].min()\n",
    "\n",
    "    # Scaling for local_Re_ columns\n",
    "    for col in local_re_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_local_re) / (global_max_local_re - global_min_local_re)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_local_re) / (global_max_local_re - global_min_local_re)\n",
    "\n",
    "    # Scaling for vpx_ columns\n",
    "    for col in vpx_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_vpx) / (global_max_vpx - global_min_vpx)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_vpx) / (global_max_vpx - global_min_vpx)\n",
    "\n",
    "    # Scaling for vpy_ columns\n",
    "    for col in vpy_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_vpy) / (global_max_vpy - global_min_vpy)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_vpy) / (global_max_vpy - global_min_vpy)\n",
    "\n",
    "    # Scaling for vpz_ columns\n",
    "    for col in vpz_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_vpz) / (global_max_vpz - global_min_vpz)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_vpz) / (global_max_vpz - global_min_vpz)\n",
    "        \n",
    "    # Scaling the displacement columns \n",
    "    train_df[\"disp_x\"] = (train_df[\"disp_x\"]-disp_x_min)/(disp_x_max-disp_x_min)\n",
    "    train_df[\"disp_y\"] = (train_df[\"disp_y\"]-disp_y_min)/(disp_y_max-disp_y_min)\n",
    "    train_df[\"disp_z\"] = (train_df[\"disp_z\"]-disp_z_min)/(disp_z_max-disp_z_min)\n",
    "    \n",
    "    # Scaling for other columns using MinMaxScaler\n",
    "    for col in train_df.columns:\n",
    "        if not (col.startswith('x_') or col.startswith('y_') or col.startswith('z_') or \n",
    "                col.startswith('local_Re') or col.startswith('vpx_') or \n",
    "                col.startswith('vpy_') or col.startswith('vpz_') or col.startswith('vpz_') or \n",
    "                col.startswith('disp_')):\n",
    "            \n",
    "            print(\"Cols with robust scaler : \",col)\n",
    "            train_df_copy[col] = scaler.fit_transform(train_df_copy[[col]])\n",
    "            test_df_copy[col] = scaler.transform(test_df_copy[[col]])\n",
    "            \n",
    "    return train_df_copy, test_df_copy,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a88cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "def plot_3d_trajectory(trajectory,color=\"red\",title=\"plot\",limits=[-5,10],middle_cube=\"big\"):\n",
    "    \"\"\"\n",
    "    Plots a 3D trajectory given a NumPy array with x, y, z coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - trajectory: A NumPy array of shape (n, 3), where n is the number of timesteps,\n",
    "                  and each row represents the [x, y, z] coordinates at a timestep.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a NumPy array\n",
    "    trajectory = np.array(trajectory)\n",
    "    \n",
    "    # Check if the input has the correct shape\n",
    "#     if trajectory.shape[1] != 3:\n",
    "#         raise ValueError(\"Input array must have shape (n, 3) where n is the number of timesteps.\")\n",
    "    \n",
    "    # Extract x, y, z coordinates\n",
    "    x = trajectory[:, 0]\n",
    "    y = trajectory[:, 1]\n",
    "    z = trajectory[:, 2]\n",
    "    \n",
    "    # Create a 3D plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the trajectory\n",
    "    ax.scatter(x[0], y[0], z[0], label='3D trajectory',c=\"red\",s=100)\n",
    "    ax.scatter(x[1:], y[1:], z[1:], label='3D trajectory',c=color,s=25,alpha=0.25)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # set limits\n",
    "    ax.set_xlim([limits[0],limits[1]])\n",
    "    ax.set_ylim([limits[0],limits[1]])\n",
    "    ax.set_zlim([limits[0],limits[1]])\n",
    "    \n",
    "    # Show legend\n",
    "    ax.legend()\n",
    "    \n",
    "    if middle_cube==\"big\":\n",
    "        # Create vertices for the cube\n",
    "        vertices = [[0, 0, 0], [5, 0, 0], [5, 5, 0], [0, 5, 0],\n",
    "                [0, 0, 5], [5, 0, 5], [5, 5, 5], [0, 5, 5]]\n",
    "    \n",
    "    if middle_cube==\"small\":\n",
    "        \n",
    "        # Create vertices for the cube\n",
    "        vertices = [[0.333, 0.333, 0.333], [0.666, 0.333, 0.333], [0.666, 0.666, 0.333], [0.333, 0.666, 0.333],\n",
    "                [0.333, 0.333, 0.666], [0.666, 0.333, 0.666], [0.666, 0.666, 0.666], [0.333, 0.666, 0.666]]\n",
    "    \n",
    "    # Define the 6 faces of the cube\n",
    "    faces = [[vertices[j] for j in [0, 1, 2, 3]],\n",
    "             [vertices[j] for j in [4, 5, 6, 7]], \n",
    "             [vertices[j] for j in [0, 1, 5, 4]], \n",
    "             [vertices[j] for j in [2, 3, 7, 6]], \n",
    "             [vertices[j] for j in [1, 2, 6, 5]], \n",
    "             [vertices[j] for j in [4, 0, 3, 7]]]\n",
    "    \n",
    "    # Create a Poly3DCollection for the cube\n",
    "    cube = Poly3DCollection(faces, alpha=0.1, facecolors='cyan')\n",
    "    \n",
    "    \n",
    "    ax.add_collection3d(cube)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0d93504-ae34-4f09-b4f2-cf14bf501100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on case_time subgroup :  443\n"
     ]
    }
   ],
   "source": [
    "# ### Read data ###\n",
    "experiment = \"rho2_20percent_Re100\"\n",
    "\n",
    "time_series_data = pd.read_csv(\"../../ze_time_series_data_raw/data_with_xyz_velocities/\"+experiment+\".dat\",index_col=False)\n",
    "pd_list = group_time_series_data(time_series_data)\n",
    "\n",
    "### generate nearest neighbor data ###\n",
    "num_nearest_neighbors = 3\n",
    "nearest_neighbor_data = generate_nearest_neighbor_data(time_series_data,num_nearest_neighbors)\n",
    "\n",
    "### add particle id, case and time column to the dataset ###\n",
    "nearest_neighbor_data_modified = modify_nearest_neighbor_data(nearest_neighbor_data,pd_list)\n",
    "nearest_neighbor_data_modified = pd.DataFrame(nearest_neighbor_data_modified)\n",
    "save=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b305a118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>particle_ID</th>\n",
       "      <th>time</th>\n",
       "      <th>x_poi</th>\n",
       "      <th>y_poi</th>\n",
       "      <th>z_poi</th>\n",
       "      <th>local_Re</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>local_Re_1</th>\n",
       "      <th>density_ratio</th>\n",
       "      <th>glb_phi</th>\n",
       "      <th>glb_Re</th>\n",
       "      <th>local_Re_aux</th>\n",
       "      <th>Drag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.63471</td>\n",
       "      <td>2.512280</td>\n",
       "      <td>2.532390</td>\n",
       "      <td>102.648781</td>\n",
       "      <td>3.629660</td>\n",
       "      <td>2.041030</td>\n",
       "      <td>2.206200</td>\n",
       "      <td>99.381715</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.648781</td>\n",
       "      <td>12.711219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.49839</td>\n",
       "      <td>3.529310</td>\n",
       "      <td>1.308230</td>\n",
       "      <td>76.218555</td>\n",
       "      <td>2.790870</td>\n",
       "      <td>3.058350</td>\n",
       "      <td>0.610705</td>\n",
       "      <td>71.660712</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>76.218555</td>\n",
       "      <td>11.988314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.71764</td>\n",
       "      <td>4.030450</td>\n",
       "      <td>4.377230</td>\n",
       "      <td>93.296803</td>\n",
       "      <td>2.009580</td>\n",
       "      <td>3.225240</td>\n",
       "      <td>4.025480</td>\n",
       "      <td>85.270485</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.296803</td>\n",
       "      <td>10.516945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.65962</td>\n",
       "      <td>0.752162</td>\n",
       "      <td>0.526114</td>\n",
       "      <td>96.815827</td>\n",
       "      <td>4.439864</td>\n",
       "      <td>0.444844</td>\n",
       "      <td>1.237750</td>\n",
       "      <td>128.037675</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.815827</td>\n",
       "      <td>16.419202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.17826</td>\n",
       "      <td>0.785864</td>\n",
       "      <td>1.654650</td>\n",
       "      <td>93.572512</td>\n",
       "      <td>3.659620</td>\n",
       "      <td>0.752162</td>\n",
       "      <td>0.526114</td>\n",
       "      <td>96.815827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.572512</td>\n",
       "      <td>13.938380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21259</th>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1.09900</td>\n",
       "      <td>3.856810</td>\n",
       "      <td>4.333542</td>\n",
       "      <td>104.247340</td>\n",
       "      <td>0.388460</td>\n",
       "      <td>4.298040</td>\n",
       "      <td>5.048612</td>\n",
       "      <td>85.645202</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.247340</td>\n",
       "      <td>9.914260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21260</th>\n",
       "      <td>2.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>3.96937</td>\n",
       "      <td>3.293380</td>\n",
       "      <td>3.942260</td>\n",
       "      <td>80.274175</td>\n",
       "      <td>4.500100</td>\n",
       "      <td>4.250290</td>\n",
       "      <td>3.833680</td>\n",
       "      <td>68.046004</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>80.274175</td>\n",
       "      <td>14.761188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21261</th>\n",
       "      <td>2.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>2.71975</td>\n",
       "      <td>4.287516</td>\n",
       "      <td>1.591660</td>\n",
       "      <td>94.079438</td>\n",
       "      <td>3.027730</td>\n",
       "      <td>5.084540</td>\n",
       "      <td>0.710190</td>\n",
       "      <td>110.418150</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>94.079438</td>\n",
       "      <td>14.175416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21262</th>\n",
       "      <td>2.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>3.92364</td>\n",
       "      <td>1.290420</td>\n",
       "      <td>2.639290</td>\n",
       "      <td>86.991999</td>\n",
       "      <td>4.986120</td>\n",
       "      <td>1.647000</td>\n",
       "      <td>2.724220</td>\n",
       "      <td>74.191998</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>86.991999</td>\n",
       "      <td>12.139554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21263</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>3.44493</td>\n",
       "      <td>1.495750</td>\n",
       "      <td>3.831960</td>\n",
       "      <td>103.965327</td>\n",
       "      <td>4.603500</td>\n",
       "      <td>0.944949</td>\n",
       "      <td>3.856840</td>\n",
       "      <td>84.031062</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>103.965327</td>\n",
       "      <td>9.535640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21264 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  particle_ID   time    x_poi     y_poi     z_poi    local_Re  \\\n",
       "0       1.0          1.0    1.0  2.63471  2.512280  2.532390  102.648781   \n",
       "1       1.0          2.0    1.0  3.49839  3.529310  1.308230   76.218555   \n",
       "2       1.0          3.0    1.0  2.71764  4.030450  4.377230   93.296803   \n",
       "3       1.0          4.0    1.0  3.65962  0.752162  0.526114   96.815827   \n",
       "4       1.0          5.0    1.0  3.17826  0.785864  1.654650   93.572512   \n",
       "...     ...          ...    ...      ...       ...       ...         ...   \n",
       "21259   2.0         44.0  276.0  1.09900  3.856810  4.333542  104.247340   \n",
       "21260   2.0         45.0  276.0  3.96937  3.293380  3.942260   80.274175   \n",
       "21261   2.0         46.0  276.0  2.71975  4.287516  1.591660   94.079438   \n",
       "21262   2.0         47.0  276.0  3.92364  1.290420  2.639290   86.991999   \n",
       "21263   2.0         48.0  276.0  3.44493  1.495750  3.831960  103.965327   \n",
       "\n",
       "            x_1       y_1       z_1  local_Re_1  density_ratio  glb_phi  \\\n",
       "0      3.629660  2.041030  2.206200   99.381715            2.0      0.2   \n",
       "1      2.790870  3.058350  0.610705   71.660712            2.0      0.2   \n",
       "2      2.009580  3.225240  4.025480   85.270485            2.0      0.2   \n",
       "3      4.439864  0.444844  1.237750  128.037675            2.0      0.2   \n",
       "4      3.659620  0.752162  0.526114   96.815827            2.0      0.2   \n",
       "...         ...       ...       ...         ...            ...      ...   \n",
       "21259  0.388460  4.298040  5.048612   85.645202            2.0      0.2   \n",
       "21260  4.500100  4.250290  3.833680   68.046004            2.0      0.2   \n",
       "21261  3.027730  5.084540  0.710190  110.418150            2.0      0.2   \n",
       "21262  4.986120  1.647000  2.724220   74.191998            2.0      0.2   \n",
       "21263  4.603500  0.944949  3.856840   84.031062            2.0      0.2   \n",
       "\n",
       "       glb_Re  local_Re_aux       Drag  \n",
       "0       100.0    102.648781  12.711219  \n",
       "1       100.0     76.218555  11.988314  \n",
       "2       100.0     93.296803  10.516945  \n",
       "3       100.0     96.815827  16.419202  \n",
       "4       100.0     93.572512  13.938380  \n",
       "...       ...           ...        ...  \n",
       "21259   100.0    104.247340   9.914260  \n",
       "21260   100.0     80.274175  14.761188  \n",
       "21261   100.0     94.079438  14.175416  \n",
       "21262   100.0     86.991999  12.139554  \n",
       "21263   100.0    103.965327   9.535640  \n",
       "\n",
       "[21264 rows x 16 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbor_data_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### splitting the data such that each grouped df is the trajectory of a single particle across all cases ###\n",
    "### Renaming the columns\n",
    "rename_columns(nearest_neighbor_data_modified,num_nearest_neighbors)\n",
    "\n",
    "### Getting the mean drag force ###\n",
    "# mean_drag_force = nearest_neighbor_data_modified[\"Drag\"].mean() ### including negative drag forces\n",
    "mean_drag_force = nearest_neighbor_data_modified[nearest_neighbor_data_modified[\"Drag\"]>=0][\"Drag\"].values.mean() ### not including negative drag forces\n",
    "\n",
    "### each element of grouped_dfs is a particle and its trajectory ###\n",
    "nearest_neighbor_data_grouped = nearest_neighbor_data_modified.groupby([\"case\",\"particle_ID\"])\n",
    "grouped_dfs = [group for _, group in nearest_neighbor_data_grouped]\n",
    "\n",
    "### IMPORTANT : generating the sequential data ###\n",
    "history_length = 5\n",
    "sampling_rate = 1\n",
    "extracted_sequences = generate_temporally_history_datasets(grouped_dfs,history_length=history_length,\n",
    "                                                           sampling_rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136efd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop outlier drag forces (when train data HAS input drag forces,here the mean will be caluclated on the target   ###\n",
    "### drag forces , but the datapoints will be rejected if any of the drags in the input sequence will not satisfy the ###\n",
    "### the condition ###\n",
    "\n",
    "idx = list()\n",
    "\n",
    "### Finding indices not satisfying the condition ### \n",
    "for i in range(len(extracted_sequences)):\n",
    "    for j in range(len(extracted_sequences[i])):\n",
    "        \n",
    "        \n",
    "        # if np.all( np.abs(extracted_sequences[i][j][\"Drag\"].values) < np.abs(mean_drag_force)*3 ):\n",
    "        if np.all( (extracted_sequences[i][j][\"Drag\"].values > 0 )&\n",
    "                   (np.abs(extracted_sequences[i][j][\"Drag\"].values) < np.abs(mean_drag_force)*3) ):\n",
    "                                                                     \n",
    "            idx.append([i,j])\n",
    "\n",
    "            \n",
    "### Filtering out the indices ### \n",
    "print(\"number of datapoints before filterting : \",np.sum([len(extracted_sequences[i]) \n",
    "                                                  for i in range(len(extracted_sequences))]))\n",
    "\n",
    "extracted_sequences = [\n",
    "    [df for j, df in enumerate(sublist) if [i, j]  in idx]\n",
    "    for i, sublist in enumerate(extracted_sequences)]\n",
    "\n",
    "print(\"number of datapoints before filterting : \",np.sum([len(extracted_sequences[i]) \n",
    "                                                  for i in range(len(extracted_sequences))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_local(dataframe,num_neighbors=5):\n",
    "    \n",
    "    ### converts the location of nearest neighbors coordinates from global to local ###\n",
    "    ### the poi coordinates are still global ###\n",
    "    \n",
    "    # List of prefixes to modify\n",
    "    prefixes = ['x_', 'y_', 'z_']\n",
    "\n",
    "    # Loop through each prefix\n",
    "    for prefix in prefixes:\n",
    "        for i in range(1, num_neighbors+1):\n",
    "            column_name = f'{prefix}{i}'\n",
    "            dataframe[column_name] = dataframe[column_name] - dataframe[f'{prefix}poi']\n",
    "            \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74826557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_displacement_columns(dataframe):\n",
    "    \n",
    "    ### Adds three columns about the x,y,z displacmenent between timesteps ###\n",
    "    \n",
    "    dataframe['disp_x'] = dataframe['x_poi'] - dataframe['x_poi'].shift(1)\n",
    "    dataframe['disp_y'] = dataframe['y_poi'] - dataframe['y_poi'].shift(1)\n",
    "    dataframe['disp_z'] = dataframe['z_poi'] - dataframe['z_poi'].shift(1)\n",
    "    \n",
    "    ### make the first arrangement (first index) equal to zero ###\n",
    "    dataframe.loc[dataframe.index == 0, ['disp_x', 'disp_y', 'disp_z']] = 0\n",
    "    train_dataset[[\"x_poi\",\"y_poi\",\"z_poi\"]] = 0\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88195b4e",
   "metadata": {},
   "source": [
    "# Train Val Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For splitting data as train and test (half time Datasplit) ###\n",
    "import pandas as pd\n",
    "\n",
    "# Example data setup (assuming df_list_of_lists is your list of list of DataFrames)\n",
    "# df_list_of_lists = [[df1, df2], [df3, df4], ...]\n",
    "\n",
    "# Define your time limits for each case\n",
    "half_time_1 = int(grouped_dfs[0].time.values[-1]*0.80)  # Replace with your actual limit for case_1\n",
    "half_time_2 = int(grouped_dfs[-1].time.values[-1]*0.80)  # Replace with your actual limit for case_2\n",
    "\n",
    "# Initialize the lists to collect pairs (i, j)\n",
    "train_pairs = []\n",
    "test_pairs = []\n",
    "\n",
    "# Iterate through the outer and inner lists\n",
    "for i, outer_list in enumerate(extracted_sequences):\n",
    "    \n",
    "    for j, df in enumerate(outer_list):\n",
    "        # Check the case and apply the appropriate time filter\n",
    "        if 'case' in df.columns and 'time' in df.columns:\n",
    "            \n",
    "            ### Train Pairs ###\n",
    "            if all(df[\"case\"].values==1) & all(df[\"time\"].values<=half_time_1):\n",
    "                train_pairs.append([i, j])\n",
    "                                   \n",
    "            if all(df[\"case\"].values==2) & all(df[\"time\"].values<=half_time_2):\n",
    "                train_pairs.append([i, j])\n",
    "                                                          \n",
    "            ### Test Pairs ###\n",
    "            if all(df[\"case\"].values==1) & any(df[\"time\"].values>half_time_1):\n",
    "                test_pairs.append([i, j])\n",
    "                                   \n",
    "            if all(df[\"case\"].values==2) & any(df[\"time\"].values>half_time_2):\n",
    "                test_pairs.append([i, j])\n",
    "                \n",
    "train_dataset = pd.concat([extracted_sequences[i][j] for (i, j) in train_pairs])\n",
    "test_dataset = pd.concat([extracted_sequences[i][j] for (i, j) in test_pairs])\n",
    "\n",
    "### shift the new functions here ###\n",
    "train_dataset = convert_to_local(train_dataset,num_neighbors=num_nearest_neighbors)\n",
    "train_dataset = add_displacement_columns(train_dataset)\n",
    "\n",
    "test_dataset = convert_to_local(test_dataset,num_neighbors=num_nearest_neighbors)\n",
    "test_dataset = add_displacement_columns(test_dataset)\n",
    "\n",
    "### Shift the drag columns to the rightmost position ###\n",
    "column_to_move = 'Drag'\n",
    "\n",
    "columns = [col for col in train_dataset.columns if col != column_to_move] + [column_to_move]\n",
    "train_dataset = train_dataset.reindex(columns=columns)\n",
    "test_dataset = test_dataset.reindex(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e6b70",
   "metadata": {},
   "source": [
    "\n",
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2701e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scaling the data ###\n",
    "train_dataset_scaled, test_dataset_scaled , scaler = scale_xyz_and_other_columns(train_dataset,test_dataset)\n",
    "\n",
    "### if poi coordinates (now doing this as default)###\n",
    "### Set poi to zeros ###\n",
    "train_dataset_scaled[[\"x_poi\",\"y_poi\",\"z_poi\"]] = 0\n",
    "test_dataset_scaled[[\"x_poi\",\"y_poi\",\"z_poi\"]] = 0\n",
    "\n",
    "### Reshaping by chunking the data ### \n",
    "n_train = train_dataset.shape[0]//(extracted_sequences[0][0].shape[0])\n",
    "train_dataset_scaled = np.array_split(train_dataset_scaled.values[:,3:],n_train,axis=0)\n",
    "\n",
    "n_test = test_dataset.shape[0]//(extracted_sequences[0][0].shape[0])\n",
    "test_dataset_scaled  = np.array_split(test_dataset_scaled.values[:,3:],n_test,axis=0)\n",
    "\n",
    "### if inputs with drag forces, make the output (last drag in each segment as zero) extract it and save it as output ###\n",
    "### train and test drag forces ###\n",
    "\n",
    "train_drag_forces_scaled = list()\n",
    "for i in range(len(train_dataset_scaled)):\n",
    "    train_drag_forces_scaled.append(train_dataset_scaled[i][-1][-1])\n",
    "    train_dataset_scaled[i][-1][-1] = 0 \n",
    "    \n",
    "train_drag_forces_scaled = np.stack(train_drag_forces_scaled)\n",
    "\n",
    "test_drag_forces_scaled = list()\n",
    "for i in range(len(test_dataset_scaled)):\n",
    "    test_drag_forces_scaled.append(test_dataset_scaled[i][-1][-1])\n",
    "    test_dataset_scaled[i][-1][-1] = 0 \n",
    "    \n",
    "test_drag_forces_scaled = np.stack(test_drag_forces_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drags auxilliary ###\n",
    "# train_drag_forces_unscaled = np.stack( [extracted_sequences[i][j][\"Drag\"].values[-1] for (i, j) in train_pairs] )[:,None]\n",
    "# test_drag_forces_unscaled = np.stack( [extracted_sequences[i][j][\"Drag\"].values[-1] for (i, j) in test_pairs] )[:,None]\n",
    "\n",
    "# scaler = RobustScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(train_drag_forces_unscaled)\n",
    "\n",
    "# train_drag_forces_scaled = scaler.transform(train_drag_forces_unscaled)\n",
    "# test_drag_forces_scaled = scaler.transform(test_drag_forces_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b415320",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the raw data trajectory ###\n",
    "single_particle_trajectory = grouped_dfs[0].values[:,3:]\n",
    "k=0\n",
    "\n",
    "# for i in range(len(single_particle_trajectory)):\n",
    "for i in range(0,len(single_particle_trajectory),sampling_rate):\n",
    "    \n",
    "    if k==history_length+1:\n",
    "        break\n",
    "    \n",
    "    title = \" time step number : \"+str(i)\n",
    "    plot_3d_trajectory(single_particle_trajectory[i][0:(num_nearest_neighbors+1)*4].reshape((num_nearest_neighbors+1),4),color=\"green\",title = title,limits=[-5,10])\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdd711",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = single_particle_trajectory[0][0:(num_nearest_neighbors+1)*4].reshape((num_nearest_neighbors+1),4)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd2346",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plotting the input data trajectory ###\n",
    "\n",
    "for i in range(1):\n",
    "    for j in range(history_length):\n",
    "        \n",
    "        plot_3d_trajectory(train_dataset_scaled[i][j][0:(num_nearest_neighbors+1)*4].reshape((num_nearest_neighbors+1),4),color=\"green\",title = \"time step : \"+str(j+1) \n",
    "                          ,limits=[0,1],middle_cube=\"small\")\n",
    "        clear_output(wait=True)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d17ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2 = train_dataset_scaled[0][0][0:(num_nearest_neighbors+1)*4].reshape((num_nearest_neighbors+1),4)\n",
    "temp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db827525",
   "metadata": {},
   "outputs": [],
   "source": [
    "((temp[:,0:3] - temp[0,0:3])+5)/15 - temp_2[:,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d049bf1",
   "metadata": {},
   "source": [
    "# New Saving location Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0a9de2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the list to a pickle file (scaled) ###\n",
    "\n",
    "### Train data ###\n",
    "with open('../num_neighbors_and_time_influence/training_and_testing_data/'+experiment+'/ridiculous/train_data_np_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dataset_scaled, f)\n",
    "    \n",
    "### Test data ### \n",
    "with open('../num_neighbors_and_time_influence/training_and_testing_data/'+experiment+'/ridiculous/test_data_np_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dataset_scaled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b02541df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save drag forces (scaled and unscaled) ###\n",
    "\n",
    "### Train data ###\n",
    "np.save('../num_neighbors_and_time_influence/training_and_testing_data/'+experiment+'/ridiculous_2/train_output_scaled'\n",
    "        ,train_drag_forces_scaled)\n",
    "\n",
    "### Test data ###\n",
    "np.save('../num_neighbors_and_time_influence/training_and_testing_data/'+experiment+'/ridiculous_2/test_output_scaled'\n",
    "        ,test_drag_forces_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "cc0d5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the list to a pickle file (unscaled) ###\n",
    "### Train data ###\n",
    "train_save = [extracted_sequences[i][j] for (i, j) in train_pairs]\n",
    "test_save = [extracted_sequences[i][j] for (i, j) in test_pairs]\n",
    "\n",
    "with open('../num_neighbors_and_time_influence/training_and_testing_data/'+experiment+'/ridiculous_2/train_data_unscaled.pkl', 'wb') as f:\n",
    "    pickle.dump(train_save, f)\n",
    "    \n",
    "### Test data ###\n",
    "with open('../num_neighbors_and_time_influence/training_and_testing_data/'+experiment+'/ridiculous_2/test_data_unscaled.pkl', 'wb') as f:\n",
    "    pickle.dump(test_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9ceec",
   "metadata": {},
   "source": [
    "# Previous saving locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Save the list to a pickle file (scaled) ###\n",
    "\n",
    "# ### Train data ###\n",
    "# with open('../simple_connections_data/temporal_split/'+experiment+'/history_2_local/train_data_np_scaled.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_dataset_scaled, f)\n",
    "    \n",
    "# ### Test data ### \n",
    "# with open('../simple_connections_data/temporal_split/'+experiment+'/history_2_local/test_data_np_scaled.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_dataset_scaled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a527a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Save drag forces (scaled and unscaled) ###\n",
    "\n",
    "# ### Train data ###\n",
    "# np.save('../simple_connections_data/temporal_split/'+experiment+'/history_2_local/train_output_scaled'\n",
    "#         ,train_drag_forces_scaled)\n",
    "\n",
    "# ### Test data ###\n",
    "# np.save('../simple_connections_data/temporal_split/'+experiment+'/history_2_local//test_output_scaled'\n",
    "#         ,test_drag_forces_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef507cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Save the list to a pickle file (unscaled) ###\n",
    "# ### Train data ###\n",
    "# train_save = [extracted_sequences[i][j] for (i, j) in train_pairs]\n",
    "# test_save = [extracted_sequences[i][j] for (i, j) in test_pairs]\n",
    "\n",
    "# with open('../simple_connections_data/temporal_split/'+experiment+'/history_2_local/train_data_unscaled.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_save, f)\n",
    "    \n",
    "# ### Test data ###\n",
    "# with open('../simple_connections_data/temporal_split/'+experiment+'/history_2_local/test_data_unscaled.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf33d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save filtered indices ###\n",
    "# with open(\"simple_connections_data/temporal_split/\"+experiment+'/history_5/filtered_indices.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_save, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

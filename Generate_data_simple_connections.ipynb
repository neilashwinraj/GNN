{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d15cf2-f47d-434b-84cb-94bb3c79c276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /localscratch-ssd/288438/matplotlib-w41__5fr because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ed6a0-c522-4929-a6d4-837c2c395502",
   "metadata": {},
   "source": [
    "Generate from Ze time series dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b597fa-8c9b-4289-aa52-78fdd1691d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_periodic_coordinates(coord, size):\n",
    "    \"\"\"\n",
    "    Generate all coordinates within a cubic domain considering periodic boundary conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        coord (pandas dataframe): A pandas dataframe containing the columns (x, y, z) of a point.\n",
    "        size (int): The size of the cubic domain along each axis.\n",
    "    Returns:\n",
    "        list: A list of tuples containing all coordinates within the cubic domain.\n",
    "    \"\"\"\n",
    "    ### Keep copy of original dataframe and copy for each periodic bc shift ###\n",
    "    coord_copy = [coord.copy() for _ in range(27)]\n",
    "    stacked_df = pd.concat(coord_copy, axis=0)\n",
    "    stacked_df = stacked_df.reset_index(drop=True, inplace=False)\n",
    "    \n",
    "    # Get coordinates ###\n",
    "    if isinstance(coord, pd.DataFrame):\n",
    "        coord = coord[[\"x\",\"y\",\"z\"]].values\n",
    "\n",
    "    # Generate all combinations of displacements (-1, 0, 1) along each axis\n",
    "    displacements = list(itertools.product([-1, 0, 1], repeat=3))\n",
    "\n",
    "    # Generate all coordinates by applying periodic boundary conditions\n",
    "    tp_coordinates = list()\n",
    "    \n",
    "    for dx, dy, dz in displacements:\n",
    "          \n",
    "        temp = list()\n",
    "        \n",
    "        for i in range(len(coord)):\n",
    "            \n",
    "            x, y, z = coord[i,0],coord[i,1],coord[i,2]\n",
    "            \n",
    "            new_x = x + dx*size\n",
    "            new_y = y + dy*size\n",
    "            new_z = z + dz*size\n",
    "\n",
    "            temp.append((new_x,new_y,new_z))\n",
    "            \n",
    "        tp_coordinates.append( np.array(temp) )\n",
    "    \n",
    "    stacked_df[[\"x\",\"y\",\"z\"]] = np.vstack(tp_coordinates) \n",
    "    \n",
    "    return np.vstack(tp_coordinates),stacked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fc9653-576c-4e90-bb65-a82d652f6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_time_series_data(time_series_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Groups the data based on case_ID and time \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    ### load raw data from ze time series data ###\n",
    "    pd_list  = list()\n",
    "    \n",
    "    for (col1_val, col2_val), group in time_series_data.groupby(['case_ID', 'time']):\n",
    "    \n",
    "        pd_list.append(group)\n",
    "    \n",
    "    return pd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c4f4e0-3397-4a2a-ba51-141f5eb0b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearest_neighbor_data(time_series_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper function (in some sense, can be condensed more)to do the data generation \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    \n",
    "    pd_list = group_time_series_data(time_series_data)\n",
    "    \n",
    "    nearest_neighbor_data = list()\n",
    "    nearest_neighbor_data_extra = list()\n",
    "    scalar_data = list()\n",
    "    \n",
    "    ### Loop over different groups ###\n",
    "    \n",
    "    for i in range(len(pd_list)):\n",
    "        \n",
    "        print(\"Currently on case_time subgroup : \",str(i+1))\n",
    "        tp_particles,stacked_df = get_periodic_coordinates(pd_list[i],5)\n",
    "        tree = cKDTree(tp_particles)\n",
    "        \n",
    "        ### Loop over all particles in a group and getting the nearest neighbors ###\n",
    "        idx = np.stack([ tree.query(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values,16)[1] for j in range(len(pd_list[i])) ])\n",
    "        nearest_neighbor_data.append(tp_particles[idx])\n",
    "        \n",
    "        ### merging nodal data to the coordinates ###\n",
    "        nearest_neighbor_data_extra.append(merge_columns_to_pandas_list(tp_particles[idx],\"local_Re\",stacked_df))\n",
    "        \n",
    "        ### Getting the scalar data ###\n",
    "        scalar_data.append( pd_list[i][[\"Density_ratio\",\"glb_phi\",\"glb_Re\",\"local_Re\",\"Drag\"]] )\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    ### Populate graph and scalar lists ###\n",
    "    nearest_neighbor_data = np.stack(nearest_neighbor_data)\n",
    "    nearest_neighbor_data_extra = np.stack(nearest_neighbor_data_extra)\n",
    "    \n",
    "    nearest_neighbor_data = nearest_neighbor_data.reshape(nearest_neighbor_data.shape[0]*nearest_neighbor_data.shape[1]\n",
    "                                           ,nearest_neighbor_data.shape[2]*nearest_neighbor_data.shape[3])\n",
    "    \n",
    "    nearest_neighbor_data_extra = nearest_neighbor_data_extra.reshape(nearest_neighbor_data_extra.shape[0]*nearest_neighbor_data_extra.shape[1]\n",
    "                                           ,nearest_neighbor_data_extra.shape[2]*nearest_neighbor_data_extra.shape[3])\n",
    "    \n",
    "    scalar_data = np.stack(scalar_data)\n",
    "    scalar_data = scalar_data.reshape(scalar_data.shape[0]*scalar_data.shape[1],scalar_data.shape[2])    \n",
    "    \n",
    "    ### change code if you want to return nearest_neighbor_data or extra ### \n",
    "    return np.concatenate( (nearest_neighbor_data_extra,scalar_data) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7561cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns_to_pandas_list(nearest_neighbor_data,variable_list,master_dataframe):\n",
    "\n",
    "    \"\"\" given a list of pandas dataframe with the x,y,z locations and re and phi ,this function will\n",
    "        merge each pandas dataframe from the list with the master dataframe with all the columns  \n",
    "    \"\"\"\n",
    "\n",
    "    joined =[pd.DataFrame(nearest_neighbor_data[i],columns=[\"x\",\"y\",\"z\"]) for i in range(len(nearest_neighbor_data))]\n",
    "\n",
    "    for i in range(len(joined)):\n",
    "        \n",
    "        temp = copy.deepcopy(joined[i])\n",
    "        add = pd.merge(temp,master_dataframe,how=\"inner\",on=['x','y','z'],sort=False)[variable_list]\n",
    "        joined[i] = pd.concat([temp,add], axis=1)\n",
    "        \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e23f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_nearest_neighbor_data(nearest_neighbor_data,pd_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes nearest neighbor data and the pd_list and it will return a pandas dataframe with each row\n",
    "    having the particle ID (integer), the time step (integer) and the case (integer) of which the particle is a part of\n",
    "    ,and the remaining columns will be the nearest neighbor row itself.\n",
    "    \"\"\"\n",
    "    case_column = np.stack( [ pd_list[i][\"case_ID\"].values for i in range(len(pd_list)) ] ).flatten()\n",
    "    particle_id_column = np.stack( [ np.arange(pd_list[i].shape[0])+1 for i in range(len(pd_list)) ] ).flatten()\n",
    "    time_column = np.stack( [ pd_list[i][\"time\"].values for i in range(len(pd_list)) ] ).flatten()\n",
    "    \n",
    "    ### Combining columns with nearest_neighbor_data ###\n",
    "    nearest_neighbor_data_modified = np.concatenate( (case_column[:,None],particle_id_column[:,None],time_column[:,None]\n",
    "                ,nearest_neighbor_data),axis=1 )\n",
    "    \n",
    "    return nearest_neighbor_data_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305234b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_temporally_related_datasets(nearest_neighbor_data_modified,history_length=3):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0d93504-ae34-4f09-b4f2-cf14bf501100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on case_time subgroup :  323\n"
     ]
    }
   ],
   "source": [
    "# ### Read data ###\n",
    "experiment = \"rho2_40percent_Re100\"\n",
    "time_series_data = pd.read_csv(\"../ze_time_series_data_raw/\"+experiment+\".dat\")\n",
    "\n",
    "pd_list = group_time_series_data(time_series_data)\n",
    "nearest_neighbor_data = generate_nearest_neighbor_data(time_series_data)\n",
    "nearest_neighbor_data_modified = modify_nearest_neighbor_data(nearest_neighbor_data,pd_list)\n",
    "save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c15a18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbor_data_modified = modify_nearest_neighbor_data(nearest_neighbor_data,pd_list)\n",
    "nearest_neighbor_data_modified = pd.DataFrame(nearest_neighbor_data_modified)\n",
    "\n",
    "new_column_names = {0: 'case', 1: 'particle_ID', 2: 'time'}\n",
    "nearest_neighbor_data_modified.rename(columns=new_column_names, inplace=True)\n",
    "nearest_neighbor_data_modified = nearest_neighbor_data_modified.groupby([\"case\",\"particle_ID\"])\n",
    "\n",
    "# Collect groups into a list of DataFrames\n",
    "grouped_dfs = [group for _, group in nearest_neighbor_data_modified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d5f69d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>particle_ID</th>\n",
       "      <th>time</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14495</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.360318</td>\n",
       "      <td>2.13072</td>\n",
       "      <td>2.23686</td>\n",
       "      <td>74.979743</td>\n",
       "      <td>0.412010</td>\n",
       "      <td>3.168920</td>\n",
       "      <td>2.38407</td>\n",
       "      <td>...</td>\n",
       "      <td>106.186847</td>\n",
       "      <td>-0.44449</td>\n",
       "      <td>0.984937</td>\n",
       "      <td>1.319230</td>\n",
       "      <td>86.832430</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>74.979743</td>\n",
       "      <td>45.680066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14591</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.373662</td>\n",
       "      <td>2.13099</td>\n",
       "      <td>2.21697</td>\n",
       "      <td>75.333743</td>\n",
       "      <td>0.418750</td>\n",
       "      <td>3.166730</td>\n",
       "      <td>2.38719</td>\n",
       "      <td>...</td>\n",
       "      <td>97.942070</td>\n",
       "      <td>-0.44401</td>\n",
       "      <td>0.987527</td>\n",
       "      <td>1.326270</td>\n",
       "      <td>86.077305</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>75.333743</td>\n",
       "      <td>16.453703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14687</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.386057</td>\n",
       "      <td>2.13072</td>\n",
       "      <td>2.19560</td>\n",
       "      <td>74.576114</td>\n",
       "      <td>0.425720</td>\n",
       "      <td>3.162310</td>\n",
       "      <td>2.38721</td>\n",
       "      <td>...</td>\n",
       "      <td>96.010325</td>\n",
       "      <td>-0.44217</td>\n",
       "      <td>0.990373</td>\n",
       "      <td>1.334160</td>\n",
       "      <td>86.117707</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>74.576114</td>\n",
       "      <td>34.332095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14783</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.402369</td>\n",
       "      <td>2.12824</td>\n",
       "      <td>2.17720</td>\n",
       "      <td>69.492254</td>\n",
       "      <td>-0.283976</td>\n",
       "      <td>2.462690</td>\n",
       "      <td>1.45661</td>\n",
       "      <td>...</td>\n",
       "      <td>96.642000</td>\n",
       "      <td>-0.44110</td>\n",
       "      <td>0.994344</td>\n",
       "      <td>1.342030</td>\n",
       "      <td>85.961561</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>69.492254</td>\n",
       "      <td>15.028256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14879</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.421006</td>\n",
       "      <td>2.12508</td>\n",
       "      <td>2.16109</td>\n",
       "      <td>69.324984</td>\n",
       "      <td>0.436720</td>\n",
       "      <td>3.150820</td>\n",
       "      <td>2.38535</td>\n",
       "      <td>...</td>\n",
       "      <td>84.235200</td>\n",
       "      <td>-1.02766</td>\n",
       "      <td>2.074730</td>\n",
       "      <td>2.992060</td>\n",
       "      <td>96.771278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>69.324984</td>\n",
       "      <td>29.294079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30623</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>4.388670</td>\n",
       "      <td>1.07292</td>\n",
       "      <td>1.04713</td>\n",
       "      <td>85.061506</td>\n",
       "      <td>3.550200</td>\n",
       "      <td>0.446929</td>\n",
       "      <td>1.13419</td>\n",
       "      <td>...</td>\n",
       "      <td>71.202699</td>\n",
       "      <td>4.56455</td>\n",
       "      <td>2.278590</td>\n",
       "      <td>-0.165850</td>\n",
       "      <td>103.224933</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>85.061506</td>\n",
       "      <td>15.587544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30719</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>4.390916</td>\n",
       "      <td>1.06134</td>\n",
       "      <td>1.06683</td>\n",
       "      <td>87.025160</td>\n",
       "      <td>4.721977</td>\n",
       "      <td>1.847300</td>\n",
       "      <td>1.67932</td>\n",
       "      <td>...</td>\n",
       "      <td>72.671017</td>\n",
       "      <td>4.54928</td>\n",
       "      <td>2.281980</td>\n",
       "      <td>-0.163383</td>\n",
       "      <td>103.322283</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>87.025160</td>\n",
       "      <td>23.345079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30815</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>4.391879</td>\n",
       "      <td>1.04840</td>\n",
       "      <td>1.08909</td>\n",
       "      <td>87.939140</td>\n",
       "      <td>3.526940</td>\n",
       "      <td>0.455561</td>\n",
       "      <td>1.14792</td>\n",
       "      <td>...</td>\n",
       "      <td>72.589558</td>\n",
       "      <td>4.53278</td>\n",
       "      <td>2.284030</td>\n",
       "      <td>-0.160131</td>\n",
       "      <td>105.174133</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>87.939140</td>\n",
       "      <td>19.300434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>4.391562</td>\n",
       "      <td>1.03443</td>\n",
       "      <td>1.11374</td>\n",
       "      <td>89.613672</td>\n",
       "      <td>3.513070</td>\n",
       "      <td>0.460734</td>\n",
       "      <td>1.15405</td>\n",
       "      <td>...</td>\n",
       "      <td>75.877944</td>\n",
       "      <td>4.51665</td>\n",
       "      <td>2.286310</td>\n",
       "      <td>-0.157473</td>\n",
       "      <td>103.386776</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>89.613672</td>\n",
       "      <td>20.887419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31007</th>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>4.391277</td>\n",
       "      <td>1.02029</td>\n",
       "      <td>1.13996</td>\n",
       "      <td>87.800650</td>\n",
       "      <td>3.500460</td>\n",
       "      <td>0.464868</td>\n",
       "      <td>1.15690</td>\n",
       "      <td>...</td>\n",
       "      <td>74.929527</td>\n",
       "      <td>4.50004</td>\n",
       "      <td>2.290050</td>\n",
       "      <td>-0.155052</td>\n",
       "      <td>105.205133</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>87.800650</td>\n",
       "      <td>17.518778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case  particle_ID   time         3        4        5          6  \\\n",
       "14495   2.0         96.0    1.0  0.360318  2.13072  2.23686  74.979743   \n",
       "14591   2.0         96.0    2.0  0.373662  2.13099  2.21697  75.333743   \n",
       "14687   2.0         96.0    3.0  0.386057  2.13072  2.19560  74.576114   \n",
       "14783   2.0         96.0    4.0  0.402369  2.12824  2.17720  69.492254   \n",
       "14879   2.0         96.0    5.0  0.421006  2.12508  2.16109  69.324984   \n",
       "...     ...          ...    ...       ...      ...      ...        ...   \n",
       "30623   2.0         96.0  169.0  4.388670  1.07292  1.04713  85.061506   \n",
       "30719   2.0         96.0  170.0  4.390916  1.06134  1.06683  87.025160   \n",
       "30815   2.0         96.0  171.0  4.391879  1.04840  1.08909  87.939140   \n",
       "30911   2.0         96.0  172.0  4.391562  1.03443  1.11374  89.613672   \n",
       "31007   2.0         96.0  173.0  4.391277  1.02029  1.13996  87.800650   \n",
       "\n",
       "              7         8        9  ...          62       63        64  \\\n",
       "14495  0.412010  3.168920  2.38407  ...  106.186847 -0.44449  0.984937   \n",
       "14591  0.418750  3.166730  2.38719  ...   97.942070 -0.44401  0.987527   \n",
       "14687  0.425720  3.162310  2.38721  ...   96.010325 -0.44217  0.990373   \n",
       "14783 -0.283976  2.462690  1.45661  ...   96.642000 -0.44110  0.994344   \n",
       "14879  0.436720  3.150820  2.38535  ...   84.235200 -1.02766  2.074730   \n",
       "...         ...       ...      ...  ...         ...      ...       ...   \n",
       "30623  3.550200  0.446929  1.13419  ...   71.202699  4.56455  2.278590   \n",
       "30719  4.721977  1.847300  1.67932  ...   72.671017  4.54928  2.281980   \n",
       "30815  3.526940  0.455561  1.14792  ...   72.589558  4.53278  2.284030   \n",
       "30911  3.513070  0.460734  1.15405  ...   75.877944  4.51665  2.286310   \n",
       "31007  3.500460  0.464868  1.15690  ...   74.929527  4.50004  2.290050   \n",
       "\n",
       "             65          66   67   68     69         70         71  \n",
       "14495  1.319230   86.832430  2.0  0.4  100.0  74.979743  45.680066  \n",
       "14591  1.326270   86.077305  2.0  0.4  100.0  75.333743  16.453703  \n",
       "14687  1.334160   86.117707  2.0  0.4  100.0  74.576114  34.332095  \n",
       "14783  1.342030   85.961561  2.0  0.4  100.0  69.492254  15.028256  \n",
       "14879  2.992060   96.771278  2.0  0.4  100.0  69.324984  29.294079  \n",
       "...         ...         ...  ...  ...    ...        ...        ...  \n",
       "30623 -0.165850  103.224933  2.0  0.4  100.0  85.061506  15.587544  \n",
       "30719 -0.163383  103.322283  2.0  0.4  100.0  87.025160  23.345079  \n",
       "30815 -0.160131  105.174133  2.0  0.4  100.0  87.939140  19.300434  \n",
       "30911 -0.157473  103.386776  2.0  0.4  100.0  89.613672  20.887419  \n",
       "31007 -0.155052  105.205133  2.0  0.4  100.0  87.800650  17.518778  \n",
       "\n",
       "[173 rows x 72 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_dfs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3f6ae2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For splitting across case ###\n",
    "case_list = np.stack ( [np.unique( pd_list[i][\"case_ID\"] ) for i in range(len(pd_list))] )\n",
    "_,index = np.unique(case_list,return_index=True)\n",
    "change_point = (pd_list[0].shape[0])*index[1]\n",
    "change_point_2 = change_point + (len(case_list[case_list==2])//2)*pd_list[-1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f9673-4a5f-4009-a1e0-64618c69bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the data as test and train (Random) ###\n",
    "X_train, X_test, y_train, y_test = train_test_split(nearest_neighbor_data[:,0:64],nearest_neighbor_data[:,64:], test_size=0.2, random_state=1)\n",
    "\n",
    "### Scaling the data ###\n",
    "### Inputs ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "train_input_scaled = scaler.transform(X_train)\n",
    "test_input_scaled = scaler.transform(X_test)\n",
    "\n",
    "### Scalar Inputs and outputs (both are under outputs and thus needs to be separated during saving) ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train)\n",
    "\n",
    "train_output_scaled = scaler.transform(y_train)\n",
    "test_output_scaled = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save==True:\n",
    "    \n",
    "    ### Creat Directory ###\n",
    "    directory = \"simple_connections_data/random_split/\"+experiment\n",
    "    \n",
    "    ### if directory does not exist ###\n",
    "    if not os.path.exists(directory):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        if os.path.exists(directory):\n",
    "            print(f\"Directory '{directory}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to create directory '{directory}'.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/train_inputs\",train_input_scaled.reshape(train_input_scaled.shape[0],16,4))\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/test_inputs\",test_input_scaled.reshape(test_input_scaled.shape[0],16,4))\n",
    "\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/train_input_scalar\",train_output_scaled[:,0:4])\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/test_input_scalar\",test_output_scaled[:,0:4])\n",
    "\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/train_output\",train_output_scaled[:,4:])\n",
    "    np.save(\"simple_connections_data/random_split/\"+experiment+\"/test_output\",test_output_scaled[:,4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45725c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the data as test and train (Case-wise) ###\n",
    "X_train, X_test = nearest_neighbor_data[0:change_point,0:64],nearest_neighbor_data[change_point:,0:64]\n",
    "y_train, y_test = nearest_neighbor_data[0:change_point,64:],nearest_neighbor_data[change_point:,64:]\n",
    "\n",
    "### Scaling the data ###\n",
    "### Inputs ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "train_input_scaled = scaler.transform(X_train)\n",
    "test_input_scaled = scaler.transform(X_test)\n",
    "\n",
    "### Scalar Inputs and outputs (both are under outputs and thus needs to be separated during saving) ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train)\n",
    "\n",
    "train_output_scaled = scaler.transform(y_train)\n",
    "test_output_scaled = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save==True:\n",
    "    \n",
    "        ### Creat Directory ###\n",
    "    directory = \"simple_connections_data/case_split/\"+experiment\n",
    "    \n",
    "    ### if directory does not exist ###\n",
    "    if not os.path.exists(directory):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        if os.path.exists(directory):\n",
    "            print(f\"Directory '{directory}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to create directory '{directory}'.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/train_inputs\",train_input_scaled.reshape(train_input_scaled.shape[0],16,4))\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/test_inputs\",test_input_scaled.reshape(test_input_scaled.shape[0],16,4))\n",
    "\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/train_input_scalar\",train_output_scaled[:,0:4])\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/test_input_scalar\",test_output_scaled[:,0:4])\n",
    "\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/train_output\",train_output_scaled[:,4:])\n",
    "    np.save(\"simple_connections_data/case_split/\"+experiment+\"/test_output\",test_output_scaled[:,4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59726d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### mid-time splitting ###\n",
    "# ### Splitting the data as test and train (Splitting each case into two halves) ###\n",
    "\n",
    "# ### Features Train and Test ###\n",
    "X_train_1,X_train_2 = nearest_neighbor_data[0:change_point//2,0:64],nearest_neighbor_data[change_point:change_point_2,0:64]\n",
    "X_train = np.concatenate((X_train_1,X_train_2),axis=0)\n",
    "\n",
    "X_test_1,X_test_2 = nearest_neighbor_data[change_point//2:change_point,0:64],nearest_neighbor_data[change_point_2:,0:64]\n",
    "X_test = np.concatenate((X_test_1,X_test_2),axis=0)\n",
    "\n",
    "### Labels Train and Test ###\n",
    "y_train_1,y_train_2 = nearest_neighbor_data[0:change_point//2,64:],nearest_neighbor_data[change_point:change_point_2,64:]\n",
    "y_train = np.concatenate((y_train_1,y_train_2),axis=0)\n",
    "\n",
    "y_test_1,y_test_2 = nearest_neighbor_data[change_point//2:change_point,64:],nearest_neighbor_data[change_point_2:,64:]\n",
    "y_test = np.concatenate((y_test_1,y_test_2),axis=0)\n",
    "\n",
    "### Scaling the data ###\n",
    "### Inputs ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "train_input_scaled = scaler.transform(X_train)\n",
    "test_input_scaled = scaler.transform(X_test)\n",
    "\n",
    "### Scalar Inputs and outputs (both are under outputs and thus needs to be separated during saving) ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train)\n",
    "\n",
    "train_output_scaled = scaler.transform(y_train)\n",
    "test_output_scaled = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save==True:\n",
    "    \n",
    "    ### Creat Directory ###\n",
    "    directory = \"simple_connections_data/time_split/\"+experiment\n",
    "    \n",
    "    ### if directory does not exist ###\n",
    "    if not os.path.exists(directory):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        if os.path.exists(directory):\n",
    "            print(f\"Directory '{directory}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to create directory '{directory}'.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/train_inputs\",train_input_scaled.reshape(train_input_scaled.shape[0],16,4))\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/test_inputs\",test_input_scaled.reshape(test_input_scaled.shape[0],16,4))\n",
    "\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/train_input_scalar\",train_output_scaled[:,0:4])\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/test_input_scalar\",test_output_scaled[:,0:4])\n",
    "\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/train_output\",train_output_scaled[:,4:])\n",
    "    np.save(\"simple_connections_data/time_split/\"+experiment+\"/test_output\",test_output_scaled[:,4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a028b90",
   "metadata": {},
   "source": [
    "# Combine all cells from above (messy clean later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8ef2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exp_list=[\n",
    "              \"rho10_10percent_Re10\",\"rho10_10percent_Re50\",\"rho10_10percent_Re100\",\"rho10_10percent_Re200\",\"rho10_10percent_Re300\",\n",
    "              \"rho10_20percent_Re10\",\"rho10_20percent_Re50\",\"rho10_20percent_Re100\",\"rho10_20percent_Re200\",\"rho10_20percent_Re300\",\n",
    "              \"rho10_30percent_Re10\",\"rho10_30percent_Re50\",\"rho10_30percent_Re100\",\"rho10_30percent_Re200\",\"rho10_30percent_Re300\",\n",
    "              \"rho10_40percent_Re10\",\"rho10_40percent_Re50\",\"rho10_40percent_Re100\",\"rho10_40percent_Re200\",\"rho10_40percent_Re300\",\n",
    "               \n",
    "              \"rho100_10percent_Re10\",\"rho100_10percent_Re50\",\"rho100_10percent_Re100\",\"rho100_10percent_Re200\",\"rho100_10percent_Re300\",\n",
    "              \"rho100_20percent_Re10\",\"rho100_20percent_Re50\",\"rho100_20percent_Re100\",\"rho100_20percent_Re200\",\"rho100_20percent_Re300\",\n",
    "              \"rho100_30percent_Re10\",\"rho100_30percent_Re50\",\"rho100_30percent_Re100\",\"rho100_30percent_Re200\",\"rho100_30percent_Re300\",\n",
    "              \"rho100_40percent_Re10\",\"rho100_40percent_Re50\",\"rho100_40percent_Re100\",\"rho100_40percent_Re200\",\"rho100_40percent_Re300\",\n",
    "      \n",
    "              \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on case_time subgroup :  241\n"
     ]
    }
   ],
   "source": [
    "### Read data ###\n",
    "\n",
    "for i in range(len(all_exp_list)):\n",
    "    \n",
    "    experiment = all_exp_list[i]\n",
    "    time_series_data = pd.read_csv(\"../ze_time_series_data_raw/\"+experiment+\".dat\")\n",
    "    pd_list = group_time_series_data(time_series_data)\n",
    "    nearest_neighbor_data = generate_nearest_neighbor_data(time_series_data)\n",
    "    save=True\n",
    "\n",
    "    ### For splitting across case ###\n",
    "    case_list = np.stack ( [np.unique( pd_list[i][\"case_ID\"] ) for i in range(len(pd_list))] )\n",
    "    _,index = np.unique(case_list,return_index=True)\n",
    "    change_point = (pd_list[0].shape[0])*index[1]\n",
    "    change_point_2 = change_point + (len(case_list[case_list==2])//2)*pd_list[-1].shape[0]\n",
    "\n",
    "    ### Splitting the data as test and train (Random) ###\n",
    "    X_train, X_test, y_train, y_test = train_test_split(nearest_neighbor_data[:,0:64],nearest_neighbor_data[:,64:], test_size=0.2, random_state=1)\n",
    "\n",
    "    ### Scaling the data ###\n",
    "    ### Inputs ###\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    train_input_scaled = scaler.transform(X_train)\n",
    "    test_input_scaled = scaler.transform(X_test)\n",
    "\n",
    "    ### Scalar Inputs and outputs (both are under outputs and thus needs to be separated during saving) ###\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(y_train)\n",
    "\n",
    "    train_output_scaled = scaler.transform(y_train)\n",
    "    test_output_scaled = scaler.transform(y_test)\n",
    "\n",
    "    if save==True:\n",
    "\n",
    "        ### Creat Directory ###\n",
    "        directory = \"simple_connections_data/random_split/\"+experiment\n",
    "\n",
    "        ### if directory does not exist ###\n",
    "        if not os.path.exists(directory):\n",
    "            # Create the directory\n",
    "            os.makedirs(directory)\n",
    "\n",
    "            if os.path.exists(directory):\n",
    "                print(f\"Directory '{directory}' created successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to create directory '{directory}'.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "        np.save(\"simple_connections_data/random_split/\"+experiment+\"/train_inputs\",train_input_scaled.reshape(train_input_scaled.shape[0],16,4))\n",
    "        np.save(\"simple_connections_data/random_split/\"+experiment+\"/test_inputs\",test_input_scaled.reshape(test_input_scaled.shape[0],16,4))\n",
    "\n",
    "        np.save(\"simple_connections_data/random_split/\"+experiment+\"/train_input_scalar\",train_output_scaled[:,0:4])\n",
    "        np.save(\"simple_connections_data/random_split/\"+experiment+\"/test_input_scalar\",test_output_scaled[:,0:4])\n",
    "\n",
    "        np.save(\"simple_connections_data/random_split/\"+experiment+\"/train_output\",train_output_scaled[:,4:])\n",
    "        np.save(\"simple_connections_data/random_split/\"+experiment+\"/test_output\",test_output_scaled[:,4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77292d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exp_list=[\"rho2_10percent_Re10\",\"rho2_10percent_Re50\",\"rho2_10percent_Re100\",\"rho2_10percent_Re200\",\"rho2_10percent_Re300\",\n",
    "              \"rho2_20percent_Re10\",\"rho2_20percent_Re50\",\"rho2_20percent_Re100\",\"rho2_20percent_Re200\",\"rho2_20percent_Re300\",\n",
    "              \"rho2_30percent_Re10\",\"rho2_30percent_Re50\",\"rho2_30percent_Re100\",\"rho2_30percent_Re200\",\"rho2_30percent_Re300\",\n",
    "              \"rho2_40percent_Re10\",\"rho2_40percent_Re50\",\"rho2_40percent_Re100\",\"rho2_40percent_Re200\",\"rho2_40percent_Re300\",\n",
    "              \n",
    "              \"rho10_10percent_Re10\",\"rho10_10percent_Re50\",\"rho10_10percent_Re100\",\"rho10_10percent_Re200\",\"rho10_10percent_Re300\",\n",
    "              \"rho10_20percent_Re10\",\"rho10_20percent_Re50\",\"rho10_20percent_Re100\",\"rho10_20percent_Re200\",\"rho10_20percent_Re300\",\n",
    "              \"rho10_30percent_Re10\",\"rho10_30percent_Re50\",\"rho10_30percent_Re100\",\"rho10_30percent_Re200\",\"rho10_30percent_Re300\",\n",
    "              \"rho10_40percent_Re10\",\"rho10_40percent_Re50\",\"rho10_40percent_Re100\",\"rho10_40percent_Re200\",\"rho10_40percent_Re300\",\n",
    "               \n",
    "              \"rho100_10percent_Re10\",\"rho100_10percent_Re50\",\"rho100_10percent_Re100\",\"rho100_10percent_Re200\",\"rho100_10percent_Re300\",\n",
    "              \"rho100_20percent_Re10\",\"rho100_20percent_Re50\",\"rho100_20percent_Re100\",\"rho100_20percent_Re200\",\"rho100_20percent_Re300\",\n",
    "              \"rho100_30percent_Re10\",\"rho100_30percent_Re50\",\"rho100_30percent_Re100\",\"rho100_30percent_Re200\",\"rho100_30percent_Re300\",\n",
    "              \"rho100_40percent_Re10\",\"rho100_40percent_Re50\",\"rho100_40percent_Re100\",\"rho100_40percent_Re200\",\"rho100_40percent_Re300\",\n",
    "      \n",
    "              \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcf031",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read data ###\n",
    "\n",
    "for i in range(len(all_exp_list)):\n",
    "    \n",
    "    experiment = all_exp_list[i]\n",
    "    time_series_data = pd.read_csv(\"../ze_time_series_data_raw/\"+experiment+\".dat\")\n",
    "    pd_list = group_time_series_data(time_series_data)\n",
    "    nearest_neighbor_data = generate_nearest_neighbor_data(time_series_data)\n",
    "    save=True\n",
    "\n",
    "    ### For splitting across case ###\n",
    "    case_list = np.stack ( [np.unique( pd_list[i][\"case_ID\"] ) for i in range(len(pd_list))] )\n",
    "    _,index = np.unique(case_list,return_index=True)\n",
    "    change_point = (pd_list[0].shape[0])*index[1]\n",
    "    change_point_2 = change_point + (len(case_list[case_list==2])//2)*pd_list[-1].shape[0]\n",
    "    \n",
    "    \n",
    "    ### mid-time splitting ###\n",
    "    ### Splitting the data as test and train (Splitting each case into two halves) ###\n",
    "\n",
    "    ### Features Train and Test ###\n",
    "    X_train_1,X_train_2 = nearest_neighbor_data[0:change_point//2,0:64],nearest_neighbor_data[change_point:change_point_2,0:64]\n",
    "    X_train = np.concatenate((X_train_1,X_train_2),axis=0)\n",
    "\n",
    "    X_test_1,X_test_2 = nearest_neighbor_data[change_point//2:change_point,0:64],nearest_neighbor_data[change_point_2:,0:64]\n",
    "    X_test = np.concatenate((X_test_1,X_test_2),axis=0)\n",
    "\n",
    "    ### Labels Train and Test ###\n",
    "    y_train_1,y_train_2 = nearest_neighbor_data[0:change_point//2,64:],nearest_neighbor_data[change_point:change_point_2,64:]\n",
    "    y_train = np.concatenate((y_train_1,y_train_2),axis=0)\n",
    "\n",
    "    y_test_1,y_test_2 = nearest_neighbor_data[change_point//2:change_point,64:],nearest_neighbor_data[change_point_2:,64:]\n",
    "    y_test = np.concatenate((y_test_1,y_test_2),axis=0)\n",
    "\n",
    "    ### Scaling the data ###\n",
    "    ### Inputs ###\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    train_input_scaled = scaler.transform(X_train)\n",
    "    test_input_scaled = scaler.transform(X_test)\n",
    "\n",
    "    ### Scalar Inputs and outputs (both are under outputs and thus needs to be separated during saving) ###\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(y_train)\n",
    "\n",
    "    train_output_scaled = scaler.transform(y_train)\n",
    "    test_output_scaled = scaler.transform(y_test)\n",
    "\n",
    "    \n",
    "    if save==True:\n",
    "\n",
    "        ### Creat Directory ###\n",
    "        directory = \"simple_connections_data/time_split/\"+experiment\n",
    "\n",
    "        ### if directory does not exist ###\n",
    "        if not os.path.exists(directory):\n",
    "            # Create the directory\n",
    "            os.makedirs(directory)\n",
    "\n",
    "            if os.path.exists(directory):\n",
    "                print(f\"Directory '{directory}' created successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to create directory '{directory}'.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "        np.save(\"simple_connections_data/time_split/\"+experiment+\"/train_inputs\",train_input_scaled.reshape(train_input_scaled.shape[0],16,4))\n",
    "        np.save(\"simple_connections_data/time_split/\"+experiment+\"/test_inputs\",test_input_scaled.reshape(test_input_scaled.shape[0],16,4))\n",
    "\n",
    "        np.save(\"simple_connections_data/time_split/\"+experiment+\"/train_input_scalar\",train_output_scaled[:,0:4])\n",
    "        np.save(\"simple_connections_data/time_split/\"+experiment+\"/test_input_scalar\",test_output_scaled[:,0:4])\n",
    "\n",
    "        np.save(\"simple_connections_data/time_split/\"+experiment+\"/train_output\",train_output_scaled[:,4:])\n",
    "        np.save(\"simple_connections_data/time_split/\"+experiment+\"/test_output\",test_output_scaled[:,4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d1378",
   "metadata": {},
   "source": [
    "# Verfiy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_euclidean(vec_1,vec_2):\n",
    "    \n",
    "    return np.sqrt( (vec_1[0]-vec_2[0])**2 + (vec_1[1]-vec_2[1])**2 + (vec_1[2]-vec_2[2])**2 )\n",
    "\n",
    "def brute_search(query,tree,n_nearest=16):\n",
    "\n",
    "    ### finds the nearest neighbors with a basic algorithm ###\n",
    "    ### both query and the 'tree' must be numpy arrays ###\n",
    "    dist = np.stack([dist_euclidean(query,tree[i]) for i in range(len(tree))])\n",
    "\n",
    "    return np.argsort(dist)[0:n_nearest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40cc837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho100_10percent_Re100.dat\")\n",
    "pd_list = group_time_series_data(time_series_data)\n",
    "nn_list = list()\n",
    "\n",
    "for i in range(len(pd_list)):\n",
    "    \n",
    "    _,stacked_df = get_periodic_coordinates(pd_list[i],5)\n",
    "    print(\"Case_time subset number : \",str(i+1))\n",
    "    \n",
    "    for j in range(len(pd_list[i])):\n",
    "        \n",
    "        ### getting nearest neighbors ###\n",
    "        poi = np.array(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values)\n",
    "        idx = brute_search(poi,stacked_df[[\"x\",\"y\",\"z\"]].values)\n",
    "        nn_list.append(stacked_df.iloc[idx])\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "### Defining the arrays to compare ###\n",
    "nn_list = np.stack(nn_list)\n",
    "previous = np.stack([ nearest_neighbor_data[i][0:64].reshape(16,4)[:,0:3] for i in range(len(nearest_neighbor_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed58281",
   "metadata": {},
   "source": [
    "# Statiscal Analysis of the Raw data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b9c7f",
   "metadata": {},
   "source": [
    "Raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a18f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho100_40percent_Re300.dat\")\n",
    "pd_list = group_time_series_data(raw_data)\n",
    "raw_data_case_1 = raw_data[raw_data[\"case_ID\"]==1]\n",
    "raw_data_case_2 = raw_data[raw_data[\"case_ID\"]==2]\n",
    "sns.kdeplot(raw_data_case_1[\"local_Re\"],fill=True)\n",
    "sns.kdeplot(raw_data_case_2[\"local_Re\"],fill=True)\n",
    "plt.legend([\"Case 1\",\"Case 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(raw_data_case_1[[\"x\",\"y\",\"z\"]].values.flatten(),fill=True)\n",
    "sns.kdeplot(raw_data_case_2[[\"x\",\"y\",\"z\"]].values.flatten(),fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5692fa",
   "metadata": {},
   "source": [
    "Scaled Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_train = np.load(\"/home/neilashwinraj/gnns/simple_connections/simple_connections_data/time_split/rho100_10percent_Re100/train_inputs.npy\")\n",
    "scaled_data_test = np.load(\"/home/neilashwinraj/gnns/simple_connections/simple_connections_data/time_split/rho100_10percent_Re100/test_inputs.npy\")\n",
    "\n",
    "sns.kdeplot(scaled_data_train[:,:,0].flatten())\n",
    "sns.kdeplot(scaled_data_test[:,:,0].flatten())\n",
    "plt.legend([\"Train\",\"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c249234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_distances_default(distance_list):\n",
    "    hist, bin_edges = np.histogram( distance_list )\n",
    "    return hist, bin_edges\n",
    "\n",
    "def histogram_distances(distance_list, max_dist, bin_size):\n",
    "    # this is the list of bins in which to calculate\n",
    "    bins = np.arange(0, max_dist+bin_size, bin_size)\n",
    "    hist, bin_edges = np.histogram( distance_list, bins=bins )\n",
    "    return hist, bin_edges\n",
    "\n",
    "def plot_histogram(hist,bin_edges):\n",
    "    #for N bins, there are N+1 bin edges. The centers can be found by averaging the positions of \n",
    "    # bin edge0 and 1, 1 and 2, ..., N-1 and N\n",
    "    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2.0\n",
    "    plt.plot(bin_centers,hist,marker='o')\n",
    "    plt.ylabel(\"N(r)\")\n",
    "    plt.xlabel(\"$r$\")\n",
    "    \n",
    "def compute_distances_minimum_image( configuration, box_size ):\n",
    "    distance_list = []\n",
    "    num_particles = configuration.shape[0]\n",
    "    k=0\n",
    "    for i in range(num_particles):\n",
    "        for j in range(num_particles):\n",
    "            if i == j: continue\n",
    "            \n",
    "            posi = configuration[i]\n",
    "            posj = configuration[j]\n",
    "            # compute the euclidian distance between pos1 and pos2 and call it 'dist' \n",
    "            # there are many ways to do this\n",
    "            # you can certainly look up how to do this online if you can't figure it out right away\n",
    "            \n",
    "            #dr is a vector (dx,dy)\n",
    "            dr = posj-posi\n",
    "            #minimum image dr - can you figure out why this works?\n",
    "            dr = dr - box_size*np.floor(dr/box_size+0.5)\n",
    "            \n",
    "            #dr2 is a vector (dx*dx,dy*dy)\n",
    "            dr2 = dr*dr \n",
    "            #dist = sqrt( dx^2 + dy^2)\n",
    "            dist = np.sqrt( dr2.sum() )            \n",
    "            distance_list.append(dist)\n",
    "            \n",
    "    return np.array(distance_list)\n",
    "\n",
    "def plot_rdf(gofr,bin_centers):\n",
    "    plt.plot(bin_centers,gofr,marker='o')\n",
    "    plt.ylabel(\"g(r)\")\n",
    "    plt.xlabel(\"$r$\")\n",
    "    \n",
    "def get_gofr(hist,bin_edges,num_particles, box_size):\n",
    "    rho = num_particles/(box_size**3)\n",
    "    bin_centers = (bin_edges[1:]+bin_edges[:-1])/2.0\n",
    "    dr = bin_edges[1]-bin_edges[0]\n",
    "    denominator = 4.*np.pi*(bin_centers**2)*dr*rho*( num_particles )\n",
    "    gofr = hist/denominator\n",
    "    \n",
    "    return gofr, bin_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f80406",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho100_40percent_Re300.dat\")\n",
    "pd_list = group_time_series_data(raw_data)\n",
    "\n",
    "x_list = list()\n",
    "y_list = list()\n",
    "\n",
    "### Apllying functions to calculate the RDF ###\n",
    "for i in range(len(pd_list)):\n",
    "   \n",
    "    \n",
    "    distance_list = compute_distances_minimum_image( pd_list[i][[\"x\",\"y\",\"z\"]].values, 5 )\n",
    "    hist, bin_edges = histogram_distances(distance_list=distance_list, max_dist=(5/2)*np.sqrt(3),bin_size=0.1)\n",
    "    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2.0\n",
    "    \n",
    "    gofr, bin_centers = get_gofr(hist,bin_edges,num_particles = len(pd_list[i]), box_size=5)\n",
    "    x_list.append(bin_centers)\n",
    "    y_list.append(gofr)\n",
    "    \n",
    "    plt.plot( bin_centers,gofr)\n",
    "    \n",
    "    plt.ylim([-0.05,3.25])\n",
    "    plt.show()\n",
    "    print(\"Currently at case : \",str(np.unique(pd_list[i][\"case_ID\"].values)[0])\n",
    "          ,\"and time step : \",np.unique(pd_list[i][\"time\"].values)[0])\n",
    "    clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d842cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_time_steps_case_1 = raw_data[raw_data[\"case_ID\"]==1][\"time\"].values.max()\n",
    "for i in range(no_time_steps_case_1):\n",
    "    plt.plot(x_list[i],y_list[i],c=\"b\",alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_time_steps_case_2 = raw_data[raw_data[\"case_ID\"]==2][\"time\"].values.max()\n",
    "for i in range(no_time_steps_case_1,no_time_steps_case_1+no_time_steps_case_2):\n",
    "    plt.plot(x_list[i],y_list[i],c=\"r\",alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "\n",
    "# Create a sample DataFrame\n",
    "raw_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho10_20percent_Re100.dat\")\n",
    "case_1_all_timsteps = raw_data[raw_data[\"case_ID\"]==1]\n",
    "case_2_all_timsteps = raw_data[raw_data[\"case_ID\"]==2]\n",
    "\n",
    "### for case 1\n",
    "# num_particles = len(case_1_all_timsteps[case_1_all_timsteps[\"time\"]==1])\n",
    "# time_steps = case_1_all_timsteps[\"time\"].values[-1]\n",
    "# df = case_1_all_timsteps.copy()\n",
    "\n",
    "### for case 2\n",
    "num_particles = len(case_2_all_timsteps[case_2_all_timsteps[\"time\"]==1])\n",
    "time_steps = case_2_all_timsteps[\"time\"].values[-1]\n",
    "df = case_2_all_timsteps.copy()\n",
    "\n",
    "# Initialize the figure and 3D axis\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Initialize a scatter plot with dummy data\n",
    "scat = ax.scatter([], [], [], c='r')\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim([0, 5])\n",
    "ax.set_ylim([0, 5])\n",
    "ax.set_zlim([0, 5])\n",
    "\n",
    "# Function to update the scatter plot for each frame\n",
    "def update(frame):\n",
    "    # Filter the DataFrame for the current time step\n",
    "    current_data = df[df['time'] == frame]\n",
    "    # Update the scatter plot data\n",
    "    scat._offsets3d = (current_data['x'], current_data['y'], current_data['z'])\n",
    "    ax.set_title(f'Time step: {frame}')\n",
    "    return scat,\n",
    "\n",
    "# Create the animation\n",
    "ani = FuncAnimation(fig, update, frames=range(time_steps), interval=200, blit=False)\n",
    "\n",
    "# Save the animation as an MP4 file\n",
    "writer = FFMpegWriter(fps=5, metadata=dict(artist='Me'), bitrate=1800)\n",
    "ani.save('particle_movement.mp4', writer=writer)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Create a sample DataFrame\n",
    "raw_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho10_20percent_Re100.dat\")\n",
    "case_1_all_timsteps = raw_data[raw_data[\"case_ID\"]==1]\n",
    "case_2_all_timsteps = raw_data[raw_data[\"case_ID\"]==2]\n",
    "\n",
    "df = case_1_all_timsteps.copy()\n",
    "\n",
    "# Function to create frames\n",
    "def create_frames(df, output_dir='frames', resolution=(1920, 1080)):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    times = df['time'].unique()\n",
    "    for time in times:\n",
    "        fig = plt.figure(figsize=(resolution[0]/100, resolution[1]/100))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_xlim(df['x'].min(), df['x'].max())\n",
    "        ax.set_ylim(df['y'].min(), df['y'].max())\n",
    "        ax.set_zlim(df['z'].min(), df['z'].max())\n",
    "        ax.set_title(f'Time: {time}')\n",
    "        \n",
    "        subset = df[df['time'] == time]\n",
    "        sc = ax.scatter(subset['x'], subset['y'], subset['z'], c=subset['z'], cmap='viridis', marker='o',s=50)\n",
    "        fig.colorbar(sc, ax=ax, label='z')\n",
    "        \n",
    "        plt.savefig(f\"{output_dir}/frame_{time:04d}.png\")\n",
    "        plt.close()\n",
    "\n",
    "# Function to create video\n",
    "def create_video(output_file='output.mp4', frame_rate=10, resolution=(1920, 1080)):\n",
    "    images = [img for img in sorted(os.listdir('frames')) if img.endswith(\".png\")]\n",
    "    frame = cv2.imread(os.path.join('frames', images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    video = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (width, height))\n",
    "\n",
    "    for image in images:\n",
    "        video.write(cv2.imread(os.path.join('frames', image)))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "# Generate frames\n",
    "create_frames(df)\n",
    "\n",
    "# Create video\n",
    "create_video()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

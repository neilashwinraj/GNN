{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d15cf2-f47d-434b-84cb-94bb3c79c276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /localscratch-ssd/301700/matplotlib-0kidihi0 because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler,RobustScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b597fa-8c9b-4289-aa52-78fdd1691d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def get_periodic_coordinates(coord, size):\n",
    "    \"\"\"\n",
    "    Generate all coordinates within a cubic domain considering periodic boundary conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        coord (pandas dataframe): A pandas dataframe containing the columns (x, y, z) of a point.\n",
    "        size (int): The size of the cubic domain along each axis.\n",
    "    Returns:\n",
    "        list: A list of tuples containing all coordinates within the cubic domain.\n",
    "    \"\"\"\n",
    "    ### Keep copy of original dataframe and copy for each periodic bc shift ###\n",
    "    coord_copy = [coord.copy() for _ in range(27)]\n",
    "    stacked_df = pd.concat(coord_copy, axis=0)\n",
    "    stacked_df = stacked_df.reset_index(drop=True, inplace=False)\n",
    "    \n",
    "    # Get coordinates ###\n",
    "    if isinstance(coord, pd.DataFrame):\n",
    "        coord = coord[[\"x\",\"y\",\"z\"]].values\n",
    "\n",
    "    # Generate all combinations of displacements (-1, 0, 1) along each axis\n",
    "    displacements = list(itertools.product([-1, 0, 1], repeat=3))\n",
    "\n",
    "    # Generate all coordinates by applying periodic boundary conditions\n",
    "    tp_coordinates = list()\n",
    "    \n",
    "    for dx, dy, dz in displacements:\n",
    "          \n",
    "        temp = list()\n",
    "        \n",
    "        for i in range(len(coord)):\n",
    "            \n",
    "            x, y, z = coord[i,0],coord[i,1],coord[i,2]\n",
    "            \n",
    "            new_x = x + dx*size\n",
    "            new_y = y + dy*size\n",
    "            new_z = z + dz*size\n",
    "\n",
    "            temp.append((new_x,new_y,new_z))\n",
    "            \n",
    "        tp_coordinates.append( np.array(temp) )\n",
    "    \n",
    "    stacked_df[[\"x\",\"y\",\"z\"]] = np.vstack(tp_coordinates) \n",
    "    \n",
    "    return np.vstack(tp_coordinates),stacked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fc9653-576c-4e90-bb65-a82d652f6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_time_series_data(time_series_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Groups the data based on case_ID and time \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    ### load raw data from ze time series data ###\n",
    "    pd_list  = list()\n",
    "    \n",
    "    for (col1_val, col2_val), group in time_series_data.groupby(['case_ID', 'time']):\n",
    "    \n",
    "        pd_list.append(group)\n",
    "    \n",
    "    return pd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c4f4e0-3397-4a2a-ba51-141f5eb0b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearest_neighbor_data(time_series_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper function (in some sense, can be condensed more)to do the data generation \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    \n",
    "    pd_list = group_time_series_data(time_series_data)\n",
    "    \n",
    "    nearest_neighbor_data = list()\n",
    "    nearest_neighbor_data_extra = list()\n",
    "    scalar_data = list()\n",
    "    \n",
    "    ### Loop over different groups ###\n",
    "    \n",
    "    for i in range(len(pd_list)):\n",
    "        \n",
    "        print(\"Currently on case_time subgroup : \",str(i+1))\n",
    "        tp_particles,stacked_df = get_periodic_coordinates(pd_list[i],5)\n",
    "        tree = cKDTree(tp_particles)\n",
    "        \n",
    "        ### Loop over all particles in a group and getting the nearest neighbors ###\n",
    "        idx = np.stack([ tree.query(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values,16)[1] for j in range(len(pd_list[i])) ])\n",
    "        nearest_neighbor_data.append(tp_particles[idx])\n",
    "        \n",
    "        ### merging nodal data to the coordinates ###\n",
    "        nearest_neighbor_data_extra.append(merge_columns_to_pandas_list(tp_particles[idx]\n",
    "                                                                       ,[\"local_Re\",\"vpx\",\"vpy\",\"vpz\"],stacked_df))\n",
    "        \n",
    "        ### Getting the scalar data ###\n",
    "        scalar_data.append( pd_list[i][[\"Density_ratio\",\"glb_phi\",\"glb_Re\",\"Drag\"]] )\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    ### Populate graph and scalar lists ###\n",
    "    nearest_neighbor_data = np.stack(nearest_neighbor_data)\n",
    "    nearest_neighbor_data_extra = np.stack(nearest_neighbor_data_extra)\n",
    "    \n",
    "    nearest_neighbor_data = nearest_neighbor_data.reshape(nearest_neighbor_data.shape[0]*nearest_neighbor_data.shape[1]\n",
    "                                           ,nearest_neighbor_data.shape[2]*nearest_neighbor_data.shape[3])\n",
    "    \n",
    "    nearest_neighbor_data_extra = nearest_neighbor_data_extra.reshape(nearest_neighbor_data_extra.shape[0]*nearest_neighbor_data_extra.shape[1]\n",
    "                                           ,nearest_neighbor_data_extra.shape[2]*nearest_neighbor_data_extra.shape[3])\n",
    "    \n",
    "    scalar_data = np.stack(scalar_data)\n",
    "    scalar_data = scalar_data.reshape(scalar_data.shape[0]*scalar_data.shape[1],scalar_data.shape[2])    \n",
    "    \n",
    "    ### change code if you want to return nearest_neighbor_data or extra ### \n",
    "    return np.concatenate( (nearest_neighbor_data_extra,scalar_data) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7561cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns_to_pandas_list(nearest_neighbor_data,variable_list,master_dataframe):\n",
    "\n",
    "    \"\"\" given a list of pandas dataframe with the x,y,z locations and re and phi ,this function will\n",
    "        merge each pandas dataframe from the list with the master dataframe with all the columns  \n",
    "    \"\"\"\n",
    "\n",
    "    joined =[pd.DataFrame(nearest_neighbor_data[i],columns=[\"x\",\"y\",\"z\"]) for i in range(len(nearest_neighbor_data))]\n",
    "\n",
    "    for i in range(len(joined)):\n",
    "        \n",
    "        temp = copy.deepcopy(joined[i])\n",
    "        add = pd.merge(temp,master_dataframe,how=\"inner\",on=['x','y','z'],sort=False)[variable_list]\n",
    "        joined[i] = pd.concat([temp,add], axis=1)\n",
    "        \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f917bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_xyz_and_other_columns(train_df, test_df):\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = RobustScaler()\n",
    "    train_df_copy, test_df_copy = train_df.copy(), test_df.copy()\n",
    "    \n",
    "    # Find the columns starting with specific prefixes\n",
    "    x_columns = [col for col in train_df.columns if col.startswith('x_')]\n",
    "    y_columns = [col for col in train_df.columns if col.startswith('y_')]\n",
    "    z_columns = [col for col in train_df.columns if col.startswith('z_')]\n",
    "    \n",
    "    local_re_columns = [col for col in train_df.columns if col.startswith('local_Re_')]\n",
    "    \n",
    "    vpx_columns = [col for col in train_df.columns if col.startswith('vpx_')]\n",
    "    vpy_columns = [col for col in train_df.columns if col.startswith('vpy_')]\n",
    "    vpz_columns = [col for col in train_df.columns if col.startswith('vpz_')]\n",
    "\n",
    "    # Custom scaling for x_, y_, and z_ columns\n",
    "    for col in x_columns + y_columns + z_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - (-5)) / (10 - (-5))\n",
    "        test_df_copy[col] = (test_df_copy[col] - (-5)) / (10 - (-5))\n",
    "\n",
    "    # Compute global max for local_Re_, vpx_, vpy_, and vpz_ columns from train data\n",
    "    global_max_local_re = train_df_copy[local_re_columns].max().max()\n",
    "    global_max_vpx = train_df_copy[vpx_columns].max().max()\n",
    "    global_max_vpy = train_df_copy[vpy_columns].max().max()\n",
    "    global_max_vpz = train_df_copy[vpz_columns].max().max()\n",
    "\n",
    "    # Compute global min for local_Re_, vpx_, vpy_, and vpz_ columns from train data\n",
    "    global_min_local_re = train_df_copy[local_re_columns].min().min()\n",
    "    global_min_vpx = train_df_copy[vpx_columns].min().min()\n",
    "    global_min_vpy = train_df_copy[vpy_columns].min().min()\n",
    "    global_min_vpz = train_df_copy[vpz_columns].min().min()\n",
    "\n",
    "    # Scaling for local_Re_ columns\n",
    "    for col in local_re_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_local_re) / (global_max_local_re - global_min_local_re)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_local_re) / (global_max_local_re - global_min_local_re)\n",
    "\n",
    "    # Scaling for vpx_ columns\n",
    "    for col in vpx_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_vpx) / (global_max_vpx - global_min_vpx)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_vpx) / (global_max_vpx - global_min_vpx)\n",
    "\n",
    "    # Scaling for vpy_ columns\n",
    "    for col in vpy_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_vpy) / (global_max_vpy - global_min_vpy)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_vpy) / (global_max_vpy - global_min_vpy)\n",
    "\n",
    "    # Scaling for vpz_ columns\n",
    "    for col in vpz_columns:\n",
    "        train_df_copy[col] = (train_df_copy[col] - global_min_vpz) / (global_max_vpz - global_min_vpz)\n",
    "        test_df_copy[col] = (test_df_copy[col] - global_min_vpz) / (global_max_vpz - global_min_vpz)\n",
    "\n",
    "    # Scaling for other columns using MinMaxScaler\n",
    "    for col in train_df.columns:\n",
    "        if not (col.startswith('x_') or col.startswith('y_') or col.startswith('z_') or \n",
    "                col.startswith('local_Re_') or col.startswith('vpx_') or \n",
    "                col.startswith('vpy_') or col.startswith('vpz_')):\n",
    "            train_df_copy[col] = scaler.fit_transform(train_df_copy[[col]])\n",
    "            test_df_copy[col] = scaler.transform(test_df_copy[[col]])\n",
    "            \n",
    "    return train_df_copy, test_df_copy,scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d2b2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_and_sample(df, time_col, n_bins, sample_frac=0.5):\n",
    "    \"\"\"\n",
    "    Bins the rows into n different groups based on the time column, and randomly selects 50% of the rows from each bin.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    time_col (str): The name of the time column (assumed to be integers).\n",
    "    n_bins (int): The number of bins to divide the time into.\n",
    "    sample_frac (float): The fraction of rows to sample from each bin (default is 0.5).\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with randomly selected rows from each bin.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create bins\n",
    "    df['time_bin'] = pd.cut(df[time_col], bins=n_bins, labels=False)\n",
    "    \n",
    "    # Function to randomly select a fraction of rows from each bin\n",
    "    def select_random_fraction(group):\n",
    "        sample_size = int(len(group) * sample_frac)\n",
    "        return group.sample(n=sample_size, random_state=3, replace=False)\n",
    "    \n",
    "    # Apply the function to each group\n",
    "    result_df = df.groupby('time_bin').apply(select_random_fraction).reset_index(drop=True)\n",
    "    \n",
    "    # Drop the bin column\n",
    "    result_df = result_df.drop(columns=['time_bin'])\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b5edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column, sigma):\n",
    "    \"\"\"\n",
    "    Remove rows from the DataFrame where the values in the specified column\n",
    "    are more than a given number of standard deviations (sigma) away from the mean.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    column (str): The column to check for outliers.\n",
    "    sigma (float): The number of standard deviations to use as the threshold.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    threshold = sigma * std\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[abs(df[column] - mean) <= threshold]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ad026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_average_based(df, column, magnitude):\n",
    "    \n",
    "    mean = df[column].mean()\n",
    "    \n",
    "    print(\"Mean Value = \",mean)\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[abs(df[column]) <= mean*magnitude]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c02bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation_matrix_x_axis():\n",
    "    \"\"\"Generates a random rotation matrix for rotation about the x-axis.\"\"\"\n",
    "    theta = np.random.uniform(0, 2 * np.pi)\n",
    "    rotation_matrix = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(theta), -np.sin(theta)],\n",
    "        [0, np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "    return rotation_matrix, theta\n",
    "\n",
    "\n",
    "def rotate_and_append(df, n,num_neighbors = 15):\n",
    "    \"\"\"\n",
    "    Randomly sample n rows from the dataframe, rotate the 3D coordinates\n",
    "    using a random rotation matrix around the x-axis, and append the rotated rows back to the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing 3D coordinates with columns x_poi, y_poi, z_poi and x_1, y_1, z_1, ..., x_15, y_15, z_15\n",
    "    n (int): Number of rows to sample from the DataFrame\n",
    "    \"\"\"\n",
    "    # Randomly sample n rows without replacement\n",
    "    sampled_df = df.sample(n=n, replace=False).copy()\n",
    "    df_copy = df.copy()\n",
    "    point_columns = [\"x_poi\",\"y_poi\",\"z_poi\"]\n",
    "    \n",
    "    for i in range(1, num_points + 1):\n",
    "        point_columns.extend([f'x_{i}', f'y_{i}', f'z_{i}'])    \n",
    "    \n",
    "    for index, row in sampled_df.iterrows():\n",
    "        \n",
    "        # Get the rotation matrix\n",
    "        rotation_matrix,_ = random_rotation_matrix_x_axis()\n",
    "         \n",
    "        # Perform the rotation on each 3D coordinate\n",
    "        rotated_position_vectors = np.stack([ np.dot(row[point_columns].values.reshape(num_neighbors+1,3)[i][None,:],\n",
    "                                                    rotation_matrix) for i in range(num_neighbors+1)]).flatten()\n",
    "\n",
    "        sampled_df.loc[index, point_columns] = rotated_position_vectors\n",
    "    \n",
    "    return pd.concat([df_copy,sampled_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a36725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_vectors(row,num_neighbors=5):\n",
    "    # Extract the origin (POI)\n",
    "    x_origin, y_origin, z_origin = row['x_poi'], row['y_poi'], row['z_poi']\n",
    "    \n",
    "    # Extract the position vectors (x_1, y_1, z_1, ... , x_15, y_15, z_15)\n",
    "    x_vectors = [row[f'x_{i}'] for i in range(1, num_neighbors+1)]\n",
    "    y_vectors = [row[f'y_{i}'] for i in range(1, num_neighbors+1)]\n",
    "    z_vectors = [row[f'z_{i}'] for i in range(1, num_neighbors+1)]\n",
    "    \n",
    "    # Create scatter plot of all vectors (including the origin)\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot the origin\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[x_origin], y=[y_origin], z=[z_origin],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='red'),\n",
    "        name='Origin'\n",
    "    ))\n",
    "\n",
    "    # Plot the vectors\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_vectors, y=y_vectors, z=z_vectors,\n",
    "        mode='markers',\n",
    "        marker=dict(size=4, color='blue'),\n",
    "        name='Vectors'\n",
    "    ))\n",
    "\n",
    "    # Draw lines from the origin to each vector\n",
    "    for i in range(num_neighbors):\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[x_origin, x_vectors[i]], \n",
    "            y=[y_origin, y_vectors[i]], \n",
    "            z=[z_origin, z_vectors[i]],\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=2),\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z'\n",
    "    ),\n",
    "    title='3D Vectors from Origin',\n",
    "    showlegend=True)\n",
    "    \n",
    "    # Show the figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa1651",
   "metadata": {},
   "source": [
    "# Outlier filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39795732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_list = [\n",
    "#                    \"rho2_10percent_Re10\",\"rho2_10percent_Re50\",\"rho2_10percent_Re100\",\"rho2_10percent_Re200\",\n",
    "#                    \"rho2_10percent_Re300\",\n",
    "#                    \"rho2_20percent_Re10\",\"rho2_20percent_Re50\",\"rho2_20percent_Re100\",\"rho2_20percent_Re200\",\n",
    "#                    \"rho2_20percent_Re300\",\n",
    "#                    \"rho2_30percent_Re10\",\"rho2_30percent_Re50\",\"rho2_30percent_Re100\",\"rho2_30percent_Re200\",\n",
    "#                    \"rho2_30percent_Re300\",\n",
    "#                    \"rho2_40percent_Re10\",\"rho2_40percent_Re50\",\"rho2_40percent_Re100\",\"rho2_40percent_Re200\",\n",
    "#                    \"rho2_40percent_Re300\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7695661f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(experiment_list)):\n",
    "    \n",
    "#     experiment = experiment_list[i]\n",
    "#     time_series_data = pd.read_csv(\"../ze_time_series_data_raw/data_with_xyz_velocities/\"+experiment+\".dat\",index_col=False)\n",
    "\n",
    "# #     reduced_time_series = remove_outliers(df=time_series_data, column=\"Drag\", sigma=4.0)\n",
    "#     reduced_time_series = remove_outliers_average_based(df=time_series_data, column=\"Drag\", magnitude=3.0)\n",
    "    \n",
    "#     sns.kdeplot(reduced_time_series[\"Drag\"].values)\n",
    "#     plt.show()\n",
    "#     print(experiment)\n",
    "#     print(\"reduction in the dataset size %\", (time_series_data.shape[0] - reduced_time_series.shape[0])/time_series_data.shape[0])\n",
    "#     print(\"size of orig and filtered data\", time_series_data.shape[0] , reduced_time_series.shape[0])\n",
    "#     print(\"Mean drag\",time_series_data[\"Drag\"].mean())\n",
    "#     print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade62ff2",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0d93504-ae34-4f09-b4f2-cf14bf501100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on case_time subgroup :  367\n"
     ]
    }
   ],
   "source": [
    "### Read data ###\n",
    "experiment = \"rho2_40percent_Re300\"\n",
    "time_series_data = pd.read_csv(\"../ze_time_series_data_raw/data_with_xyz_velocities/\"+experiment+\".dat\",index_col=False)\n",
    "\n",
    "pd_list = group_time_series_data(time_series_data)\n",
    "nearest_neighbor_data = generate_nearest_neighbor_data(time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10cc71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the particle id ###\n",
    "num_particle_1 = time_series_data[ (time_series_data[\"case_ID\"]==1) & (time_series_data[\"time\"]==1)].shape[0]\n",
    "total_time_1 = time_series_data[ (time_series_data[\"case_ID\"]==1)].iloc[-1][\"time\"]\n",
    "\n",
    "num_particle_2 = time_series_data[ (time_series_data[\"case_ID\"]==2) & (time_series_data[\"time\"]==1)].shape[0]\n",
    "total_time_2 = time_series_data[ (time_series_data[\"case_ID\"]==2)].iloc[-1][\"time\"]\n",
    "\n",
    "particle_id = np.concatenate( (np.tile( (np.arange(num_particle_1)+1)[:,None],(int(total_time_1), 1)),\n",
    "                               np.tile( (np.arange(num_particle_2)+1)[:,None],(int(total_time_2), 1))),axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51256a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of time steps for case 1 and 2 respectively 222 , 145\n",
      "80% percent length for case 1 and 2 respectively 177 , 116\n",
      "Shape before outlier rejection :   34618\n",
      "Mean Value =  57.18778606161534\n",
      "Shape after outlier rejection :   34452\n",
      "Min,Max and Mean drag forces :  171.28322 0.044876 56.44343501811216\n"
     ]
    }
   ],
   "source": [
    "### Renaming the columns ###\n",
    "num_points = 15\n",
    "\n",
    "# Names for the first 7 columns\n",
    "initial_columns = ['x_poi', 'y_poi', 'z_poi', 'local_Re_poi', 'vpx_poi', 'vpy_poi', 'vpz_poi']\n",
    "\n",
    "# Names for each set of 7 columns for each point\n",
    "point_columns = []\n",
    "for i in range(1, num_points + 1):\n",
    "    point_columns.extend([f'x_{i}', f'y_{i}', f'z_{i}', f'local_Re_{i}', f'vpx_{i}', f'vpy_{i}', f'vpz_{i}'])\n",
    "\n",
    "# Names for the last 4 columns\n",
    "final_columns = ['Density_ratio', 'glb_phi', 'glb_Re', 'Drag']\n",
    "\n",
    "# Combine all column names\n",
    "column_names = initial_columns + point_columns + final_columns\n",
    "\n",
    "### make a dataframe version of the nearesrt_negihbor_dat numpy array ###\n",
    "nearest_neighbor_data_pd = pd.DataFrame(nearest_neighbor_data, columns=column_names)\n",
    "\n",
    "### adding time and case and particle ID columns ###\n",
    "nearest_neighbor_data_pd.insert(0, 'case_ID', time_series_data[\"case_ID\"])\n",
    "nearest_neighbor_data_pd.insert(1, 'particle_ID', particle_id)\n",
    "# nearest_neighbor_data_pd.insert(2, 'time', time_series_data[\"time\"])\n",
    "nearest_neighbor_data_pd.insert(2, 'time', time_series_data[\"time\"])\n",
    "\n",
    "print(\"Total number of time steps for case 1 and 2 respectively\" , \n",
    "      time_series_data[time_series_data[\"case_ID\"]==1][\"time\"].values[-1],',',\n",
    "      time_series_data[time_series_data[\"case_ID\"]==2][\"time\"].values[-1])\n",
    "\n",
    "print(\"80% percent length for case 1 and 2 respectively\" , \n",
    "      int(time_series_data[time_series_data[\"case_ID\"]==1][\"time\"].values[-1]*0.80),',',\n",
    "      int(time_series_data[time_series_data[\"case_ID\"]==2][\"time\"].values[-1]*0.80) )\n",
    "\n",
    "### Keeping only positive Drag Forces ###\n",
    "nearest_neighbor_data_pd = nearest_neighbor_data_pd.loc[nearest_neighbor_data_pd['Drag'] >= 0]\n",
    "\n",
    "### Remove outliers of very high/ low drags ###\n",
    "print(\"Shape before outlier rejection :  \",nearest_neighbor_data_pd.shape[0])\n",
    "nearest_neighbor_data_pd = remove_outliers_average_based(df=nearest_neighbor_data_pd, column=\"Drag\", magnitude=3.0)\n",
    "print(\"Shape after outlier rejection :  \",nearest_neighbor_data_pd.shape[0])\n",
    "print(\"Min,Max and Mean drag forces : \",nearest_neighbor_data_pd[\"Drag\"].values.max(),\n",
    "                                        nearest_neighbor_data_pd[\"Drag\"].values.min(),\n",
    "                                        nearest_neighbor_data_pd[\"Drag\"].values.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40fb37e",
   "metadata": {},
   "source": [
    "# Extrapolation Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ed66174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train and test dataset :  (27537, 119) (6915, 119)\n"
     ]
    }
   ],
   "source": [
    "# # Select rows for the train dataset (80/20) split ###\n",
    "start_time = 0\n",
    "interval_length_1 = int(time_series_data[time_series_data[\"case_ID\"]==1][\"time\"].values[-1]*0.80)\n",
    "interval_length_2 = int(time_series_data[time_series_data[\"case_ID\"]==2][\"time\"].values[-1]*0.80) \n",
    "\n",
    "train_case_1 = nearest_neighbor_data_pd[(nearest_neighbor_data_pd['case_ID'] == 1) & (nearest_neighbor_data_pd['time'] <= start_time+interval_length_1)\n",
    "                                                                                   & (nearest_neighbor_data_pd['time'] >= start_time)]\n",
    "\n",
    "train_case_2 = nearest_neighbor_data_pd[(nearest_neighbor_data_pd['case_ID'] == 2) & (nearest_neighbor_data_pd['time'] <= start_time+interval_length_2)\n",
    "                                                                                   & (nearest_neighbor_data_pd['time'] >= start_time)]\n",
    "\n",
    "# # Combine the selected rows for the train dataset\n",
    "train_dataset = pd.concat([train_case_1, train_case_2])\n",
    "\n",
    "# # Select all other rows for the test dataset\n",
    "test_dataset = nearest_neighbor_data_pd.drop(train_dataset.index)\n",
    "\n",
    "print(\"shape of train and test dataset : \",train_dataset.shape,test_dataset.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594284c",
   "metadata": {},
   "source": [
    "# For POI based coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81cdf112",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for POI based coordinates ### \n",
    "for col in train_dataset.columns:\n",
    "    \n",
    "    if col.startswith(\"x_\") and not col.endswith(\"_poi\"):\n",
    "        train_dataset[col] -= train_dataset[\"x_poi\"]\n",
    "        \n",
    "    if col.startswith(\"y_\") and not col.endswith(\"_poi\"):\n",
    "        train_dataset[col] -= train_dataset[\"y_poi\"]\n",
    "        \n",
    "    if col.startswith(\"z_\") and not col.endswith(\"_poi\"):\n",
    "        train_dataset[col] -= train_dataset[\"z_poi\"]\n",
    "        \n",
    "for col in test_dataset.columns:\n",
    "    \n",
    "    if col.startswith(\"x_\") and not col.endswith(\"_poi\"):\n",
    "        test_dataset[col] -= test_dataset[\"x_poi\"]\n",
    "        \n",
    "    if col.startswith(\"y_\") and not col.endswith(\"_poi\"): \n",
    "        test_dataset[col] -= test_dataset[\"y_poi\"]\n",
    "        \n",
    "    if col.startswith(\"z_\") and not col.endswith(\"_poi\"): \n",
    "        test_dataset[col] -= test_dataset[\"z_poi\"]\n",
    "        \n",
    "\n",
    "### Set poi to zeros ###\n",
    "### un-scaled data ###\n",
    "train_dataset[[\"x_poi\",\"y_poi\",\"z_poi\"]] = 0\n",
    "test_dataset[[\"x_poi\",\"y_poi\",\"z_poi\"]] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb6eea",
   "metadata": {},
   "source": [
    "# Rotational Invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfd77639",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate synthetic datapoints for rotational invariance soft constraint ###\n",
    "num_synthetic = 500\n",
    "train_dataset = rotate_and_append(train_dataset,num_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16318ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2781056443200007 -0.2781056443200006\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "red",
          "size": 6
         },
         "mode": "markers",
         "name": "Origin",
         "type": "scatter3d",
         "x": [
          0
         ],
         "y": [
          0
         ],
         "z": [
          0
         ]
        },
        {
         "marker": {
          "color": "blue",
          "size": 4
         },
         "mode": "markers",
         "name": "Vectors",
         "type": "scatter3d",
         "x": [
          0.2885500000000001,
          -0.5706900000000001
         ],
         "y": [
          -0.70705,
          0.7025000000000001
         ],
         "z": [
          0.7204179999999996,
          0.5320099999999996
         ]
        },
        {
         "line": {
          "color": "black",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          0,
          0.2885500000000001
         ],
         "y": [
          0,
          -0.70705
         ],
         "z": [
          0,
          0.7204179999999996
         ]
        },
        {
         "line": {
          "color": "black",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          0,
          -0.5706900000000001
         ],
         "y": [
          0,
          0.7025000000000001
         ],
         "z": [
          0,
          0.5320099999999996
         ]
        },
        {
         "marker": {
          "color": "red",
          "size": 6
         },
         "mode": "markers",
         "name": "Origin",
         "type": "scatter3d",
         "x": [
          0
         ],
         "y": [
          0
         ],
         "z": [
          0
         ]
        },
        {
         "marker": {
          "color": "blue",
          "size": 4
         },
         "mode": "markers",
         "name": "Vectors",
         "type": "scatter3d",
         "x": [
          0.2885500000000001,
          -0.5706900000000001
         ],
         "y": [
          0.5465534308203935,
          0.6739696898558668
         ],
         "z": [
          0.8486466546699258,
          -0.5677197788130923
         ]
        },
        {
         "line": {
          "color": "yellow",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          0,
          0.2885500000000001
         ],
         "y": [
          0,
          0.5465534308203935
         ],
         "z": [
          0,
          0.8486466546699258
         ]
        },
        {
         "line": {
          "color": "yellow",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          0,
          -0.5706900000000001
         ],
         "y": [
          0,
          0.6739696898558668
         ],
         "z": [
          0,
          -0.5677197788130923
         ]
        }
       ],
       "layout": {
        "scene": {
         "xaxis": {
          "title": {
           "text": "X"
          }
         },
         "yaxis": {
          "title": {
           "text": "Y"
          }
         },
         "zaxis": {
          "title": {
           "text": "Z"
          }
         }
        },
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "3D Vectors from Origin"
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"59b671e0-aff6-4410-80af-541c2b1ff24f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"59b671e0-aff6-4410-80af-541c2b1ff24f\")) {                    Plotly.newPlot(                        \"59b671e0-aff6-4410-80af-541c2b1ff24f\",                        [{\"marker\":{\"color\":\"red\",\"size\":6},\"mode\":\"markers\",\"name\":\"Origin\",\"x\":[0.0],\"y\":[0.0],\"z\":[0.0],\"type\":\"scatter3d\"},{\"marker\":{\"color\":\"blue\",\"size\":4},\"mode\":\"markers\",\"name\":\"Vectors\",\"x\":[0.2885500000000001,-0.5706900000000001],\"y\":[-0.70705,0.7025000000000001],\"z\":[0.7204179999999996,0.5320099999999996],\"type\":\"scatter3d\"},{\"line\":{\"color\":\"black\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[0.0,0.2885500000000001],\"y\":[0.0,-0.70705],\"z\":[0.0,0.7204179999999996],\"type\":\"scatter3d\"},{\"line\":{\"color\":\"black\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[0.0,-0.5706900000000001],\"y\":[0.0,0.7025000000000001],\"z\":[0.0,0.5320099999999996],\"type\":\"scatter3d\"},{\"marker\":{\"color\":\"red\",\"size\":6},\"mode\":\"markers\",\"name\":\"Origin\",\"x\":[0.0],\"y\":[0.0],\"z\":[0.0],\"type\":\"scatter3d\"},{\"marker\":{\"color\":\"blue\",\"size\":4},\"mode\":\"markers\",\"name\":\"Vectors\",\"x\":[0.2885500000000001,-0.5706900000000001],\"y\":[0.5465534308203935,0.6739696898558668],\"z\":[0.8486466546699258,-0.5677197788130923],\"type\":\"scatter3d\"},{\"line\":{\"color\":\"yellow\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[0.0,0.2885500000000001],\"y\":[0.0,0.5465534308203935],\"z\":[0.0,0.8486466546699258],\"type\":\"scatter3d\"},{\"line\":{\"color\":\"yellow\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[0.0,-0.5706900000000001],\"y\":[0.0,0.6739696898558668],\"z\":[0.0,-0.5677197788130923],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"xaxis\":{\"title\":{\"text\":\"X\"}},\"yaxis\":{\"title\":{\"text\":\"Y\"}},\"zaxis\":{\"title\":{\"text\":\"Z\"}}},\"title\":{\"text\":\"3D Vectors from Origin\"},\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('59b671e0-aff6-4410-80af-541c2b1ff24f');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    ### Rotation unit tests ###\n",
    "    \n",
    "    num_neighbors=2\n",
    "    \n",
    "    temp = train_dataset[train_dataset[\"Drag\"]==train_dataset[\"Drag\"].values[-np.random.randint(750)]]\n",
    "    dot_1 = np.dot(temp.iloc[0][[\"x_1\",\"y_1\",\"z_1\"]],temp.iloc[0][[\"x_2\",\"y_2\",\"z_2\"]])\n",
    "    dot_2 = np.dot(temp.iloc[1][[\"x_1\",\"y_1\",\"z_1\"]],temp.iloc[1][[\"x_2\",\"y_2\",\"z_2\"]])\n",
    "    \n",
    "    print(dot_1,dot_2)\n",
    "    \n",
    "    # Extract the origin (POI)\n",
    "    row = temp.iloc[0]\n",
    "    x_origin, y_origin, z_origin = row['x_poi'], row['y_poi'], row['z_poi']\n",
    "    \n",
    "    # Extract the position vectors (x_1, y_1, z_1, ... , x_15, y_15, z_15)\n",
    "    x_vectors = [row[f'x_{i}'] for i in range(1, num_neighbors+1)]\n",
    "    y_vectors = [row[f'y_{i}'] for i in range(1, num_neighbors+1)]\n",
    "    z_vectors = [row[f'z_{i}'] for i in range(1, num_neighbors+1)]\n",
    "    \n",
    "    # Create scatter plot of all vectors (including the origin)\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot the origin\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[x_origin], y=[y_origin], z=[z_origin],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='red'),\n",
    "        name='Origin'\n",
    "    ))\n",
    "\n",
    "    # Plot the vectors\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_vectors, y=y_vectors, z=z_vectors,\n",
    "        mode='markers',\n",
    "        marker=dict(size=4, color='blue'),\n",
    "        name='Vectors'\n",
    "    ))\n",
    "\n",
    "    # Draw lines from the origin to each vector\n",
    "    for i in range(num_neighbors):\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[x_origin, x_vectors[i]], \n",
    "            y=[y_origin, y_vectors[i]], \n",
    "            z=[z_origin, z_vectors[i]],\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=2),\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z'\n",
    "    ),\n",
    "    title='3D Vectors from Origin',\n",
    "    showlegend=True)\n",
    "    \n",
    "    # Extract the origin (POI)\n",
    "    row = temp.iloc[1]\n",
    "    x_origin, y_origin, z_origin = row['x_poi'], row['y_poi'], row['z_poi']\n",
    "    \n",
    "    # Extract the position vectors (x_1, y_1, z_1, ... , x_15, y_15, z_15)\n",
    "    x_vectors = [row[f'x_{i}'] for i in range(1, num_neighbors+1)]\n",
    "    y_vectors = [row[f'y_{i}'] for i in range(1, num_neighbors+1)]\n",
    "    z_vectors = [row[f'z_{i}'] for i in range(1, num_neighbors+1)]\n",
    "    \n",
    "    # Plot the origin\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[x_origin], y=[y_origin], z=[z_origin],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='red'),\n",
    "        name='Origin'\n",
    "    ))\n",
    "\n",
    "    # Plot the vectors\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_vectors, y=y_vectors, z=z_vectors,\n",
    "        mode='markers',\n",
    "        marker=dict(size=4, color='blue'),\n",
    "        name='Vectors'\n",
    "    ))\n",
    "\n",
    "    # Draw lines from the origin to each vector\n",
    "    for i in range(num_neighbors):\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[x_origin, x_vectors[i]], \n",
    "            y=[y_origin, y_vectors[i]], \n",
    "            z=[z_origin, z_vectors[i]],\n",
    "            mode='lines',\n",
    "            line=dict(color='yellow', width=2),\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z'\n",
    "    ),\n",
    "    title='3D Vectors from Origin',\n",
    "    showlegend=True)\n",
    "    \n",
    "    # Show the figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cddd20",
   "metadata": {},
   "source": [
    "# Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e5bf2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_scaled,test_dataset_scaled,scaler = scale_xyz_and_other_columns(train_dataset,test_dataset)\n",
    "\n",
    "### scaled data ###\n",
    "train_dataset_scaled[[\"x_poi\",\"y_poi\",\"z_poi\"]] = 0\n",
    "test_dataset_scaled[[\"x_poi\",\"y_poi\",\"z_poi\"]] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a52797",
   "metadata": {},
   "source": [
    "# Save the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97df22b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['extrapolation/rho2_40percent_Re300/80_20_time_split/scaler.save']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "scaler_filename = \"extrapolation/\"+experiment+\"/80_20_time_split/\" + \"scaler.save\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "\n",
    "# And now to load...\n",
    "# scaler = joblib.load(scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "199bdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the case, particle and time identifiers for the test identifiers ###\n",
    "test_dataset[[\"case_ID\",\"particle_ID\",\"time\"]].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/cpt_identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8bf07134",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the unscaled data ###\n",
    "train_dataset.iloc[:, 2:-4].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/train_input_unscaled\")\n",
    "test_dataset.iloc[:, 2:-4].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/test_input_unscaled\")\n",
    "\n",
    "train_dataset.iloc[:, [0,-4,-3,-2]].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/train_input_scalar_unscaled\")\n",
    "test_dataset.iloc[:, [0,-4,-3,-2]].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/test_input_scalar_unscaled\")\n",
    "\n",
    "train_dataset.iloc[:, -1:].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/train_output_unscaled\")\n",
    "test_dataset.iloc[:, -1:].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/test_output_unscaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3fc7ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the scaled data ###\n",
    "train_dataset_scaled.iloc[:, 2:-4].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/train_input\")\n",
    "test_dataset_scaled.iloc[:, 2:-4].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/test_input\")\n",
    "\n",
    "train_dataset_scaled.iloc[:, [0,-4,-3,-2]].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/train_input_scalar\")\n",
    "test_dataset_scaled.iloc[:, [0,-4,-3,-2]].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/test_input_scalar\")\n",
    "\n",
    "train_dataset_scaled.iloc[:, -1:].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/train_output\")\n",
    "test_dataset_scaled.iloc[:, -1:].to_csv(\"extrapolation/\"+experiment+\"/80_20_time_split/test_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f07a32",
   "metadata": {},
   "source": [
    "# Combining all Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [\n",
    "                   \"rho2_10percent_Re10\",\"rho2_10percent_Re50\",\"rho2_10percent_Re100\",\"rho2_10percent_Re200\",\n",
    "                   \"rho2_10percent_Re300\",\n",
    "                   \"rho2_20percent_Re10\",\"rho2_20percent_Re50\",\"rho2_20percent_Re100\",\"rho2_20percent_Re200\",\n",
    "                   \"rho2_20percent_Re300\",\n",
    "                   \"rho2_30percent_Re10\",\"rho2_30percent_Re50\",\"rho2_30percent_Re100\",\"rho2_30percent_Re200\",\n",
    "                   \"rho2_30percent_Re300\",\n",
    "                   \"rho2_40percent_Re10\",\"rho2_40percent_Re50\",\"rho2_40percent_Re100\",\"rho2_40percent_Re200\",\n",
    "                   \"rho2_40percent_Re300\",\n",
    "                   \n",
    "                   \"rho10_10percent_Re10\",\"rho10_10percent_Re50\",\"rho10_10percent_Re100\",\"rho10_10percent_Re200\",\n",
    "                   \"rho10_10percent_Re300\",\n",
    "                   \"rho10_20percent_Re10\",\"rho10_20percent_Re50\",\"rho10_20percent_Re100\",\"rho10_20percent_Re200\",\n",
    "                   \"rho10_20percent_Re300\",\n",
    "                   \"rho10_30percent_Re10\",\"rho10_30percent_Re50\",\"rho10_30percent_Re100\",\"rho10_30percent_Re200\",\n",
    "                   \"rho10_30percent_Re300\",\n",
    "                   \"rho10_40percent_Re10\",\"rho10_40percent_Re50\",\"rho10_40percent_Re100\",\"rho10_40percent_Re200\",\n",
    "                   \"rho10_40percent_Re300\",\n",
    "    \n",
    "                   \"rho100_10percent_Re10\",\"rho100_10percent_Re50\",\"rho100_10percent_Re100\",\"rho100_10percent_Re200\",\n",
    "                   \"rho100_10percent_Re300\",\n",
    "                   \"rho100_20percent_Re10\",\"rho100_20percent_Re50\",\"rho100_20percent_Re100\",\"rho100_20percent_Re200\",\n",
    "                   \"rho100_20percent_Re300\",\n",
    "                   \"rho100_30percent_Re10\",\"rho100_30percent_Re50\",\"rho100_30percent_Re100\",\"rho100_30percent_Re200\",\n",
    "                   \"rho100_30percent_Re300\",\n",
    "                   \"rho100_40percent_Re10\",\"rho100_40percent_Re50\",\"rho100_40percent_Re100\",\"rho100_40percent_Re200\",\n",
    "                   \"rho100_40percent_Re300\"]\n",
    "\n",
    "### define lists\n",
    "train_input_list = list()\n",
    "train_input_scalar_list = list()\n",
    "train_output_list = list()\n",
    "\n",
    "test_input_list = list()\n",
    "test_input_scalar_list = list()\n",
    "test_output_list = list()\n",
    "\n",
    "for i in range(len(experiment_list)):\n",
    "    \n",
    "    loc = \"extrapolation/\"+experiment_list[i]+\"/120_150_simple/\"\n",
    "    \n",
    "    train_input = pd.read_csv(loc+\"train_input_unscaled\",index_col=False)\n",
    "    train_input_scalar = pd.read_csv(loc+\"train_input_scalar_unscaled\",index_col=False)\n",
    "    train_output = pd.read_csv(loc+\"train_output_unscaled\",index_col=False)\n",
    "    \n",
    "    test_input = pd.read_csv(loc+\"test_input_unscaled\",index_col=False)\n",
    "    test_input_scalar = pd.read_csv(loc+\"test_input_scalar_unscaled\",index_col=False)\n",
    "    test_output = pd.read_csv(loc+\"test_output_unscaled\",index_col=False)\n",
    "    \n",
    "    ### Removing first columns ###\n",
    "    train_input = train_input.drop('Unnamed: 0', axis=1)\n",
    "    train_input_scalar = train_input_scalar.drop('Unnamed: 0', axis=1)\n",
    "    train_output = train_output.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    test_input = test_input.drop('Unnamed: 0', axis=1)\n",
    "    test_input_scalar = test_input_scalar.drop('Unnamed: 0', axis=1)\n",
    "    test_output = test_output.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    ### appending to the lists ###\n",
    "    train_input_list.append(train_input)\n",
    "    train_input_scalar_list.append(train_input_scalar)\n",
    "    train_output_list.append(train_output)\n",
    "    \n",
    "    test_input_list.append(test_input)\n",
    "    test_input_scalar_list.append(test_input_scalar)\n",
    "    test_output_list.append(test_output)\n",
    "    \n",
    "### Converting to a dataframe ###\n",
    "train_input_list = pd.concat(train_input_list)\n",
    "train_input_scalar_list = pd.concat(train_input_scalar_list)\n",
    "train_output_list = pd.concat(train_output_list)\n",
    "\n",
    "test_input_list = pd.concat(test_input_list)\n",
    "test_input_scalar_list = pd.concat(test_input_scalar_list)\n",
    "test_output_list = pd.concat(test_output_list)\n",
    "\n",
    "### combine all daatframes to form a common dataframe for train and test ###\n",
    "train_dataset = pd.concat([train_input_list,train_input_scalar_list,train_output_list], axis=1)\n",
    "test_dataset = pd.concat([test_input_list,test_input_scalar_list,test_output_list], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for POI based coordinates ### \n",
    "for col in train_dataset.columns:\n",
    "    \n",
    "    if col.startswith(\"x_\") and not col.endswith(\"_poi\"):\n",
    "        train_dataset[col] -= train_dataset[\"x_poi\"]\n",
    "        \n",
    "    if col.startswith(\"y_\") and not col.endswith(\"_poi\"):\n",
    "        train_dataset[col] -= train_dataset[\"y_poi\"]\n",
    "        \n",
    "    if col.startswith(\"z_\") and not col.endswith(\"_poi\"):\n",
    "        train_dataset[col] -= train_dataset[\"z_poi\"]\n",
    "        \n",
    "for col in test_dataset.columns:\n",
    "    \n",
    "    if col.startswith(\"x_\") and not col.endswith(\"_poi\"):\n",
    "        test_dataset[col] -= test_dataset[\"x_poi\"]\n",
    "        \n",
    "    if col.startswith(\"y_\") and not col.endswith(\"_poi\"): \n",
    "        test_dataset[col] -= test_dataset[\"y_poi\"]\n",
    "        \n",
    "    if col.startswith(\"z_\") and not col.endswith(\"_poi\"): \n",
    "        test_dataset[col] -= test_dataset[\"z_poi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combined dataset scaling ###\n",
    "train_dataset_scaled,test_dataset_scaled = scale_xyz_and_other_columns(train_dataset,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the unscaled data ###\n",
    "train_dataset.iloc[:, 0:-4].to_csv(\"extrapolation/combined_data/120_150_simple/train_input_unscaled\")\n",
    "test_dataset.iloc[:, 0:-4].to_csv(\"extrapolation/combined_data/120_150_simple/test_input_unscaled\")\n",
    "\n",
    "train_dataset.iloc[:, -4:-1].to_csv(\"extrapolation/combined_data/120_150_simple/train_input_scalar_unscaled\")\n",
    "test_dataset.iloc[:, -4:-1].to_csv(\"extrapolation/combined_data/120_150_simple/test_input_scalar_unscaled\")\n",
    "\n",
    "train_dataset.iloc[:, -1:].to_csv(\"extrapolation/combined_data/120_150_simple/train_output_unscaled\")\n",
    "test_dataset.iloc[:, -1:].to_csv(\"extrapolation/combined_data/120_150_simple/test_output_unscaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290aeb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the scaled data ###\n",
    "train_dataset_scaled.iloc[:, 0:-4].to_csv(\"extrapolation/combined_data/120_150_simple/train_input\")\n",
    "test_dataset_scaled.iloc[:, 0:-4].to_csv(\"extrapolation/combined_data/120_150_simple/test_input\")\n",
    "\n",
    "train_dataset_scaled.iloc[:, -4:-1].to_csv(\"extrapolation/combined_data/120_150_simple/train_input_scalar\")\n",
    "test_dataset_scaled.iloc[:, -4:-1].to_csv(\"extrapolation/combined_data/120_150_simple/test_input_scalar\")\n",
    "\n",
    "train_dataset_scaled.iloc[:, -1:].to_csv(\"extrapolation/combined_data/120_150_simple/train_output\")\n",
    "test_dataset_scaled.iloc[:, -1:].to_csv(\"extrapolation/combined_data/120_150_simple/test_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d1378",
   "metadata": {},
   "source": [
    "# Verfiy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_euclidean(vec_1,vec_2):\n",
    "    \n",
    "    return np.sqrt( (vec_1[0]-vec_2[0])**2 + (vec_1[1]-vec_2[1])**2 + (vec_1[2]-vec_2[2])**2 )\n",
    "\n",
    "def brute_search(query,tree,n_nearest=16):\n",
    "\n",
    "    ### finds the nearest neighbors with a basic algorithm ###\n",
    "    ### both query and the 'tree' must be numpy arrays ###\n",
    "    dist = np.stack([dist_euclidean(query,tree[i]) for i in range(len(tree))])\n",
    "\n",
    "    return np.argsort(dist)[0:n_nearest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40cc837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho100_10percent_Re100.dat\")\n",
    "pd_list = group_time_series_data(time_series_data)\n",
    "nn_list = list()\n",
    "\n",
    "for i in range(len(pd_list)):\n",
    "    \n",
    "    _,stacked_df = get_periodic_coordinates(pd_list[i],5)\n",
    "    print(\"Case_time subset number : \",str(i+1))\n",
    "    \n",
    "    for j in range(len(pd_list[i])):\n",
    "        \n",
    "        ### getting nearest neighbors ###\n",
    "        poi = np.array(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values)\n",
    "        idx = brute_search(poi,stacked_df[[\"x\",\"y\",\"z\"]].values)\n",
    "        nn_list.append(stacked_df.iloc[idx])\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "### Defining the arrays to compare ###\n",
    "nn_list = np.stack(nn_list)\n",
    "previous = np.stack([ nearest_neighbor_data[i][0:64].reshape(16,4)[:,0:3] for i in range(len(nearest_neighbor_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d7e46",
   "metadata": {},
   "source": [
    "# Verify 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a71af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Excel file\n",
    "time_series_data = pd.read_csv(\"../ze_time_series_data_raw/data_with_xyz_velocities/\"+experiment+\".dat\",index_col=False)\n",
    "pd_list = group_time_series_data(time_series_data)\n",
    "\n",
    "def create_periodic_images(coords, Lx, Ly, Lz):\n",
    "    \n",
    "    shifts = [-1, 0, 1]\n",
    "    extended_coords = []\n",
    "\n",
    "    for shift_x in shifts:\n",
    "        for shift_y in shifts:\n",
    "            for shift_z in shifts:\n",
    "                shift_vector = np.array([shift_x * Lx, shift_y * Ly, shift_z * Lz])\n",
    "                extended_coords.append(coords + shift_vector)\n",
    "\n",
    "    return np.concatenate(extended_coords, axis=0)\n",
    "\n",
    "def find_nearest_neighbors(coords, extended_coords, Lx, Ly, Lz, num_neighbors=15):\n",
    "    \n",
    "    tree = cKDTree(extended_coords)\n",
    "    distances, indices = tree.query(coords, k=num_neighbors + 1)  # +1 because the query point itself is included\n",
    "    \n",
    "    return indices  # Exclude the query point itself\n",
    "\n",
    "\n",
    "nearest_neighbor_data_verify_2 = list()\n",
    "\n",
    "for i in range(len(pd_list)):\n",
    "    print(\"case_time subgroup number : \",str(i+1))\n",
    "    df = pd_list[i]\n",
    "\n",
    "    # Extract coordinates into a numpy array\n",
    "    coordinates = df[['x', 'y', 'z']].values\n",
    "\n",
    "    # Define the domain size (replace with actual domain sizes)\n",
    "    Lx, Ly, Lz = 5, 5, 5\n",
    "    \n",
    "    # Generate extended coordinates considering periodic images\n",
    "    extended_coords = create_periodic_images(coordinates, Lx, Ly, Lz)\n",
    "\n",
    "    # Find the 15 nearest neighbors for each point\n",
    "    nearest_neighbors = find_nearest_neighbors(coordinates, extended_coords, Lx, Ly, Lz)\n",
    "\n",
    "    # Output the nearest neighbors\n",
    "    # print(nearest_neighbors)\n",
    "    nearest_neighbor_data_verify_2.append(extended_coords[nearest_neighbors])\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "nearest_neighbor_data_verify_2 = np.vstack(nearest_neighbor_data_verify_2)\n",
    "nearest_neighbor_data_verify_2 = nearest_neighbor_data_verify_2.reshape(nearest_neighbor_data_verify_2.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6017c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = nearest_neighbor_data_pd.filter(regex=r'^(x_|y_|z_)')\n",
    "decision = np.array_equal(nearest_neighbor_data_verify_2,filtered_df.values)\n",
    "if decision:\n",
    "    print(\"Its correct !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263ce26",
   "metadata": {},
   "source": [
    "# Ze data comparision unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load ze training data ###\n",
    "ze_data = np.load(\"../ze_time_series_data_raw/TrainTest_shared_by_ze/train_random_all.npy\")\n",
    "# ze_data = ze_data[:, np.concatenate([np.arange(45), [-2]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adba909",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_columns = []\n",
    "for i in range(1, num_points + 1):\n",
    "    point_columns.extend([f'x_{i}', f'y_{i}', f'z_{i}'])\n",
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "for i in range(100):\n",
    "    matching_index = np.where((ze_data[:,0:45] == train_dataset.iloc[i][point_columns].values[None,:].astype('float32')).all(axis=1))[0]\n",
    "    print(\"Index in the train_dataset Data Frame and the index in ze_dat : \",i,matching_index)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c12271",
   "metadata": {},
   "source": [
    "# Verify Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check scaling ###\n",
    "temp = train_dataset.values\n",
    "temp = np.array(temp[:,2:-4])\n",
    "\n",
    "temp_nodal = temp.reshape(temp.shape[0],16,7)\n",
    "temp_nodal = temp_nodal.reshape(temp_nodal.shape[0]*16,7)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "temp_nodal_scaled = scaler.fit_transform(temp_nodal)\n",
    "\n",
    "temp = temp_nodal_scaled.reshape(temp.shape[0],temp.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39abb00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = train_dataset_scaled.values[:,2:-4] - temp\n",
    "\n",
    "res_pd = pd.DataFrame(res,columns=[\"x_poi\",\"y_poi\",\"z_poi\",\"local_Re_poi\",\"vpx_poi\",\"vpy_poi\",\"vpz_poi\",\n",
    "                                   \"x_1\",\"y_1\",\"z_1\",\"local_Re_1\",\"vpx_1\",\"vpy_1\",\"vpz_1\",\n",
    "                                   \"x_2\",\"y_2\",\"z_2\",\"local_Re_2\",\"vpx_2\",\"vpy_2\",\"vpz_2\",\n",
    "                                   \"x_3\",\"y_3\",\"z_3\",\"local_Re_3\",\"vpx_3\",\"vpy_3\",\"vpz_3\",\n",
    "                                   \"x_4\",\"y_4\",\"z_4\",\"local_Re_4\",\"vpx_4\",\"vpy_4\",\"vpz_4\",\n",
    "                                   \"x_5\",\"y_5\",\"z_5\",\"local_Re_5\",\"vpx_5\",\"vpy_5\",\"vpz_5\",\n",
    "                                   \"x_6\",\"y_6\",\"z_6\",\"local_Re_6\",\"vpx_6\",\"vpy_6\",\"vpz_6\",\n",
    "                                   \"x_7\",\"y_7\",\"z_7\",\"local_Re_7\",\"vpx_7\",\"vpy_7\",\"vpz_7\",\n",
    "                                   \"x_8\",\"y_8\",\"z_8\",\"local_Re_8\",\"vpx_8\",\"vpy_8\",\"vpz_8\",\n",
    "                                   \"x_9\",\"y_9\",\"z_9\",\"local_Re_9\",\"vpx_9\",\"vpy_9\",\"vpz_9\",\n",
    "                                   \"x_10\",\"y_10\",\"z_10\",\"local_Re_10\",\"vpx_10\",\"vpy_10\",\"vpz_10\",  \n",
    "                                   \"x_11\",\"y_11\",\"z_11\",\"local_Re_11\",\"vpx_11\",\"vpy_11\",\"vpz_11\",\n",
    "                                   \"x_12\",\"y_12\",\"z_12\",\"local_Re_12\",\"vpx_12\",\"vpy_12\",\"vpz_12\",\n",
    "                                   \"x_13\",\"y_13\",\"z_13\",\"local_Re_13\",\"vpx_13\",\"vpy_13\",\"vpz_13\",\n",
    "                                   \"x_14\",\"y_14\",\"z_14\",\"local_Re_14\",\"vpx_14\",\"vpy_14\",\"vpz_14\",\n",
    "                                   \"x_15\",\"y_15\",\"z_15\",\"local_Re_15\",\"vpx_15\",\"vpy_15\",\"vpz_15\",\n",
    "                                  ])\n",
    "\n",
    "columns_to_drop = res_pd.filter(regex='^(x_|y_|z_)').columns\n",
    "res_pd = res_pd.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed58281",
   "metadata": {},
   "source": [
    "# Statiscal Analysis of the Raw data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b9c7f",
   "metadata": {},
   "source": [
    "Raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a18f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho100_40percent_Re300.dat\")\n",
    "pd_list = group_time_series_data(raw_data)\n",
    "raw_data_case_1 = raw_data[raw_data[\"case_ID\"]==1]\n",
    "raw_data_case_2 = raw_data[raw_data[\"case_ID\"]==2]\n",
    "sns.kdeplot(raw_data_case_1[\"local_Re\"],fill=True)\n",
    "sns.kdeplot(raw_data_case_2[\"local_Re\"],fill=True)\n",
    "plt.legend([\"Case 1\",\"Case 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(raw_data_case_1[[\"x\",\"y\",\"z\"]].values.flatten(),fill=True)\n",
    "sns.kdeplot(raw_data_case_2[[\"x\",\"y\",\"z\"]].values.flatten(),fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5692fa",
   "metadata": {},
   "source": [
    "Scaled Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_train = np.load(\"/home/neilashwinraj/gnns/simple_connections/simple_connections_data/time_split/rho100_10percent_Re100/train_inputs.npy\")\n",
    "scaled_data_test = np.load(\"/home/neilashwinraj/gnns/simple_connections/simple_connections_data/time_split/rho100_10percent_Re100/test_inputs.npy\")\n",
    "\n",
    "sns.kdeplot(scaled_data_train[:,:,0].flatten())\n",
    "sns.kdeplot(scaled_data_test[:,:,0].flatten())\n",
    "plt.legend([\"Train\",\"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c249234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_distances_default(distance_list):\n",
    "    hist, bin_edges = np.histogram( distance_list )\n",
    "    return hist, bin_edges\n",
    "\n",
    "def histogram_distances(distance_list, max_dist, bin_size):\n",
    "    # this is the list of bins in which to calculate\n",
    "    bins = np.arange(0, max_dist+bin_size, bin_size)\n",
    "    hist, bin_edges = np.histogram( distance_list, bins=bins )\n",
    "    return hist, bin_edges\n",
    "\n",
    "def plot_histogram(hist,bin_edges):\n",
    "    #for N bins, there are N+1 bin edges. The centers can be found by averaging the positions of \n",
    "    # bin edge0 and 1, 1 and 2, ..., N-1 and N\n",
    "    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2.0\n",
    "    plt.plot(bin_centers,hist,marker='o')\n",
    "    plt.ylabel(\"N(r)\")\n",
    "    plt.xlabel(\"$r$\")\n",
    "    \n",
    "def compute_distances_minimum_image( configuration, box_size ):\n",
    "    distance_list = []\n",
    "    num_particles = configuration.shape[0]\n",
    "    k=0\n",
    "    for i in range(num_particles):\n",
    "        for j in range(num_particles):\n",
    "            if i == j: continue\n",
    "            \n",
    "            posi = configuration[i]\n",
    "            posj = configuration[j]\n",
    "            # compute the euclidian distance between pos1 and pos2 and call it 'dist' \n",
    "            # there are many ways to do this\n",
    "            # you can certainly look up how to do this online if you can't figure it out right away\n",
    "            \n",
    "            #dr is a vector (dx,dy)\n",
    "            dr = posj-posi\n",
    "            #minimum image dr - can you figure out why this works?\n",
    "            dr = dr - box_size*np.floor(dr/box_size+0.5)\n",
    "            \n",
    "            #dr2 is a vector (dx*dx,dy*dy)\n",
    "            dr2 = dr*dr \n",
    "            #dist = sqrt( dx^2 + dy^2)\n",
    "            dist = np.sqrt( dr2.sum() )            \n",
    "            distance_list.append(dist)\n",
    "            \n",
    "    return np.array(distance_list)\n",
    "\n",
    "def plot_rdf(gofr,bin_centers):\n",
    "    plt.plot(bin_centers,gofr,marker='o')\n",
    "    plt.ylabel(\"g(r)\")\n",
    "    plt.xlabel(\"$r$\")\n",
    "    \n",
    "def get_gofr(hist,bin_edges,num_particles, box_size):\n",
    "    \n",
    "    rho = num_particles/(box_size**3)\n",
    "    bin_centers = (bin_edges[1:]+bin_edges[:-1])/2.0\n",
    "    dr = bin_edges[1]-bin_edges[0]\n",
    "    denominator = 4.*np.pi*(bin_centers**2)*dr*rho*( num_particles )\n",
    "    gofr = hist/denominator\n",
    "    \n",
    "    return gofr, bin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f80406",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho100_40percent_Re300.dat\")\n",
    "pd_list = group_time_series_data(raw_data)\n",
    "\n",
    "x_list = list()\n",
    "y_list = list()\n",
    "\n",
    "### Apllying functions to calculate the RDF ###\n",
    "for i in range(len(pd_list)):\n",
    "   \n",
    "    \n",
    "    distance_list = compute_distances_minimum_image( pd_list[i][[\"x\",\"y\",\"z\"]].values, 5 )\n",
    "    hist, bin_edges = histogram_distances(distance_list=distance_list, max_dist=(5/2)*np.sqrt(3),bin_size=0.1)\n",
    "    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2.0\n",
    "    \n",
    "    gofr, bin_centers = get_gofr(hist,bin_edges,num_particles = len(pd_list[i]), box_size=5)\n",
    "    x_list.append(bin_centers)\n",
    "    y_list.append(gofr)\n",
    "    \n",
    "    plt.plot( bin_centers,gofr)\n",
    "    \n",
    "    plt.ylim([-0.05,3.25])\n",
    "    plt.show()\n",
    "    print(\"Currently at case : \",str(np.unique(pd_list[i][\"case_ID\"].values)[0])\n",
    "          ,\"and time step : \",np.unique(pd_list[i][\"time\"].values)[0])\n",
    "    clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d842cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_time_steps_case_1 = raw_data[raw_data[\"case_ID\"]==1][\"time\"].values.max()\n",
    "for i in range(no_time_steps_case_1):\n",
    "    plt.plot(x_list[i],y_list[i],c=\"b\",alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_time_steps_case_2 = raw_data[raw_data[\"case_ID\"]==2][\"time\"].values.max()\n",
    "for i in range(no_time_steps_case_1,no_time_steps_case_1+no_time_steps_case_2):\n",
    "    plt.plot(x_list[i],y_list[i],c=\"r\",alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "\n",
    "# Create a sample DataFrame\n",
    "raw_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho10_20percent_Re100.dat\")\n",
    "case_1_all_timsteps = raw_data[raw_data[\"case_ID\"]==1]\n",
    "case_2_all_timsteps = raw_data[raw_data[\"case_ID\"]==2]\n",
    "\n",
    "### for case 1\n",
    "# num_particles = len(case_1_all_timsteps[case_1_all_timsteps[\"time\"]==1])\n",
    "# time_steps = case_1_all_timsteps[\"time\"].values[-1]\n",
    "# df = case_1_all_timsteps.copy()\n",
    "\n",
    "### for case 2\n",
    "num_particles = len(case_2_all_timsteps[case_2_all_timsteps[\"time\"]==1])\n",
    "time_steps = case_2_all_timsteps[\"time\"].values[-1]\n",
    "df = case_2_all_timsteps.copy()\n",
    "\n",
    "# Initialize the figure and 3D axis\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Initialize a scatter plot with dummy data\n",
    "scat = ax.scatter([], [], [], c='r')\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim([0, 5])\n",
    "ax.set_ylim([0, 5])\n",
    "ax.set_zlim([0, 5])\n",
    "\n",
    "# Function to update the scatter plot for each frame\n",
    "def update(frame):\n",
    "    # Filter the DataFrame for the current time step\n",
    "    current_data = df[df['time'] == frame]\n",
    "    # Update the scatter plot data\n",
    "    scat._offsets3d = (current_data['x'], current_data['y'], current_data['z'])\n",
    "    ax.set_title(f'Time step: {frame}')\n",
    "    return scat,\n",
    "\n",
    "# Create the animation\n",
    "ani = FuncAnimation(fig, update, frames=range(time_steps), interval=200, blit=False)\n",
    "\n",
    "# Save the animation as an MP4 file\n",
    "writer = FFMpegWriter(fps=5, metadata=dict(artist='Me'), bitrate=1800)\n",
    "ani.save('particle_movement.mp4', writer=writer)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Create a sample DataFrame\n",
    "raw_data = pd.read_csv(\"/home/neilashwinraj/gnns/ze_time_series_data_raw/rho10_20percent_Re100.dat\")\n",
    "case_1_all_timsteps = raw_data[raw_data[\"case_ID\"]==1]\n",
    "case_2_all_timsteps = raw_data[raw_data[\"case_ID\"]==2]\n",
    "\n",
    "df = case_1_all_timsteps.copy()\n",
    "\n",
    "# Function to create frames\n",
    "def create_frames(df, output_dir='frames', resolution=(1920, 1080)):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    times = df['time'].unique()\n",
    "    for time in times:\n",
    "        fig = plt.figure(figsize=(resolution[0]/100, resolution[1]/100))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_xlim(df['x'].min(), df['x'].max())\n",
    "        ax.set_ylim(df['y'].min(), df['y'].max())\n",
    "        ax.set_zlim(df['z'].min(), df['z'].max())\n",
    "        ax.set_title(f'Time: {time}')\n",
    "        \n",
    "        subset = df[df['time'] == time]\n",
    "        sc = ax.scatter(subset['x'], subset['y'], subset['z'], c=subset['z'], cmap='viridis', marker='o',s=50)\n",
    "        fig.colorbar(sc, ax=ax, label='z')\n",
    "        \n",
    "        plt.savefig(f\"{output_dir}/frame_{time:04d}.png\")\n",
    "        plt.close()\n",
    "\n",
    "# Function to create video\n",
    "def create_video(output_file='output.mp4', frame_rate=10, resolution=(1920, 1080)):\n",
    "    images = [img for img in sorted(os.listdir('frames')) if img.endswith(\".png\")]\n",
    "    frame = cv2.imread(os.path.join('frames', images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    video = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (width, height))\n",
    "\n",
    "    for image in images:\n",
    "        video.write(cv2.imread(os.path.join('frames', image)))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "# Generate frames\n",
    "create_frames(df)\n",
    "\n",
    "# Create video\n",
    "create_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4026a5",
   "metadata": {},
   "source": [
    "# Garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2298ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the time thresholds 80/20 split ###\n",
    "# time_1 = int(time_series_data[time_series_data[\"case_ID\"]==1][\"time\"].values[-1]*0.80)\n",
    "# time_2 = int(time_series_data[time_series_data[\"case_ID\"]==2][\"time\"].values[-1]*0.80)\n",
    "\n",
    "# # Select rows for the train dataset based on the given conditions\n",
    "# train_case_1 = nearest_neighbor_data_pd[(nearest_neighbor_data_pd['case_ID'] == 1) & (nearest_neighbor_data_pd['time'] <= time_1)]\n",
    "# train_case_2 = nearest_neighbor_data_pd[(nearest_neighbor_data_pd['case_ID'] == 2) & (nearest_neighbor_data_pd['time'] <= time_2)]\n",
    "\n",
    "# # Combine the selected rows for the train dataset\n",
    "# train_dataset = pd.concat([train_case_1, train_case_2])\n",
    "\n",
    "# # Select all other rows for the test dataset\n",
    "# test_dataset = nearest_neighbor_data_pd.drop(train_dataset.index)\n",
    "\n",
    "# print(\"shape of train and test dataset : \",train_dataset.shape,test_dataset.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Ze paper split with time interval definition ###\n",
    "# start_time = 0\n",
    "# interval_length = 150\n",
    "# interval_length_val = 30\n",
    "\n",
    "# # Select rows for the train dataset based on the given conditions ###\n",
    "# train_case_1 = nearest_neighbor_data_pd[(nearest_neighbor_data_pd['case_ID'] == 1) & (nearest_neighbor_data_pd['time'] <= start_time+interval_length)\n",
    "#                                                                    & (nearest_neighbor_data_pd['time'] >= start_time)]\n",
    "\n",
    "# train_case_2 = nearest_neighbor_data_pd[(nearest_neighbor_data_pd['case_ID'] == 2) & (nearest_neighbor_data_pd['time'] <= start_time+interval_length)\n",
    "#                                                                    & (nearest_neighbor_data_pd['time'] >= start_time)  ]\n",
    "\n",
    "# # Combine the selected rows for the train dataset\n",
    "# train_dataset = pd.concat([train_case_1, train_case_2])\n",
    "# print(\"Size of the non sample train dataframe\",train_dataset.shape)\n",
    "\n",
    "# # Select all other rows for the test dataset\n",
    "# test_dataset = nearest_neighbor_data_pd.drop(train_dataset.index)\n",
    "# test_dataset = test_dataset[(test_dataset[\"time\"] <= start_time+interval_length+interval_length_val)&\n",
    "#                             (test_dataset[\"time\"] >= start_time+interval_length)]\n",
    "\n",
    "# ### Randomly sample after binning ###\n",
    "# train_case_1_bin_sampled = bin_and_sample(df = train_dataset[train_dataset[\"case_ID\"]==1], time_col=\"time\", n_bins=10, sample_frac=0.5)\n",
    "# train_case_2_bin_sampled = bin_and_sample(df = train_dataset[train_dataset[\"case_ID\"]==2], time_col=\"time\", n_bins=10, sample_frac=0.5)\n",
    "\n",
    "# ### reconstruct train dataset ###\n",
    "# train_dataset = pd.concat([train_case_1_bin_sampled,train_case_2_bin_sampled])\n",
    "\n",
    "# print(\"shape of train (after sampling) and test dataset : \",train_dataset.shape,test_dataset.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Saving the unscaled data ###\n",
    "# train_dataset.iloc[:, 2:-4].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global/train_input_unscaled\")\n",
    "# test_dataset.iloc[:, 2:-4].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global/test_input_unscaled\")\n",
    "\n",
    "# train_dataset.iloc[:, [0,-4,-3,-2]].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global/train_input_scalar_unscaled\")\n",
    "# test_dataset.iloc[:, [0,-4,-3,-2]].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global/test_input_scalar_unscaled\")\n",
    "\n",
    "# train_dataset.iloc[:, -1:].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global//train_output_unscaled\")\n",
    "# test_dataset.iloc[:, -1:].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global//test_output_unscaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633fd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Saving the scaled data ###\n",
    "# train_dataset_scaled.iloc[:, 2:-4].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global/train_input\")\n",
    "# test_dataset_scaled.iloc[:, 2:-4].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global//test_input\")\n",
    "\n",
    "# train_dataset_scaled.iloc[:, [0,-4,-3,-2]].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global//train_input_scalar\")\n",
    "# test_dataset_scaled.iloc[:, [0,-4,-3,-2]].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global//test_input_scalar\")\n",
    "\n",
    "# train_dataset_scaled.iloc[:, -1:].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global//train_output\")\n",
    "# test_dataset_scaled.iloc[:, -1:].to_csv(\"/home/neilashwinraj/gnns/temporal_gnns/ablation_study/datasets_dev/rho2_phi20_Re10_simple_global//test_output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

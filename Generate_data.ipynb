{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "63d15cf2-f47d-434b-84cb-94bb3c79c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial import cKDTree\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from torch import nn\n",
    "import torch_geometric.nn as geom_nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0531b3fe-c11f-4302-8564-17ed23db92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_shift(particles,shift=10):\n",
    "\n",
    "    ### Y shift ###\n",
    "    y_pos = particles.copy()\n",
    "    y_pos[\"y\"] = particles[\"y\"]+shift\n",
    "\n",
    "    y_neg = particles.copy()\n",
    "    y_neg[\"y\"] = particles[\"y\"]-shift\n",
    "\n",
    "    ### Z shift ###\n",
    "    z_pos = particles.copy()\n",
    "    z_pos[\"z\"] = particles[\"z\"]+shift\n",
    "\n",
    "    z_neg = particles.copy()\n",
    "    z_neg[\"z\"] = particles[\"z\"]-shift\n",
    "\n",
    "    ### Diagonal shifts ###\n",
    "    ### y_pos_z_pos ###\n",
    "    y_pos_z_pos = particles.copy()\n",
    "    y_pos_z_pos[\"y\"] = particles[\"y\"]+shift\n",
    "    y_pos_z_pos[\"z\"] = particles[\"z\"]+shift\n",
    "\n",
    "    ### y_pos_z_neg ###\n",
    "    y_pos_z_neg = particles.copy()\n",
    "    y_pos_z_neg[\"y\"] = particles[\"y\"]+shift\n",
    "    y_pos_z_neg[\"z\"] = particles[\"z\"]-shift\n",
    "\n",
    "    ### y_neg_z_pos ###\n",
    "    y_neg_z_pos = particles.copy()\n",
    "    y_neg_z_pos[\"y\"] = particles[\"y\"]-shift\n",
    "    y_neg_z_pos[\"z\"] = particles[\"z\"]+shift\n",
    "\n",
    "    ### y_neg_z_neg ###\n",
    "    y_neg_z_neg = particles.copy()\n",
    "    y_neg_z_neg[\"y\"] = particles[\"y\"]-shift\n",
    "    y_neg_z_neg[\"z\"] = particles[\"z\"]-shift\n",
    "\n",
    "    return pd.concat([particles,y_pos,y_neg,z_pos,z_neg,y_pos_z_pos,y_pos_z_neg,y_neg_z_pos,y_neg_z_neg])\n",
    "\n",
    "### array Differences function ###\n",
    "\n",
    "def array_difference(array1,array2):\n",
    "    ### First one must be the bigger array ###\n",
    "    set1 = set(map(tuple, array1))\n",
    "    set2 = set(map(tuple, array2))\n",
    "    \n",
    "    # Find the set difference\n",
    "    set_difference = set1 - set2\n",
    "\n",
    "    # Convert the set difference back to a NumPy array\n",
    "    return np.array(list(set_difference))\n",
    "\n",
    "def add_const_parameter(data,const_param):\n",
    "\n",
    "    ### Given a numpy array of size m*n , it will return a numpy array of size m*(n+1), where\n",
    "    ### the last column has all elements as const_param ###\n",
    "\n",
    "    add_column = np.ones(len(data))*const_param\n",
    "    \n",
    "    return np.hstack((data, add_column[:,None]))\n",
    "\n",
    "\n",
    "def mirror_edge_index(edge_index):\n",
    "\n",
    "    first_row = edge_index[0:1, :]\n",
    "    second_row = edge_index[1:, :]\n",
    "    output_tensor = np.concatenate((second_row, first_row), axis=0)\n",
    "    \n",
    "    return np.concatenate((edge_index,output_tensor),axis=1)\n",
    "\n",
    "\n",
    "def weighted_averaging(data):\n",
    "    \n",
    "    idx = np.where(data[0][:,4]==0)[0]\n",
    "    deg_1 = data[0][idx,1:4].sum()\n",
    "\n",
    "    idx = np.where(data[0][:,4]==1)[0]\n",
    "    deg_2 = data[0][idx,1:4].sum()\n",
    "\n",
    "    idx = np.where(data[0][:,4]==2)[0]\n",
    "    deg_3 = data[0][idx,1:4].sum()\n",
    "\n",
    "    return deg_1 + 0.5*deg_2 + 0.25*deg_3 \n",
    "\n",
    "\n",
    "def merge_columns_to_pandas_list(pandas_list,variable_list,master_dataframe):\n",
    "\n",
    "    \"\"\" given a list of pandas dataframe with the x,y,z locations and re and phi ,this function will\n",
    "        merge each pandas dataframe from the list with the master dataframe with all the columns  \n",
    "    \"\"\"\n",
    "\n",
    "    joined = copy.deepcopy(pandas_list)\n",
    "\n",
    "    for i in range(len(joined)):\n",
    "        print(\"Particle number : \",str(i+1))\n",
    "        \n",
    "        temp = copy.deepcopy(joined[i])\n",
    "\n",
    "        add = pd.merge(temp,master_dataframe,how=\"inner\",on=['Center_x','Re'],sort=False)[variable_list]\n",
    "        joined[i] = pd.concat([temp,add], axis=1)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    return joined\n",
    "\n",
    "\n",
    "def generator(all_particles,dev_particles,sf,master_dataframe):\n",
    "\n",
    "    ### Given the pandas data of developed particles and all the particles including the particle shifted for periodicity ###\n",
    "    ### This functions works for one dataset for a particular solid fraction at a time ###\n",
    "    ### Master dataframe for the particular aspect ratio needs to be input to combine with the other \n",
    "    \n",
    "    tree = cKDTree(all_particles.values)\n",
    "    idx = np.stack([tree.query(dev_particles.iloc[i][[\"x\",\"y\",\"z\"]].values,16)[1] for i in range(len(dev_particles))])\n",
    "    input_dat = np.stack([all_particles.iloc[idx[i]] for i in range(len(idx))])\n",
    "\n",
    "    ### Adding solid fraction column ###\n",
    "    input_dat = [add_const_parameter(input_dat[i],sf) for i in range(len(input_dat))]\n",
    "\n",
    "    ### Adding 4 Reynolds numbers and combining the datasets ###\n",
    "    input_dat = np.vstack( ( np.stack( [add_const_parameter(input_dat[i],10) for i in range(len(input_dat))] ),\n",
    "                              np.stack( [add_const_parameter(input_dat[i],50) for i in range(len(input_dat))] ),\n",
    "                              np.stack( [add_const_parameter(input_dat[i],100) for i in range(len(input_dat))] ),\n",
    "                              np.stack( [add_const_parameter(input_dat[i],200) for i in range(len(input_dat))] ),\n",
    "                    ) )\n",
    "    input_dat = [ pd.DataFrame( input_dat[i],columns=[\"Center_x\",\"Center_y\",\"Center_z\",\"Phi\",\"Re\"]) for i in range(len(input_dat)) ]\n",
    "    \n",
    "    ### Joing with df array to get inclination and drag and other quantities if needed ###\n",
    "    input_dat = merge_columns_to_pandas_list(pandas_list=input_dat ,variable_list=[\"AR\",\"Inclination_angle\",\"Drag_local\"],master_dataframe=master_dataframe)\n",
    "    input_dat = [ np.concatenate( (input_dat[i][[\"Center_x\",\"Center_y\",\"Center_z\",\"Inclination_angle\"]].values.flatten()\n",
    "                                       ,input_dat[i].iloc[0][[\"AR\",\"Phi\",\"Re\",\"Drag_local\"]].values) )  for i in range(len(input_dat))]\n",
    "\n",
    "    return input_dat\n",
    "\n",
    "def dist_euclidean(vec_1,vec_2):\n",
    "\n",
    "    return np.sqrt( (vec_1[0]-vec_2[0])**2 + (vec_1[1]-vec_2[1])**2 + (vec_1[2]-vec_2[2])**2 )\n",
    "\n",
    "def brute_search(query,tree,n_nearest=16):\n",
    "\n",
    "    ### finds the nearest neighbors with a basic algorithm ###\n",
    "    ### both query and the 'tree' must be numpy arrays ###\n",
    "    \n",
    "    dist = np.stack([dist_euclidean(query,tree[i]) for i in range(len(tree))])\n",
    "\n",
    "    return np.argsort(dist)[0:n_nearest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611e8d71-5d84-4947-9883-4e40d6737748",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading Raw Data ###\n",
    "all_particles_2p5 = pd.read_csv('all_particle_data_files/AR2p5_all_particle_drag.dat.csv', header=0)\n",
    "all_particles_5 = pd.read_csv('all_particle_data_files/AR5_all_particle_drag.dat.csv', header=0)\n",
    "all_particles_10 = pd.read_csv('all_particle_data_files/AR10_all_particle_drag.dat.csv', header=0)\n",
    "all_tags_with_case_no = np.load(\"quick_load/all_tags_with_case_no.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1744312-f4c8-433e-9cb7-fab6531f4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ar=5 Sf=0.1 case 1 ###\n",
    "np.save(\"all_particle_data_files/center_phi_10_5_case_1.dat\",\n",
    "all_particles_5[(all_particles_5[\"phi\"]==10)&(all_particles_5[\"case\"]==2)&(all_particles_5[\"Re\"]==10)].values[:,2:11])\n",
    "\n",
    "### Ar=5 Sf=0.1 case 2 ###\n",
    "np.save(\"all_particle_data_files/center_phi_10_5_case_2.dat\",\n",
    "all_particles_5[(all_particles_5[\"phi\"]==10)&(all_particles_5[\"case\"]==3)&(all_particles_5[\"Re\"]==10)].values[:,2:11])\n",
    "\n",
    "### Ar=5 Sf=0.2 case 1 ###\n",
    "np.save(\"all_particle_data_files/center_phi_20_5_case_1.dat\",\n",
    "all_particles_5[(all_particles_5[\"phi\"]==20)&(all_particles_5[\"case\"]==1)&(all_particles_5[\"Re\"]==10)].values[:,2:11])\n",
    "\n",
    "### Ar=5 Sf=0.2 case 2 ###\n",
    "np.save(\"all_particle_data_files/center_phi_20_5_case_2.dat\",\n",
    "all_particles_5[(all_particles_5[\"phi\"]==20)&(all_particles_5[\"case\"]==3)&(all_particles_5[\"Re\"]==10)].values[:,2:11])\n",
    "\n",
    "### Ar=5 Sf=0.3 case 1 ###\n",
    "np.save(\"all_particle_data_files/center_phi_30_5_case_1.dat\",\n",
    "all_particles_5[(all_particles_5[\"phi\"]==30)&(all_particles_5[\"case\"]==1)&(all_particles_5[\"Re\"]==10)].values[:,2:11])\n",
    "\n",
    "### Ar=5 Sf=0.3 case 2 ###\n",
    "np.save(\"all_particle_data_files/center_phi_30_5_case_2.dat\",\n",
    "all_particles_5[(all_particles_5[\"phi\"]==30)&(all_particles_5[\"case\"]==2)&(all_particles_5[\"Re\"]==10)].values[:,2:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db408c0e-bdfd-43e2-8494-9870e29419b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ar=10 Sf=0.1 case 1 ###\n",
    "np.save(\"all_particle_data_files/center_phi_10_10_case_1.dat\",\n",
    "all_particles_10[(all_particles_10[\"phi\"]==10)&(all_particles_10[\"case\"]==1)&(all_particles_10[\"Re\"]==10)].values[:,2:11])\n",
    "\n",
    "### Ar=10 Sf=0.1 case 2 ###\n",
    "np.save(\"all_particle_data_files/center_phi_10_10_case_2.dat\",\n",
    "all_particles_10[(all_particles_10[\"phi\"]==10)&(all_particles_10[\"case\"]==2)&(all_particles_10[\"Re\"]==10)].values[:,2:11])\n",
    "\n",
    "# ### Ar=10 Sf=0.15 case 1 ###\n",
    "np.save(\"all_particle_data_files/center_phi_15_10_case_1.dat\",\n",
    "all_particles_10[(all_particles_10[\"phi\"]==15)&(all_particles_10[\"case\"]==1)&(all_particles_10[\"Re\"]==10)].values[:,2:11])\n",
    "\n",
    "# ### Ar=10 Sf=0.15 case 2 ###\n",
    "np.save(\"all_particle_data_files/center_phi_15_10_case_2.dat\",\n",
    "all_particles_10[(all_particles_10[\"phi\"]==15)&(all_particles_10[\"case\"]==2)&(all_particles_10[\"Re\"]==10)].values[:,2:11])\n",
    "\n",
    "# ### Ar=10 Sf=0.2 case 1 ###\n",
    "np.save(\"all_particle_data_files/center_phi_20_10_case_1.dat\",\n",
    "all_particles_10[(all_particles_10[\"phi\"]==20)&(all_particles_10[\"case\"]==1)&(all_particles_10[\"Re\"]==10)].values[:,2:11])\n",
    "\n",
    "# ### Ar=10 Sf=0.2 case 2 ###\n",
    "np.save(\"all_particle_data_files/center_phi_20_10_case_2.dat\",\n",
    "all_particles_10[(all_particles_10[\"phi\"]==20)&(all_particles_10[\"case\"]==2)&(all_particles_10[\"Re\"]==10)].values[:,2:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ddd36-9cb8-472e-95af-387ec7531b0d",
   "metadata": {},
   "source": [
    "AR = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6fb5f50f-cce5-45ba-ad3a-98ebf0d41eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all center data ###\n",
    "center_10 = pd.DataFrame(np.loadtxt(\"all_particle_data_files/center_phi_10_2p5.dat\"))\n",
    "center_10 = center_10.rename(columns={ center_10.columns[3]: \"x\" , center_10.columns[4]: \"y\" , center_10.columns[5]: \"z\"  })\n",
    "center_10_dev = center_10[(center_10['x']>7.5) & (center_10['x']<17.5)]\n",
    "\n",
    "center_20 = pd.DataFrame(np.loadtxt(\"all_particle_data_files/center_phi_20_2p5.dat\"))\n",
    "center_20 = center_20.rename(columns={ center_20.columns[3]: \"x\" , center_20.columns[4]: \"y\" , center_20.columns[5]: \"z\"  })\n",
    "center_20_dev = center_20[(center_20['x']>7.5) & (center_20['x']<17.5)]\n",
    "\n",
    "center_30 = pd.DataFrame(np.loadtxt(\"all_particle_data_files/center_phi_30_2p5.dat\"))\n",
    "center_30 = center_30.rename(columns={ center_30.columns[3]: \"x\" , center_30.columns[4]: \"y\" , center_30.columns[5]: \"z\"  })\n",
    "center_30_dev = center_30[(center_30['x']>7.5) & (center_30['x']<17.5)]\n",
    "\n",
    "# ### Defining all particles ###\n",
    "center_shifted_10 = particle_shift(center_10,shift=10)[['x','y','z']]\n",
    "center_shifted_20 = particle_shift(center_20,shift=10)[['x','y','z']]\n",
    "center_shifted_30 = particle_shift(center_30,shift=10)[['x','y','z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "66a637f1-728e-4832-ad14-a33988ae8c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particle number :  2300\n"
     ]
    }
   ],
   "source": [
    "input_dat_sf10_ar2p5 = generator(all_particles=center_shifted_10, dev_particles=center_10_dev ,sf=0.1, master_dataframe=all_particles_2p5)\n",
    "input_dat_sf20_ar2p5 = generator(all_particles=center_shifted_20, dev_particles=center_20_dev ,sf=0.2, master_dataframe=all_particles_2p5)\n",
    "input_dat_sf30_ar2p5 = generator(all_particles=center_shifted_30, dev_particles=center_30_dev ,sf=0.3, master_dataframe=all_particles_2p5)\n",
    "\n",
    "### Combine all subsets to form the ar=2.5 dataset ###\n",
    "all_data_ar2p5 = input_dat_sf10_ar2p5 + input_dat_sf20_ar2p5 + input_dat_sf30_ar2p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4498fe61-636b-43ac-b755-8de5713cbadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### start here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "845e2be1-ba9c-451a-be6e-18a1b24f0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_particles_2p5 = pd.read_csv('all_particle_data_files/AR2p5_all_particle_drag.dat.csv', header=0)\n",
    "verify_2p5 = pd.DataFrame(all_particles_2p5[[\"AR\",\"phi\",\"Re\",\"case\",\"Center_x\",\"Center_y\",\"Center_z\",\"Inclination_angle\",\"Drag_local\"]])\n",
    "verify_2p5 = verify_2p5.rename(columns={ verify_2p5.columns[4]: \"x\" , verify_2p5.columns[5]: \"y\" , verify_2p5.columns[6]: \"z\"  })\n",
    "verify_2p5 = particle_shift(verify_2p5)\n",
    "\n",
    "all_particles_5 = pd.read_csv('all_particle_data_files/AR5_all_particle_drag.dat.csv', header=0)\n",
    "verify_5 = pd.DataFrame(all_particles_5[[\"AR\",\"phi\",\"Re\",\"case\",\"Center_x\",\"Center_y\",\"Center_z\",\"Inclination_angle\",\"Drag_local\"]])\n",
    "verify_5 = verify_5.rename(columns={ verify_5.columns[4]: \"x\" , verify_5.columns[5]: \"y\" , verify_5.columns[6]: \"z\"  })\n",
    "verify_5 = particle_shift(verify_5)\n",
    "\n",
    "\n",
    "all_particles_10 = pd.read_csv('all_particle_data_files/AR10_all_particle_drag.dat.csv', header=0)\n",
    "verify_10 = pd.DataFrame(all_particles_10[[\"AR\",\"phi\",\"Re\",\"case\",\"Center_x\",\"Center_y\",\"Center_z\",\"Inclination_angle\",\"Drag_local\"]])\n",
    "verify_10 = verify_10.rename(columns={ verify_10.columns[4]: \"x\" , verify_10.columns[5]: \"y\" , verify_10.columns[6]: \"z\"  })\n",
    "verify_10 = particle_shift(verify_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "448ec67c-6249-453e-96a1-ca84ee306309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True]), array([ True]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Verify Ar2p5 ###\n",
    "\n",
    "### specify case of the experiment ###\n",
    "case = 1\n",
    "decision_coords_inclination = list()\n",
    "decision_drag = list()\n",
    "\n",
    "### Specify solid fraction ###\n",
    "data_in_use = np.array(input_dat_sf30_ar2p5)\n",
    "\n",
    "for i in range(len(data_in_use)):\n",
    "    \n",
    "    print(\"particle number : \",str(i+1))\n",
    "    verify_reduced = verify_2p5[(verify_2p5[\"Re\"]==data_in_use[i][-2])&(verify_2p5[\"phi\"]==data_in_use[i][-3]*100)&(verify_2p5[\"case\"]==case)]\n",
    "    indx = brute_search(data_in_use[i][0:3],verify_reduced[[\"x\",\"y\",\"z\"]].values,n_nearest=16)\n",
    "    neighbors = verify_reduced.iloc[indx]\n",
    "    decision_coords_inclination.append(np.array_equal( data_in_use[i][0:64].reshape(16,4),neighbors[[\"x\",\"y\",\"z\",\"Inclination_angle\"]]  ) )\n",
    "    decision_drag.append( np.array_equal(data_in_use[i][-1],neighbors.iloc[0][\"Drag_local\"]) )\n",
    "    clear_output(wait=True)\n",
    "\n",
    "np.unique(decision_coords_inclination),np.unique(decision_drag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "562af1ad-e5f4-4bf4-b7ef-874c33e1db09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True]), array([ True]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Verify Ar5 ###\n",
    "### specify case of the experiment ###\n",
    "case = 2\n",
    "decision_coords_inclination = list()\n",
    "decision_drag = list()\n",
    "\n",
    "### Specify solid fraction ###\n",
    "data_in_use = np.array(input_dat_sf30_case_2_ar5)\n",
    "\n",
    "for i in range(len(data_in_use)):\n",
    "    \n",
    "    print(\"particle number : \",str(i+1))\n",
    "    verify_reduced = verify_5[(verify_5[\"Re\"]==data_in_use[i][-2])&(verify_5[\"phi\"]==data_in_use[i][-3]*100)&(verify_5[\"case\"]==case)]\n",
    "    indx = brute_search(data_in_use[i][0:3],verify_reduced[[\"x\",\"y\",\"z\"]].values,n_nearest=16)\n",
    "    neighbors = verify_reduced.iloc[indx]\n",
    "    decision_coords_inclination.append(np.array_equal( data_in_use[i][0:64].reshape(16,4),neighbors[[\"x\",\"y\",\"z\",\"Inclination_angle\"]]  ) )\n",
    "    decision_drag.append( np.array_equal(data_in_use[i][-1],neighbors.iloc[0][\"Drag_local\"]) )\n",
    "    clear_output(wait=True)\n",
    "\n",
    "np.unique(decision_coords_inclination),np.unique(decision_drag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "701aa17a-7618-452d-808c-82c05762e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True]), array([ True]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Verify Ar10 ###\n",
    "### specify case of the experiment ###\n",
    "case = 2\n",
    "decision_coords_inclination = list()\n",
    "decision_drag = list()\n",
    "\n",
    "### Specify solid fraction ###\n",
    "data_in_use = np.array(input_dat_sf20_case_2_ar10)\n",
    "\n",
    "for i in range(len(data_in_use)):\n",
    "    \n",
    "    print(\"particle number : \",str(i+1))\n",
    "    verify_reduced = verify_10[(verify_10[\"Re\"]==data_in_use[i][-2])&(verify_10[\"phi\"]==data_in_use[i][-3]*100)&(verify_10[\"case\"]==case)]\n",
    "    indx = brute_search(data_in_use[i][0:3],verify_reduced[[\"x\",\"y\",\"z\"]].values,n_nearest=16)\n",
    "    neighbors = verify_reduced.iloc[indx]\n",
    "    decision_coords_inclination.append(np.array_equal( data_in_use[i][0:64].reshape(16,4),neighbors[[\"x\",\"y\",\"z\",\"Inclination_angle\"]]  ) )\n",
    "    decision_drag.append( np.array_equal(data_in_use[i][-1],neighbors.iloc[0][\"Drag_local\"]) )\n",
    "    clear_output(wait=True)\n",
    "\n",
    "np.unique(decision_coords_inclination),np.unique(decision_drag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4b3009d3-7226-467f-8dbe-faf732c858c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### end here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655774e7-ed16-4770-bdca-42707eeb781a",
   "metadata": {},
   "source": [
    "AR = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e58fda70-3fc1-4125-963d-36b4b6cd7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all center data ###\n",
    "\n",
    "### Sf=0.1 ###\n",
    "center_10_case_1 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_10_5_case_1.dat.npy\"))\n",
    "center_10_case_1 = center_10_case_1.rename(columns={ center_10_case_1.columns[3]: \"x\" , center_10_case_1.columns[4]: \"y\" , center_10_case_1.columns[5]: \"z\"  })\n",
    "center_10_dev_case_1 = center_10_case_1[(center_10_case_1['x']>7.5) & (center_10_case_1['x']<17.5)]\n",
    "\n",
    "center_10_case_2 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_10_5_case_2.dat.npy\"))\n",
    "center_10_case_2 = center_10_case_2.rename(columns={ center_10_case_2.columns[3]: \"x\" , center_10_case_2.columns[4]: \"y\" , center_10_case_2.columns[5]: \"z\"  })\n",
    "center_10_dev_case_2 = center_10_case_2[(center_10_case_2['x']>7.5) & (center_10_case_2['x']<17.5)]\n",
    "\n",
    "### Sf=0.2 ###\n",
    "center_20_case_1 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_20_5_case_1.dat.npy\"))\n",
    "center_20_case_1 = center_20_case_1.rename(columns={ center_20_case_1.columns[3]: \"x\" , center_20_case_1.columns[4]: \"y\" , center_20_case_1.columns[5]: \"z\"  })\n",
    "center_20_dev_case_1 = center_20_case_1[(center_20_case_1['x']>7.5) & (center_20_case_1['x']<17.5)]\n",
    "\n",
    "center_20_case_2 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_20_5_case_2.dat.npy\"))\n",
    "center_20_case_2 = center_20_case_2.rename(columns={ center_20_case_2.columns[3]: \"x\" , center_20_case_2.columns[4]: \"y\" , center_20_case_2.columns[5]: \"z\"  })\n",
    "center_20_dev_case_2 = center_20_case_2[(center_20_case_2['x']>7.5) & (center_20_case_2['x']<17.5)]\n",
    "\n",
    "### Sf=0.3 ###\n",
    "center_30_case_1 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_30_5_case_1.dat.npy\"))\n",
    "center_30_case_1 = center_30_case_1.rename(columns={ center_30_case_1.columns[3]: \"x\" , center_30_case_1.columns[4]: \"y\" , center_30_case_1.columns[5]: \"z\"  })\n",
    "center_30_dev_case_1 = center_30_case_1[(center_30_case_1['x']>7.5) & (center_30_case_1['x']<17.5)]\n",
    "\n",
    "center_30_case_2 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_30_5_case_2.dat.npy\"))\n",
    "center_30_case_2 = center_30_case_2.rename(columns={ center_30_case_2.columns[3]: \"x\" , center_30_case_2.columns[4]: \"y\" , center_30_case_2.columns[5]: \"z\"  })\n",
    "center_30_dev_case_2 = center_30_case_2[(center_30_case_2['x']>7.5) & (center_30_case_2['x']<17.5)]\n",
    "\n",
    "# ### Defining all particles ###\n",
    "center_shifted_10_case_1 = particle_shift(center_10_case_1,shift=10)[['x','y','z']]\n",
    "center_shifted_10_case_2 = particle_shift(center_10_case_2,shift=10)[['x','y','z']]\n",
    "\n",
    "center_shifted_20_case_1 = particle_shift(center_20_case_1,shift=10)[['x','y','z']]\n",
    "center_shifted_20_case_2 = particle_shift(center_20_case_2,shift=10)[['x','y','z']]\n",
    "\n",
    "center_shifted_30_case_1 = particle_shift(center_30_case_1,shift=10)[['x','y','z']]\n",
    "center_shifted_30_case_2 = particle_shift(center_30_case_2,shift=10)[['x','y','z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "18931e02-48d6-4912-977a-867447ffe24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particle number :  2324\n"
     ]
    }
   ],
   "source": [
    "input_dat_sf10_case_1_ar5 = generator(all_particles=center_shifted_10_case_1,dev_particles=center_10_dev_case_1,sf=0.1,master_dataframe=all_particles_5)\n",
    "input_dat_sf10_case_2_ar5 = generator(all_particles=center_shifted_10_case_2,dev_particles=center_10_dev_case_2,sf=0.1,master_dataframe=all_particles_5)\n",
    "\n",
    "input_dat_sf20_case_1_ar5 = generator(all_particles=center_shifted_20_case_1,dev_particles=center_20_dev_case_1,sf=0.2,master_dataframe=all_particles_5)\n",
    "input_dat_sf20_case_2_ar5 = generator(all_particles=center_shifted_20_case_2,dev_particles=center_20_dev_case_2,sf=0.2,master_dataframe=all_particles_5)\n",
    "\n",
    "input_dat_sf30_case_1_ar5 = generator(all_particles=center_shifted_30_case_1,dev_particles=center_30_dev_case_1,sf=0.3,master_dataframe=all_particles_5)\n",
    "input_dat_sf30_case_2_ar5 = generator(all_particles=center_shifted_30_case_2,dev_particles=center_30_dev_case_2,sf=0.3,master_dataframe=all_particles_5)\n",
    "\n",
    "### Combine all subsets to form the ar=2.5 dataset ###\n",
    "all_data_ar5 = input_dat_sf10_case_1_ar5 + input_dat_sf10_case_2_ar5 + input_dat_sf20_case_1_ar5 + input_dat_sf20_case_2_ar5 + input_dat_sf30_case_1_ar5 + input_dat_sf30_case_2_ar5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a4d065e4-71ef-40c1-af96-beaf1cc25de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_ar5 = input_dat_sf10_case_1_ar5 + input_dat_sf10_case_2_ar5 + input_dat_sf20_case_1_ar5 + input_dat_sf20_case_2_ar5 + input_dat_sf30_case_1_ar5 + input_dat_sf30_case_2_ar5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9240f44-f09d-4555-abed-c293b91b9592",
   "metadata": {},
   "source": [
    "AR=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "98fc336c-9519-49be-802f-909543a29a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all center data ###\n",
    "\n",
    "### Sf=0.1 ###\n",
    "center_10_case_1 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_10_10_case_1.dat.npy\"))\n",
    "center_10_case_1 = center_10_case_1.rename(columns={ center_10_case_1.columns[3]: \"x\" , center_10_case_1.columns[4]: \"y\" , center_10_case_1.columns[5]: \"z\"  })\n",
    "center_10_dev_case_1 = center_10_case_1[(center_10_case_1['x']>7.5) & (center_10_case_1['x']<17.5)]\n",
    "\n",
    "center_10_case_2 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_10_10_case_2.dat.npy\"))\n",
    "center_10_case_2 = center_10_case_2.rename(columns={ center_10_case_2.columns[3]: \"x\" , center_10_case_2.columns[4]: \"y\" , center_10_case_2.columns[5]: \"z\"  })\n",
    "center_10_dev_case_2 = center_10_case_2[(center_10_case_2['x']>7.5) & (center_10_case_2['x']<17.5)]\n",
    "\n",
    "### Sf=0.2 ###\n",
    "center_15_case_1 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_15_10_case_1.dat.npy\"))\n",
    "center_15_case_1 = center_15_case_1.rename(columns={ center_15_case_1.columns[3]: \"x\" , center_15_case_1.columns[4]: \"y\" , center_15_case_1.columns[5]: \"z\"  })\n",
    "center_15_dev_case_1 = center_15_case_1[(center_15_case_1['x']>7.5) & (center_15_case_1['x']<17.5)]\n",
    "\n",
    "center_15_case_2 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_15_10_case_2.dat.npy\"))\n",
    "center_15_case_2 = center_15_case_2.rename(columns={ center_15_case_2.columns[3]: \"x\" , center_15_case_2.columns[4]: \"y\" , center_15_case_2.columns[5]: \"z\"  })\n",
    "center_15_dev_case_2 = center_15_case_2[(center_15_case_2['x']>7.5) & (center_15_case_2['x']<17.5)]\n",
    "\n",
    "### Sf=0.3 ###\n",
    "center_20_case_1 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_20_10_case_1.dat.npy\"))\n",
    "center_20_case_1 = center_20_case_1.rename(columns={ center_20_case_1.columns[3]: \"x\" , center_20_case_1.columns[4]: \"y\" , center_20_case_1.columns[5]: \"z\"  })\n",
    "center_20_dev_case_1 = center_20_case_1[(center_20_case_1['x']>7.5) & (center_20_case_1['x']<17.5)]\n",
    "\n",
    "center_20_case_2 = pd.DataFrame(np.load(\"all_particle_data_files/center_phi_20_10_case_2.dat.npy\"))\n",
    "center_20_case_2 = center_20_case_2.rename(columns={ center_20_case_2.columns[3]: \"x\" , center_20_case_2.columns[4]: \"y\" , center_20_case_2.columns[5]: \"z\"  })\n",
    "center_20_dev_case_2 = center_20_case_2[(center_20_case_2['x']>7.5) & (center_20_case_2['x']<17.5)]\n",
    "\n",
    "# ### Defining all particles ###\n",
    "center_shifted_10_case_1 = particle_shift(center_10_case_1,shift=10)[['x','y','z']]\n",
    "center_shifted_10_case_2 = particle_shift(center_10_case_2,shift=10)[['x','y','z']]\n",
    "\n",
    "center_shifted_15_case_1 = particle_shift(center_15_case_1,shift=10)[['x','y','z']]\n",
    "center_shifted_15_case_2 = particle_shift(center_15_case_2,shift=10)[['x','y','z']]\n",
    "\n",
    "center_shifted_20_case_1 = particle_shift(center_20_case_1,shift=10)[['x','y','z']]\n",
    "center_shifted_20_case_2 = particle_shift(center_20_case_2,shift=10)[['x','y','z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "e89686f5-c073-45ef-b2f9-2ea3cbd10427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particle number :  1592\n"
     ]
    }
   ],
   "source": [
    "input_dat_sf10_case_1_ar10 = generator(all_particles=center_shifted_10_case_1,dev_particles=center_10_dev_case_1,sf=0.1,master_dataframe=all_particles_10)\n",
    "input_dat_sf10_case_2_ar10 = generator(all_particles=center_shifted_10_case_2,dev_particles=center_10_dev_case_2,sf=0.1,master_dataframe=all_particles_10)\n",
    "\n",
    "input_dat_sf15_case_1_ar10 = generator(all_particles=center_shifted_15_case_1,dev_particles=center_15_dev_case_1,sf=0.15,master_dataframe=all_particles_10)\n",
    "input_dat_sf15_case_2_ar10 = generator(all_particles=center_shifted_15_case_2,dev_particles=center_15_dev_case_2,sf=0.15,master_dataframe=all_particles_10)\n",
    "\n",
    "input_dat_sf20_case_1_ar10 = generator(all_particles=center_shifted_20_case_1,dev_particles=center_20_dev_case_1,sf=0.2,master_dataframe=all_particles_10)\n",
    "input_dat_sf20_case_2_ar10 = generator(all_particles=center_shifted_20_case_2,dev_particles=center_20_dev_case_2,sf=0.2,master_dataframe=all_particles_10)\n",
    "\n",
    "### Combine all subsets to form the ar=2.5 dataset ###\n",
    "all_data_ar10 = input_dat_sf10_case_1_ar10+input_dat_sf10_case_2_ar10+input_dat_sf15_case_1_ar10+input_dat_sf15_case_2_ar10+input_dat_sf20_case_1_ar10+input_dat_sf20_case_2_ar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2e8dc1be-619c-40e2-820c-09bad0442050",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_ar10 = input_dat_sf10_case_1_ar10 + input_dat_sf10_case_2_ar10 + input_dat_sf15_case_1_ar10 + input_dat_sf15_case_2_ar10 + input_dat_sf20_case_1_ar10 + input_dat_sf20_case_2_ar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "1fdab8b7-6235-4403-8d7c-f7a023df71f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POI_x_coord</th>\n",
       "      <th>POI_y_coord</th>\n",
       "      <th>POI_z_coord</th>\n",
       "      <th>inclination_poi</th>\n",
       "      <th>neigh_1_x</th>\n",
       "      <th>neigh_1_y</th>\n",
       "      <th>neigh_1_z</th>\n",
       "      <th>inclination_1</th>\n",
       "      <th>neigh_2_x</th>\n",
       "      <th>neigh_2_y</th>\n",
       "      <th>...</th>\n",
       "      <th>neigh_14_z</th>\n",
       "      <th>inclination_14</th>\n",
       "      <th>neigh_15_x</th>\n",
       "      <th>neigh_15_y</th>\n",
       "      <th>neigh_15_z</th>\n",
       "      <th>inclination_15</th>\n",
       "      <th>AR</th>\n",
       "      <th>Phi</th>\n",
       "      <th>Re</th>\n",
       "      <th>Drag_local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.876728</td>\n",
       "      <td>8.580914</td>\n",
       "      <td>1.499941</td>\n",
       "      <td>32.081738</td>\n",
       "      <td>10.469863</td>\n",
       "      <td>8.375915</td>\n",
       "      <td>0.380001</td>\n",
       "      <td>59.653803</td>\n",
       "      <td>12.365468</td>\n",
       "      <td>8.985445</td>\n",
       "      <td>...</td>\n",
       "      <td>1.279047</td>\n",
       "      <td>46.541607</td>\n",
       "      <td>10.606896</td>\n",
       "      <td>9.100682</td>\n",
       "      <td>-1.328129</td>\n",
       "      <td>58.238513</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.903370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.929652</td>\n",
       "      <td>8.518986</td>\n",
       "      <td>8.546638</td>\n",
       "      <td>77.315739</td>\n",
       "      <td>13.137204</td>\n",
       "      <td>7.388673</td>\n",
       "      <td>9.134358</td>\n",
       "      <td>38.412987</td>\n",
       "      <td>14.129487</td>\n",
       "      <td>8.677728</td>\n",
       "      <td>...</td>\n",
       "      <td>7.350211</td>\n",
       "      <td>61.077618</td>\n",
       "      <td>13.048629</td>\n",
       "      <td>11.408079</td>\n",
       "      <td>8.215324</td>\n",
       "      <td>63.637424</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.023779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.018229</td>\n",
       "      <td>8.503029</td>\n",
       "      <td>1.464684</td>\n",
       "      <td>52.642073</td>\n",
       "      <td>15.846247</td>\n",
       "      <td>8.562797</td>\n",
       "      <td>1.726459</td>\n",
       "      <td>64.790014</td>\n",
       "      <td>17.168299</td>\n",
       "      <td>7.801587</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141856</td>\n",
       "      <td>78.979347</td>\n",
       "      <td>17.064104</td>\n",
       "      <td>11.427227</td>\n",
       "      <td>1.433072</td>\n",
       "      <td>49.657668</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.573232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.603652</td>\n",
       "      <td>1.499732</td>\n",
       "      <td>9.825560</td>\n",
       "      <td>60.625083</td>\n",
       "      <td>16.301634</td>\n",
       "      <td>1.485659</td>\n",
       "      <td>8.521393</td>\n",
       "      <td>87.749395</td>\n",
       "      <td>17.064104</td>\n",
       "      <td>1.427227</td>\n",
       "      <td>...</td>\n",
       "      <td>7.689390</td>\n",
       "      <td>64.057556</td>\n",
       "      <td>16.342990</td>\n",
       "      <td>4.722934</td>\n",
       "      <td>11.133123</td>\n",
       "      <td>60.235139</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.699245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.737969</td>\n",
       "      <td>1.485545</td>\n",
       "      <td>9.792119</td>\n",
       "      <td>51.275008</td>\n",
       "      <td>12.637778</td>\n",
       "      <td>2.087273</td>\n",
       "      <td>10.319745</td>\n",
       "      <td>66.724320</td>\n",
       "      <td>11.783217</td>\n",
       "      <td>0.809170</td>\n",
       "      <td>...</td>\n",
       "      <td>7.937789</td>\n",
       "      <td>80.703918</td>\n",
       "      <td>11.646745</td>\n",
       "      <td>1.887016</td>\n",
       "      <td>7.009987</td>\n",
       "      <td>70.790087</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.034383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   POI_x_coord  POI_y_coord  POI_z_coord  inclination_poi  neigh_1_x  \\\n",
       "0    10.876728     8.580914     1.499941        32.081738  10.469863   \n",
       "1    12.929652     8.518986     8.546638        77.315739  13.137204   \n",
       "2    17.018229     8.503029     1.464684        52.642073  15.846247   \n",
       "3    16.603652     1.499732     9.825560        60.625083  16.301634   \n",
       "4    11.737969     1.485545     9.792119        51.275008  12.637778   \n",
       "\n",
       "   neigh_1_y  neigh_1_z  inclination_1  neigh_2_x  neigh_2_y  ...  neigh_14_z  \\\n",
       "0   8.375915   0.380001      59.653803  12.365468   8.985445  ...    1.279047   \n",
       "1   7.388673   9.134358      38.412987  14.129487   8.677728  ...    7.350211   \n",
       "2   8.562797   1.726459      64.790014  17.168299   7.801587  ...    1.141856   \n",
       "3   1.485659   8.521393      87.749395  17.064104   1.427227  ...    7.689390   \n",
       "4   2.087273  10.319745      66.724320  11.783217   0.809170  ...    7.937789   \n",
       "\n",
       "   inclination_14  neigh_15_x  neigh_15_y  neigh_15_z  inclination_15   AR  \\\n",
       "0       46.541607   10.606896    9.100682   -1.328129       58.238513  2.5   \n",
       "1       61.077618   13.048629   11.408079    8.215324       63.637424  2.5   \n",
       "2       78.979347   17.064104   11.427227    1.433072       49.657668  2.5   \n",
       "3       64.057556   16.342990    4.722934   11.133123       60.235139  2.5   \n",
       "4       80.703918   11.646745    1.887016    7.009987       70.790087  2.5   \n",
       "\n",
       "   Phi    Re  Drag_local  \n",
       "0  0.1  10.0    3.903370  \n",
       "1  0.1  10.0    5.023779  \n",
       "2  0.1  10.0    4.573232  \n",
       "3  0.1  10.0    6.699245  \n",
       "4  0.1  10.0    7.034383  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = all_data_ar2p5 + all_data_ar5 + all_data_ar10\n",
    "\n",
    "all_data_pd = pd.DataFrame(all_data,columns=[\"POI_x_coord\",\"POI_y_coord\",\"POI_z_coord\",\"inclination_poi\",\n",
    "                                             \"neigh_1_x\",\"neigh_1_y\",\"neigh_1_z\",\"inclination_1\",\n",
    "                                             \"neigh_2_x\",\"neigh_2_y\",\"neigh_2_z\",\"inclination_2\",\n",
    "                                             \"neigh_3_x\",\"neigh_3_y\",\"neigh_3_z\",\"inclination_3\",\n",
    "                                             \"neigh_4_x\",\"neigh_4_y\",\"neigh_4_z\",\"inclination_4\",\n",
    "                                             \"neigh_5_x\",\"neigh_5_y\",\"neigh_5_z\",\"inclination_5\",\n",
    "                                             \"neigh_6_x\",\"neigh_6_y\",\"neigh_6_z\",\"inclination_6\",\n",
    "                                             \"neigh_7_x\",\"neigh_7_y\",\"neigh_7_z\",\"inclination_7\",\n",
    "                                             \"neigh_8_x\",\"neigh_8_y\",\"neigh_8_z\",\"inclination_8\",\n",
    "                                             \"neigh_9_x\",\"neigh_9_y\",\"neigh_9_z\",\"inclination_9\",\n",
    "                                             \"neigh_10_x\",\"neigh_10_y\",\"neigh_10_z\",\"inclination_10\",\n",
    "                                             \"neigh_11_x\",\"neigh_11_y\",\"neigh_11_z\",\"inclination_11\",\n",
    "                                             \"neigh_12_x\",\"neigh_12_y\",\"neigh_12_z\",\"inclination_12\",\n",
    "                                             \"neigh_13_x\",\"neigh_13_y\",\"neigh_13_z\",\"inclination_13\",\n",
    "                                             \"neigh_14_x\",\"neigh_14_y\",\"neigh_14_z\",\"inclination_14\",\n",
    "                                             \"neigh_15_x\",\"neigh_15_y\",\"neigh_15_z\",\"inclination_15\",\n",
    "                                              \"AR\",\"Phi\",\"Re\",\"Drag_local\"])\n",
    "all_data_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "5c566668-3c86-42c9-a226-3932a9fe4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining train and test indices (first 40 particle from each experiment) ###\n",
    "check_points = np.array([0,\n",
    "                         len(input_dat_sf10_ar2p5)//4,len(input_dat_sf10_ar2p5)//4,len(input_dat_sf10_ar2p5)//4,len(input_dat_sf10_ar2p5)//4,\n",
    "                         len(input_dat_sf20_ar2p5)//4,len(input_dat_sf20_ar2p5)//4,len(input_dat_sf20_ar2p5)//4,len(input_dat_sf20_ar2p5)//4,\n",
    "                         len(input_dat_sf30_ar2p5)//4,len(input_dat_sf30_ar2p5)//4,len(input_dat_sf30_ar2p5)//4,len(input_dat_sf30_ar2p5)//4,\n",
    "\n",
    "                         len(input_dat_sf10_case_1_ar5)//4,len(input_dat_sf10_case_1_ar5)//4,len(input_dat_sf10_case_1_ar5)//4,len(input_dat_sf10_case_1_ar5)//4,\n",
    "                         len(input_dat_sf10_case_2_ar5)//4,len(input_dat_sf10_case_2_ar5)//4,len(input_dat_sf10_case_2_ar5)//4,len(input_dat_sf10_case_2_ar5)//4,\n",
    "\n",
    "                         len(input_dat_sf20_case_1_ar5)//4,len(input_dat_sf20_case_1_ar5)//4,len(input_dat_sf20_case_1_ar5)//4,len(input_dat_sf20_case_1_ar5)//4,\n",
    "                         len(input_dat_sf20_case_2_ar5)//4,len(input_dat_sf20_case_2_ar5)//4,len(input_dat_sf20_case_2_ar5)//4,len(input_dat_sf20_case_2_ar5)//4,\n",
    "\n",
    "                         len(input_dat_sf30_case_1_ar5)//4,len(input_dat_sf30_case_1_ar5)//4,len(input_dat_sf30_case_1_ar5)//4,len(input_dat_sf30_case_1_ar5)//4,\n",
    "                         len(input_dat_sf30_case_2_ar5)//4,len(input_dat_sf30_case_2_ar5)//4,len(input_dat_sf30_case_2_ar5)//4,len(input_dat_sf30_case_2_ar5)//4,\n",
    "\n",
    "\n",
    "                         len(input_dat_sf10_case_1_ar10)//4,len(input_dat_sf10_case_1_ar10)//4,len(input_dat_sf10_case_1_ar10)//4,len(input_dat_sf10_case_1_ar10)//4,\n",
    "                         len(input_dat_sf10_case_2_ar10)//4,len(input_dat_sf10_case_2_ar10)//4,len(input_dat_sf10_case_2_ar10)//4,len(input_dat_sf10_case_2_ar10)//4,\n",
    "\n",
    "                         len(input_dat_sf15_case_1_ar10)//4,len(input_dat_sf15_case_1_ar10)//4,len(input_dat_sf15_case_1_ar10)//4,len(input_dat_sf15_case_1_ar10)//4,\n",
    "                         len(input_dat_sf15_case_2_ar10)//4,len(input_dat_sf15_case_2_ar10)//4,len(input_dat_sf15_case_2_ar10)//4,len(input_dat_sf15_case_2_ar10)//4,\n",
    "\n",
    "                         len(input_dat_sf20_case_1_ar10)//4,len(input_dat_sf20_case_1_ar10)//4,len(input_dat_sf20_case_1_ar10)//4,len(input_dat_sf20_case_1_ar10)//4,\n",
    "                         len(input_dat_sf20_case_2_ar10)//4,len(input_dat_sf20_case_2_ar10)//4,len(input_dat_sf20_case_2_ar10)//4,len(input_dat_sf20_case_2_ar10)//4,\n",
    "\n",
    "                        ]).cumsum() \n",
    "\n",
    "test_indices = np.stack([np.arange(40) + check_points[i] for i in range(len(check_points)-1)]).flatten()\n",
    "train_indices = np.setdiff1d( np.arange(len(all_data)) , test_indices )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b9be97a3-1601-4928-86f7-11afb0203e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic Data ###\n",
    "### All the locations are wrt the global coordinates ###\n",
    "\n",
    "### Scaling the data ###\n",
    "train_data_pd = all_data_pd.iloc[train_indices]\n",
    "test_data_pd = all_data_pd.iloc[test_indices]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data_pd)\n",
    "\n",
    "train_data_pd_scaled = scaler.transform(train_data_pd)\n",
    "test_data_pd_scaled = scaler.transform(test_data_pd)\n",
    "all_data_pd_scaled = scaler.transform(all_data_pd)\n",
    "\n",
    "### splitting to input and outputs ##3\n",
    "train_inputs,train_inputs_global,train_outputs = train_data_pd_scaled[:,0:64],train_data_pd_scaled[:,64:67],train_data_pd_scaled[:,67:]\n",
    "test_inputs,test_inputs_global,test_outputs = test_data_pd_scaled[:,0:64],test_data_pd_scaled[:,64:67],test_data_pd_scaled[:,67:]\n",
    "\n",
    "### Saving the data ###\n",
    "### Train ###\n",
    "np.save(\"quick_load/first_40/train_inputs_MLP\",train_inputs)\n",
    "np.save(\"quick_load/first_40/train_inputs_global\",train_inputs_global)\n",
    "np.save(\"quick_load/first_40/train_outputs_MLP\",train_outputs)\n",
    "\n",
    "# ### Test ###\n",
    "np.save(\"quick_load/first_40/test_inputs_MLP\",test_inputs)\n",
    "np.save(\"quick_load/first_40/test_inputs_global\",test_inputs_global)\n",
    "np.save(\"quick_load/first_40/test_outputs_MLP\",test_outputs)\n",
    "\n",
    "# ### All Data ###\n",
    "np.save(\"quick_load/first_40/all_data_gnn\",all_data_pd_scaled)\n",
    "np.save(\"quick_load/first_40/train_indices\",train_indices)\n",
    "np.save(\"quick_load/first_40/test_indices\",test_indices)\n",
    "\n",
    "### Save min and max drag values ###\n",
    "np.save(\"quick_load/first_40/min_max_drags\", np.array([scaler.data_min_[-1],scaler.data_max_[-1]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "2c741569-a3fa-4194-abd7-ad5b3d152a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Local Data ###\n",
    "\n",
    "### All the locations are wrt to the coordinates of the POI ###\n",
    "### location translation ###\n",
    "all_data_pd_local = all_data_pd.copy()\n",
    "\n",
    "### x-coordinate ###\n",
    "neigh_columns = [col for col in all_data_pd.columns if col.startswith('neigh_') and col.endswith('_x')]\n",
    "for col in neigh_columns:\n",
    "    all_data_pd_local[col] = all_data_pd['POI_x_coord'] - all_data_pd[col]\n",
    "\n",
    "### x-coordinate ###\n",
    "neigh_columns = [col for col in all_data_pd.columns if col.startswith('neigh_') and col.endswith('_y')]\n",
    "for col in neigh_columns:\n",
    "    all_data_pd_local[col] = all_data_pd['POI_y_coord'] - all_data_pd[col]\n",
    "\n",
    "### x-coordinate ###\n",
    "neigh_columns = [col for col in all_data_pd.columns if col.startswith('neigh_') and col.endswith('_z')]\n",
    "for col in neigh_columns:\n",
    "    all_data_pd_local[col] = all_data_pd['POI_z_coord'] - all_data_pd[col]\n",
    "\n",
    "\n",
    "### Scaling the data ###\n",
    "train_data_pd = all_data_pd_local.iloc[train_indices]\n",
    "test_data_pd = all_data_pd_local.iloc[test_indices]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data_pd)\n",
    "\n",
    "train_data_pd_scaled = scaler.transform(train_data_pd)\n",
    "test_data_pd_scaled = scaler.transform(test_data_pd)\n",
    "all_data_pd_scaled = scaler.transform(all_data_pd_local)\n",
    "\n",
    "### splitting to input and outputs ##3\n",
    "train_inputs,train_inputs_global,train_outputs = train_data_pd_scaled[:,0:64],train_data_pd_scaled[:,64:67],train_data_pd_scaled[:,67:]\n",
    "test_inputs,test_inputs_global,test_outputs = test_data_pd_scaled[:,0:64],test_data_pd_scaled[:,64:67],test_data_pd_scaled[:,67:]\n",
    "\n",
    "### Saving the data ###\n",
    "### Train ###\n",
    "np.save(\"quick_load/first_40_local_coordinates/train_inputs_MLP\",train_inputs)\n",
    "np.save(\"quick_load/first_40_local_coordinates/train_inputs_global\",train_inputs_global)\n",
    "np.save(\"quick_load/first_40_local_coordinates/train_outputs_MLP\",train_outputs)\n",
    "\n",
    "# ### Test ###\n",
    "np.save(\"quick_load/first_40_local_coordinates/test_inputs_MLP\",test_inputs)\n",
    "np.save(\"quick_load/first_40_local_coordinates/test_inputs_global\",test_inputs_global)\n",
    "np.save(\"quick_load/first_40_local_coordinates/test_outputs_MLP\",test_outputs)\n",
    "\n",
    "# ### All Data ###\n",
    "np.save(\"quick_load/first_40_local_coordinates/all_data_gnn_local_coord\",all_data_pd_scaled)\n",
    "np.save(\"quick_load/first_40_local_coordinates/train_indices\",train_indices)\n",
    "np.save(\"quick_load/first_40_local_coordinates/test_indices\",test_indices)\n",
    "\n",
    "### Save min and max drag values ###\n",
    "np.save(\"quick_load/first_40_local_coordinates/min_max_drags\", np.array([scaler.data_min_[-1],scaler.data_max_[-1]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "169389bd-fb12-4caa-b10b-adc996d1d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining train and test indices (setting an experiment with all Reynolds numbers as test dataset) ###\n",
    "\n",
    "Ar=5\n",
    "Sf=0.2\n",
    "Re=100\n",
    "case=1\n",
    "\n",
    "test_indices = np.where((all_tags_with_case_no[:,0]==Ar)&(all_tags_with_case_no[:,1]==Sf)&(all_tags_with_case_no[:,3]==case))\n",
    "train_indices = np.setdiff1d( np.arange(len(all_data)) , test_indices )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "46218a2d-9821-49e3-9054-4df138ff3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Single experiment split basic data ###\n",
    "### All the locations are wrt the global coordinates ###\n",
    "\n",
    "### Scaling the data ###\n",
    "train_data_pd = all_data_pd.iloc[train_indices]\n",
    "test_data_pd = all_data_pd.iloc[test_indices]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data_pd)\n",
    "\n",
    "train_data_pd_scaled = scaler.transform(train_data_pd)\n",
    "test_data_pd_scaled = scaler.transform(test_data_pd)\n",
    "all_data_pd_scaled = scaler.transform(all_data_pd)\n",
    "\n",
    "### splitting to input and outputs ##3\n",
    "train_inputs,train_inputs_global,train_outputs = train_data_pd_scaled[:,0:64],train_data_pd_scaled[:,64:67],train_data_pd_scaled[:,67:]\n",
    "test_inputs,test_inputs_global,test_outputs = test_data_pd_scaled[:,0:64],test_data_pd_scaled[:,64:67],test_data_pd_scaled[:,67:]\n",
    "\n",
    "### Saving the data ###\n",
    "### Train ###\n",
    "np.save(\"quick_load/single_experiment_ar5_sf_20_case_1/train_inputs_MLP\",train_inputs)\n",
    "np.save(\"quick_load/single_experiment_ar5_sf_20_case_1/train_inputs_global\",train_inputs_global)\n",
    "np.save(\"quick_load/single_experiment_ar5_sf_20_case_1/train_outputs_MLP\",train_outputs)\n",
    "\n",
    "# ### Test ###\n",
    "np.save(\"quick_load/single_experiment_ar5_sf_20_case_1/test_inputs_MLP\",test_inputs)\n",
    "np.save(\"quick_load/single_experiment_ar5_sf_20_case_1/test_inputs_global\",test_inputs_global)\n",
    "np.save(\"quick_load/single_experiment_ar5_sf_20_case_1/test_outputs_MLP\",test_outputs)\n",
    "\n",
    "# ### All Data ###\n",
    "np.save(\"quick_load/single_experiment_ar5_sf_20_case_1/all_data_gnn\",all_data_pd_scaled)\n",
    "np.save(\"quick_load/single_experiment_ar5_sf_20_case_1/train_indices\",train_indices)\n",
    "np.save(\"quick_load/single_experiment_ar5_sf_20_case_1/test_indices\",test_indices)\n",
    "\n",
    "### Save min and max drag values ###\n",
    "np.save(\"quick_load/single_experiment_ar5_sf_20_case_1/min_max_drags\", np.array([scaler.data_min_[-1],scaler.data_max_[-1]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a16f6c-6c94-4d17-be8e-f6c9dda61871",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ed6a0-c522-4929-a6d4-837c2c395502",
   "metadata": {},
   "source": [
    "Generate from Ze time series dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a2b597fa-8c9b-4289-aa52-78fdd1691d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_periodic_coordinates(coord, size):\n",
    "    \"\"\"\n",
    "    Generate all coordinates within a cubic domain considering periodic boundary conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        coord (pandas dataframe): A pandas dataframe containing the columns (x, y, z) of a point.\n",
    "        size (int): The size of the cubic domain along each axis.\n",
    "    Returns:\n",
    "        list: A list of tuples containing all coordinates within the cubic domain.\n",
    "    \"\"\"\n",
    "    # Get coordinates ###\n",
    "    coord = coord[[\"x\",\"y\",\"z\"]].values\n",
    "\n",
    "    # Generate all combinations of displacements (-1, 0, 1) along each axis\n",
    "    displacements = list(itertools.product([-1, 0, 1], repeat=3))\n",
    "\n",
    "    # Generate all coordinates by applying periodic boundary conditions\n",
    "    tp_coordinates = list()\n",
    "    \n",
    "    for i in range(len(coord)):\n",
    "        \n",
    "        x, y, z = coord[i,0],coord[i,1],coord[i,2]  \n",
    "        temp = list()\n",
    "        \n",
    "        for dx, dy, dz in displacements:\n",
    "            \n",
    "            new_x = x + dx*size\n",
    "            new_y = y + dy*size\n",
    "            new_z = z + dz*size\n",
    "\n",
    "            temp.append((new_x,new_y,new_z))\n",
    "            \n",
    "        tp_coordinates.append( np.array(temp) )\n",
    "    \n",
    "    return np.vstack(tp_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "12fc9653-576c-4e90-bb65-a82d652f6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_time_series_data(time_series_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Groups the data based on case_ID and time \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    ### load raw data from ze time series data ###\n",
    "    pd_list  = list()\n",
    "    \n",
    "    for (col1_val, col2_val), group in time_series_data.groupby(['case_ID', 'time']):\n",
    "    \n",
    "        pd_list.append(group)\n",
    "    \n",
    "    return pd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "38c4f4e0-3397-4a2a-ba51-141f5eb0b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearest_neighbor_data(time_series_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper function to do the data generation \n",
    "    \n",
    "    Parameters:\n",
    "       time_series_data (pandas dataframe) : obtained from Ze's final data directory \n",
    "    Returns:\n",
    "        list: A list of pandas dataframes each with a unique case id and time-stamp\n",
    "    \"\"\"\n",
    "    \n",
    "    pd_list = group_time_series_data(time_series_data)\n",
    "    \n",
    "    nearest_neighbor_data = list()\n",
    "    scalar_data = list()\n",
    "    \n",
    "    ### Loop over different groups ###\n",
    "    \n",
    "    for i in range(len(pd_list)):\n",
    "\n",
    "        tp_particles = get_periodic_coordinates(pd_list[i],5)\n",
    "        tree = cKDTree(tp_particles)\n",
    "\n",
    "        ### Loop over all particles in a group and getting the nearest neighbors ###\n",
    "        idx = np.stack([ tree.query(pd_list[i].iloc[j][[\"x\",\"y\",\"z\"]].values,16)[1] for j in range(len(pd_list[i])) ])\n",
    "        nearest_neighbor_data.append(tp_particles[idx])\n",
    "\n",
    "        ### Getting the scalar data ###\n",
    "        scalar_data.append( pd_list[i][[\"Density_ratio\",\"glb_phi\",\"glb_Re\",\"cor_drag\",\"local_Re\",\"Drag\"]] )\n",
    "\n",
    "    nearest_neighbor_data = np.stack(nearest_neighbor_data)\n",
    "    nearest_neighbor_data = nearest_neighbor_data.reshape(nearest_neighbor_data.shape[0]*nearest_neighbor_data.shape[1]\n",
    "                                           ,nearest_neighbor_data.shape[2]*nearest_neighbor_data.shape[3])\n",
    "    \n",
    "    scalar_data = np.stack(scalar_data)\n",
    "    scalar_data = scalar_data.reshape(scalar_data.shape[0]*scalar_data.shape[1],scalar_data.shape[2])    \n",
    "    \n",
    "    return np.concatenate( (nearest_neighbor_data,scalar_data) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d0d93504-ae34-4f09-b4f2-cf14bf501100",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read data ###\n",
    "time_series_data = pd.read_csv(\"Ze_time_series/final_data/final_data/rho10_30percent_Re10.dat\")\n",
    "pd_list = generate_from_time_series_data(time_series_data)\n",
    "nearest_neighbor_data = generate_nearest_neighbor_data(time_series_data)\n",
    "# nearest_neighbor_data,scalar_data = generate_nearest_neighbor_data(time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "e12f9673-4a5f-4009-a1e0-64618c69bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the data as test and train ###\n",
    "X_train, X_test, y_train, y_test = train_test_split(nearest_neighbor_data[:,0:48],nearest_neighbor_data[:,48:], test_size=0.2, random_state=1)\n",
    "\n",
    "### Scaling the data ###\n",
    "\n",
    "### Inputs ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "train_input_scaled = scaler.transform(X_train)\n",
    "test_input_scaled = scaler.transform(X_test)\n",
    "\n",
    "### Scalar Inputs and outputs ###\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train)\n",
    "\n",
    "train_output_scaled = scaler.transform(y_train)\n",
    "test_output_scaled = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "7c7e819c-89f1-42fe-b223-6016133b663d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36057, 6)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "2a564962-80ee-479a-bf6a-f6305c1db403",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Ze_time_series/quick_load/train_input\",train_input_scaled.reshape(train_input_scaled.shape[0],16,3))\n",
    "np.save(\"Ze_time_series/quick_load/test_input\",test_input_scaled.reshape(test_input_scaled.shape[0],16,3))\n",
    "\n",
    "np.save(\"Ze_time_series/quick_load/train_input_scalar\",train_output_scaled[:,0:5])\n",
    "np.save(\"Ze_time_series/quick_load/test_input_scalar\",test_output_scaled[:,0:5])\n",
    "\n",
    "np.save(\"Ze_time_series/quick_load/train_output\",train_output_scaled[:,5:])\n",
    "np.save(\"Ze_time_series/quick_load/test_output\",test_output_scaled[:,5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a7f7b-addc-4fbc-8c78-f1472d5e07b8",
   "metadata": {},
   "source": [
    "End here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ec6a583-7800-4b43-bcb6-1db1ce001046",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing GNN data ###\n",
    "all_data_graph_struct = list()\n",
    "\n",
    "edge_index = torch.tensor([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                           [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]])\n",
    "\n",
    "for i in range(len(all_data_pd)):\n",
    "\n",
    "    ### setting inputs ###\n",
    "    x = torch.tensor(all_data_pd_scaled[i][0:64].reshape(16,4)).float().clone().detach()\n",
    "    \n",
    "    ### adding drag force as y ###\n",
    "    y = torch.tensor(all_data_pd_scaled[i][-1][None,None]).float().clone().detach()\n",
    "    \n",
    "    # all_data_graph_struct.append(Data(x=x , edge_index=torch.tensor(mirror_edge_index(edge_index)) , y=y))\n",
    "    all_data_graph_struct.append(Data(x=x.clone().detach() , edge_index=edge_index.clone().detach() , y=y.clone().detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d48fec-9b57-4422-8bb7-28c16250b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sf=0.20 case_2 ###\n",
    "### Generating the n nearest neighbors ###\n",
    "all_particles = center_shifted_20_case_2.copy()\n",
    "dev_particles = center_20_dev_case_2.copy()\n",
    "\n",
    "tree = cKDTree(all_particles.values)\n",
    "idx = np.stack([tree.query(dev_particles.iloc[i][[\"x\",\"y\",\"z\"]].values,16)[1] for i in range(len(dev_particles))])\n",
    "input_dat = np.stack([all_particles.iloc[idx[i]] for i in range(len(idx))])\n",
    "\n",
    "### Adding solid fraction column ###\n",
    "input_dat = [add_const_parameter(input_dat[i],0.20) for i in range(len(input_dat))]\n",
    "\n",
    "### Adding 4 Reynolds numbers and combining the datasets ###\n",
    "input_dat = np.vstack( ( np.stack( [add_const_parameter(input_dat[i],10) for i in range(len(input_dat))] ),\n",
    "                              np.stack( [add_const_parameter(input_dat[i],50) for i in range(len(input_dat))] ),\n",
    "                              np.stack( [add_const_parameter(input_dat[i],50) for i in range(len(input_dat))] ),\n",
    "                              np.stack( [add_const_parameter(input_dat[i],50) for i in range(len(input_dat))] ),\n",
    "                    ) )\n",
    "input_dat = [ pd.DataFrame( input_dat[i],columns=[\"Center_x\",\"Center_y\",\"Center_z\",\"Phi\",\"Re\"]) for i in range(len(input_dat)) ]\n",
    "\n",
    "### Joing with df array to get inclination and drag and other quantities if needed ###\n",
    "input_dat = merge_columns_to_pandas_list(pandas_list=input_dat ,variable_list=[\"Inclination_angle\",\"Drag_local\"],master_dataframe=df)\n",
    "input_dat_sf20_ar10_case_2 = [ np.concatenate( (input_dat[i][[\"Center_x\",\"Center_y\",\"Center_z\",\"Inclination_angle\"]].values.flatten()\n",
    "                                       ,input_dat[i].iloc[0][[\"Phi\",\"Re\",\"Drag_local\"]].values) )  for i in range(len(input_dat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc3aab-d15d-45fa-a225-420eab373653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_list = np.concatenate((all_particles_2p5[ (all_particles_2p5[\"Center_x\"]>7.5) & (all_particles_2p5[\"Center_x\"]<17.5) ][\"case\"].values[:,None],\n",
    "# all_particles_5[ (all_particles_5[\"Center_x\"]>7.5) & (all_particles_5[\"Center_x\"]<17.5) ][\"case\"].values[:,None],\n",
    "# all_particles_10[ (all_particles_10[\"Center_x\"]>7.5) & (all_particles_10[\"Center_x\"]<17.5) ][\"case\"].values[:,None]),axis=0)\n",
    "\n",
    "# all_tags_with_case = np.concatenate((all_tags,case_list),axis=1)\n",
    "# all_tags_with_case\n",
    "# np.save(\"quick_load/all_tags_with_case_no.npy\",all_tags_with_case)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
